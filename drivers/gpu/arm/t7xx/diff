diff -Nur r5p0/platform/gpu_balance.c r15p0/platform/exynos/gpu_balance.c
--- r5p0/platform/gpu_balance.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_balance.c	2017-07-20 16:14:04.620559419 +0200
@@ -1,11 +1,27 @@
+/* drivers/gpu/arm/.../platform/gpu_balance.c
+ *
+ * Copyright 2011 by S.LSI. Samsung Electronics Inc.
+ * San#24, Nongseo-Dong, Giheung-Gu, Yongin, Korea
+ *
+ * Samsung SoC Mali-T Series gpu h/w init code.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software FoundatIon.
+ */
+
+/**
+ * @file gpu_balance.c
+ */
+
 #include <mali_kbase.h>
 #include <linux/delay.h>
 #include <asm/dma-mapping.h>
 #include <mach/map.h>
 
 #include "mali_kbase_platform.h"
-#include "gpu_balance.h"
 
+#include "gpu_balance.h"
 #include "streamout.h"
 #include "streamout_SRAM.h"
 
@@ -85,7 +101,7 @@
 
 	if(checksum != golden_sum)
 	{
-		dev_dbg(kbdev->dev, "start 0x%p, end 0x%p %ld != %ld\n", &start[0], end, checksum, golden_sum);
+		dev_dbg(kbdev->dev, "start 0x%p, end 0x%p %ld != %ld\n", &start[i], end, checksum, golden_sum);
 		return 1;
 	}
 
@@ -129,6 +145,7 @@
 
 	vfree(pages);
 }
+
 void preload_balance_setup(struct kbase_device *kbdev)
 {
 	memcpy(mali_balance.stream_reg, streamout, streamout_size);
@@ -137,7 +154,7 @@
 
 bool balance_init(struct kbase_device *kbdev)
 {
-	mali_bool ret = MALI_TRUE;
+	bool ret = true;
 
 	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "power initialized\n");
 
@@ -373,8 +390,8 @@
 
 	if(test_checksum_all() != 0)
 	{
-		KBASE_TRACE_ADD_EXYNOS(kbdev, LSI_CHECKSUM, NULL, NULL, 0u, MALI_FALSE);
-		ret = MALI_FALSE;
+		KBASE_TRACE_ADD_EXYNOS(kbdev, LSI_CHECKSUM, NULL, NULL, 0u, false);
+		ret = false;
 	}
 
 	/* Mali reset */
@@ -388,15 +405,17 @@
 	return ret;
 }
 
-int exynos_gpu_init_hw(struct kbase_device *kbdev)
+int exynos_gpu_init_hw(void *dev)
 {
 	unsigned int temp;
 	int i, count = 0;
 	static int first_call = 1;
+	struct kbase_device *kbdev = (struct kbase_device *)dev;
 	struct exynos_context * platform = (struct exynos_context *)kbdev->platform_context;
 
 	/* G3D_CFGREG0 register bit16 set to 1.
 	 * It is change the blkg3d clock path from aclk-g3d to aclk-g3d feed-back path */
+	GPU_LOG(DVFS_INFO, LSI_EXYNOS_GPU_INIT_HW, 0u, 0u, "LSI_EXYNOS_GPU_INIT_HW\n");
 	temp = __raw_readl(EXYNOS7420_VA_SYSREG + 0x1234);
 	temp |= (0x1 << 16);
 	__raw_writel(temp, EXYNOS7420_VA_SYSREG + 0x1234);
@@ -413,7 +432,7 @@
 	}
 	else {
 		do {
-			if(balance_init(kbdev) == MALI_TRUE)
+			if(balance_init(kbdev) == true)
 				break;
 			if(count == 5)
 				break;
diff -Nur r5p0/platform/gpu_balance.h r15p0/platform/exynos/gpu_balance.h
--- r5p0/platform/gpu_balance.h	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_balance.h	2017-07-20 16:14:04.620559419 +0200
@@ -62,15 +62,15 @@
 #define MALI_WRITE_REG(reg, val) kbase_os_reg_write(kbdev, reg, val)
 
 #define MALI_GPU_CONTROL_WAIT(reg, val, timeout) \
-	if(mali_wait_reg(kbdev, GPU_CONTROL_REG(reg), val, timeout) != MALI_TRUE) \
+	if(mali_wait_reg(kbdev, GPU_CONTROL_REG(reg), val, timeout) != true) \
 	{\
-		return MALI_FALSE;\
+		return false;\
 	}
 
 #define MALI_JOB_IRQ_WAIT(val, timeout) \
-	if(mali_wait_reg(kbdev, JOB_IRQ_RAWSTAT, val, timeout) != MALI_TRUE) \
+	if(mali_wait_reg(kbdev, JOB_IRQ_RAWSTAT, val, timeout) != true) \
 	{\
-		return MALI_FALSE;\
+		return false;\
 	}
 
 #define MALI_WRITE_MEM(dst, val) \
@@ -102,7 +102,7 @@
 		count++;
 	} while(1);
 
-	return MALI_TRUE;
+	return true;
 }
 
 typedef struct _checksum_block{
diff -Nur r5p0/platform/gpu_control.c r15p0/platform/exynos/gpu_control.c
--- r5p0/platform/gpu_control.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_control.c	2017-07-20 16:14:04.000000000 +0200
@@ -17,8 +17,23 @@
 
 #include <mali_kbase.h>
 
+#include <linux/of_device.h>
 #include <linux/pm_qos.h>
+#include <linux/pm_domain.h>
+#include <linux/clk.h>
 #include <mach/pm_domains.h>
+#if defined(CONFIG_SOC_EXYNOS8890) && defined(CONFIG_PWRCAL)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
+#include <../pwrcal/pwrcal.h>
+#include <../pwrcal/S5E8890/S5E8890-vclk.h>
+#include <mach/pm_domains-cal.h>
+#else
+#include <../../../../../soc/samsung/pwrcal/pwrcal.h>
+#include <../../../../../soc/samsung/pwrcal/S5E8890/S5E8890-vclk.h>
+#include <../../../../../soc/samsung/pwrcal/S5E8890/S5E8890-vclk-internal.h>
+#include <soc/samsung/pm_domains-cal.h>
+#endif /* LINUX_VERSION */
+#endif /* CONFIG_SOC_EXYNOS8890 && CONFIG_PWRCAL */
 
 #include "mali_kbase_platform.h"
 #include "gpu_dvfs_handler.h"
@@ -50,6 +65,7 @@
 }
 #endif /* CONFIG_MALI_RT_PM */
 
+#ifdef CONFIG_SOC_EXYNOS7420
 int get_cpu_clock_speed(u32 *cpu_clock)
 {
 	struct clk *cpu_clk;
@@ -61,6 +77,7 @@
 	*cpu_clock = (freq/MHZ);
 	return 0;
 }
+#endif
 
 int gpu_control_set_voltage(struct kbase_device *kbdev, int voltage)
 {
@@ -116,11 +133,12 @@
 			"%s: can't set clock in the dvs mode (requested clock %d)\n", __func__, clock);
 		return 0;
 	}
-
+#ifdef CONFIG_MALI_DVFS
 	if (gpu_dvfs_get_level(clock) < 0) {
 		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: mismatch clock error (%d)\n", __func__, clock);
 		return -1;
 	}
+#endif
 
 	is_up = prev_clock < clock;
 
@@ -182,9 +200,15 @@
 		ret = ctr_ops->disable_clock(platform);
 	mutex_unlock(&platform->gpu_clock_lock);
 
+#ifdef CONFIG_MALI_SEC_HWCNT
+	dvfs_hwcnt_clear_tripipe(kbdev);
+#endif
 	gpu_dvfs_update_time_in_state(platform->cur_clock);
 #ifdef CONFIG_MALI_DVFS
 	gpu_pm_qos_command(platform, GPU_CONTROL_PM_QOS_RESET);
+#ifdef CONFIG_MALI_DVFS_USER
+	proactive_pm_qos_command(platform, GPU_CONTROL_PM_QOS_RESET);
+#endif
 #endif /* CONFIG_MALI_DVFS */
 
 	return ret;
@@ -222,9 +246,17 @@
 
 	if (ctr_ops->set_clock_to_osc)
 		ctr_ops->set_clock_to_osc(platform);
-	ret = gpu_enable_dvs(platform);
+
+#ifdef CONFIG_MALI_SEC_HWCNT
+	mutex_lock(&kbdev->hwcnt.dvs_lock);
+#endif
 
 	platform->dvs_is_enabled = true;
+	ret = gpu_enable_dvs(platform);
+
+#ifdef CONFIG_MALI_SEC_HWCNT
+	mutex_unlock(&kbdev->hwcnt.dvs_lock);
+#endif
 
 	mutex_unlock(&platform->gpu_clock_lock);
 #endif /* CONFIG_REGULATOR */
diff -Nur r5p0/platform/gpu_control.h r15p0/platform/exynos/gpu_control.h
--- r5p0/platform/gpu_control.h	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_control.h	2017-07-20 16:14:04.620559419 +0200
@@ -36,6 +36,9 @@
 
 int get_cpu_clock_speed(u32 *cpu_clock);
 int gpu_control_set_voltage(struct kbase_device *kbdev, int voltage);
+#if defined (CONFIG_SOC_EXYNOS8890)
+int gpu_control_set_m_voltage(struct kbase_device *kbdev, int clk);
+#endif
 int gpu_control_set_clock(struct kbase_device *kbdev, int clock);
 int gpu_control_enable_clock(struct kbase_device *kbdev);
 int gpu_control_disable_clock(struct kbase_device *kbdev);
@@ -61,9 +64,9 @@
 int gpu_control_module_init(struct kbase_device *kbdev);
 void gpu_control_module_term(struct kbase_device *kbdev);
 
-
 int gpu_exynos7420_set_rate(struct kbase_device *kbdev, int clk);
 void gpu_exynos7420_clock_disable(struct kbase_device *kbdev);
+int gpu_device_specific_init(struct kbase_device *kbdev);
 
 int *get_mif_table(int *size);
 #endif /* _GPU_CONTROL_H_ */
diff -Nur r5p0/platform/gpu_custom_interface.c r15p0/platform/exynos/gpu_custom_interface.c
--- r5p0/platform/gpu_custom_interface.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_custom_interface.c	2017-07-20 16:14:04.620559419 +0200
@@ -1188,7 +1188,7 @@
 }
 #endif
 
-#ifdef MALI_SEC_HWCNT
+#ifdef CONFIG_MALI_SEC_HWCNT
 static ssize_t show_hwcnt_dvfs(struct device *dev, struct device_attribute *attr, char *buf)
 {
 	ssize_t ret = 0;
@@ -1237,7 +1237,7 @@
 
 	if (sysfs_streq("0", buf)) {
 		platform->hwcnt_gathering_status = false;
-		platform->hwcnt_bt_clk = FALSE;
+		platform->hwcnt_bt_clk = false;
 	} else if (sysfs_streq("1", buf))
 		platform->hwcnt_gathering_status = true;
 	else
@@ -1293,6 +1293,7 @@
 
 	if (sysfs_streq("0", buf)) {
 		platform->hwcnt_gpr_status = false;
+		kbdev->hwcnt.is_hwcnt_gpr_enable = false;
 	} else if (sysfs_streq("1", buf))
 		platform->hwcnt_gpr_status = true;
 	else
@@ -1302,6 +1303,58 @@
 	return count;
 }
 
+static ssize_t show_hwcnt_profile(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	ssize_t ret = 0;
+	struct kbase_device *kbdev;
+	struct exynos_context *platform;
+
+	kbdev = dev_get_drvdata(dev);
+	if (!kbdev)
+		return -ENODEV;
+
+	platform = (struct exynos_context *)kbdev->platform_context;
+	if (!platform)
+		return -ENODEV;
+
+	ret += snprintf(buf+ret, PAGE_SIZE-ret, "%d", platform->hwcnt_profile);
+
+	if (ret < PAGE_SIZE - 1) {
+		ret += snprintf(buf+ret, PAGE_SIZE-ret, "\n");
+	} else {
+		buf[PAGE_SIZE-2] = '\n';
+		buf[PAGE_SIZE-1] = '\0';
+		ret = PAGE_SIZE-1;
+	}
+	return ret;
+}
+
+static ssize_t set_hwcnt_profile(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct kbase_device *kbdev;
+	struct exynos_context *platform;
+
+	kbdev = dev_get_drvdata(dev);
+	if (!kbdev)
+		return -ENODEV;
+
+	platform = (struct exynos_context *)kbdev->platform_context;
+	if (!platform)
+		return -ENODEV;
+
+	mutex_lock(&kbdev->hwcnt.mlock);
+
+	if (sysfs_streq("0", buf))
+		platform->hwcnt_profile = false;
+	else if (sysfs_streq("1", buf))
+		platform->hwcnt_profile = true;
+	else
+		GPU_LOG(DVFS_WARNING, DUMMY, 0u, 0u, "invalid val -only [0 or 1] is accepted\n");
+
+	mutex_unlock(&kbdev->hwcnt.mlock);
+	return count;
+}
+
 static ssize_t show_hwcnt_bt_state(struct device *dev, struct device_attribute *attr, char *buf)
 {
 	ssize_t ret = 0;
@@ -1338,8 +1391,8 @@
 	if (!platform)
 		return -ENODEV;
 
-	ret += snprintf(buf+ret, PAGE_SIZE-ret, "%d : (arith, ls, tex) = (%d, %d, %d)",
-		platform->hwcnt_gathering_status, kbdev->hwcnt.resources.arith_words, kbdev->hwcnt.resources.ls_issues, kbdev->hwcnt.resources.tex_issues);
+	ret += snprintf(buf+ret, PAGE_SIZE-ret, "%d : (active, arith, ls, tex) = (%llu, %llu, %llu, %llu)",
+		platform->hwcnt_gathering_status, kbdev->hwcnt.resources_log.tripipe_active, kbdev->hwcnt.resources_log.arith_words, kbdev->hwcnt.resources_log.ls_issues, kbdev->hwcnt.resources_log.tex_issues);
 
 	if (ret < PAGE_SIZE - 1) {
 		ret += snprintf(buf+ret, PAGE_SIZE-ret, "\n");
@@ -1348,12 +1401,16 @@
 		buf[PAGE_SIZE-1] = '\0';
 		ret = PAGE_SIZE-1;
 	}
+	kbdev->hwcnt.resources_log.tripipe_active = 0;
+	kbdev->hwcnt.resources_log.arith_words = 0;
+	kbdev->hwcnt.resources_log.ls_issues = 0;
+	kbdev->hwcnt.resources_log.tex_issues = 0;
+
 #else
 	GPU_LOG(DVFS_WARNING, DUMMY, 0u, 0u, "G3D DVFS build config is disabled. You can not see\n");
 #endif /* CONFIG_MALI_DVFS */
 	return ret;
 }
-
 #endif
 
 static int gpu_get_status(struct exynos_context *platform, char *buf, size_t buf_size)
@@ -1438,11 +1495,12 @@
 #ifdef DEBUG_FBDEV
 DEVICE_ATTR(fbdev, S_IRUGO, show_fbdev, NULL);
 #endif
-#ifdef MALI_SEC_HWCNT
+#ifdef CONFIG_MALI_SEC_HWCNT
 DEVICE_ATTR(hwcnt_dvfs, S_IRUGO|S_IWUSR, show_hwcnt_dvfs, set_hwcnt_dvfs);
 DEVICE_ATTR(hwcnt_gpr, S_IRUGO|S_IWUSR, show_hwcnt_gpr, set_hwcnt_gpr);
 DEVICE_ATTR(hwcnt_bt_state, S_IRUGO, show_hwcnt_bt_state, NULL);
 DEVICE_ATTR(hwcnt_tripipe, S_IRUGO, show_hwcnt_tripipe, NULL);
+DEVICE_ATTR(hwcnt_profile, S_IRUGO|S_IWUSR, show_hwcnt_profile, set_hwcnt_profile);
 #endif
 DEVICE_ATTR(gpu_status, S_IRUGO, show_gpu_status, NULL);
 
@@ -1586,7 +1644,7 @@
 	}
 #endif
 
-#ifdef MALI_SEC_HWCNT
+#ifdef CONFIG_MALI_SEC_HWCNT
 	if (device_create_file(dev, &dev_attr_hwcnt_dvfs)) {
 		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u,"Couldn't create sysfs file [hwcnt_dvfs]\n");
 		goto out;
@@ -1606,6 +1664,11 @@
 		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "couldn't create sysfs file [hwcnt_tripipe]\n");
 		goto out;
 	}
+
+	if (device_create_file(dev, &dev_attr_hwcnt_profile)) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "Couldn't create sysfs file [hwcnt_profile]\n");
+		goto out;
+	}
 #endif
 
 	if (device_create_file(dev, &dev_attr_gpu_status)) {
@@ -1655,11 +1718,12 @@
 #ifdef DEBUG_FBDEV
 	device_remove_file(dev, &dev_attr_fbdev);
 #endif
-#ifdef MALI_SEC_HWCNT
+#ifdef CONFIG_MALI_SEC_HWCNT
 	device_remove_file(dev, &dev_attr_hwcnt_dvfs);
 	device_remove_file(dev, &dev_attr_hwcnt_gpr);
 	device_remove_file(dev, &dev_attr_hwcnt_bt_state);
 	device_remove_file(dev, &dev_attr_hwcnt_tripipe);
+	device_remove_file(dev, &dev_attr_hwcnt_profile);
 #endif
 	device_remove_file(dev, &dev_attr_gpu_status);
 }
diff -Nur r5p0/platform/gpu_dvfs_api.c r15p0/platform/exynos/gpu_dvfs_api.c
--- r5p0/platform/gpu_dvfs_api.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_dvfs_api.c	2017-07-20 16:14:04.000000000 +0200
@@ -143,7 +143,6 @@
 	}
 
 	target_vol = MAX(gpu_dvfs_get_voltage(target_clk) + platform->voltage_margin, platform->cold_min_vol);
-	target_vol = target_vol < (int) platform->table[0].voltage ? target_vol : (int) platform->table[0].voltage;
 
 	prev_clk = gpu_get_cur_clock(platform);
 
@@ -163,7 +162,7 @@
 	mutex_unlock(&platform->gpu_clock_lock);
 
 	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "clk[%d -> %d], vol[%d (margin : %d)]\n",
-		prev_clk, gpu_get_cur_clock(platform), gpu_get_cur_voltage(platform), platform->voltage_margin);
+			prev_clk, gpu_get_cur_clock(platform), gpu_get_cur_voltage(platform), platform->voltage_margin);
 
 	return ret;
 }
@@ -189,7 +188,6 @@
 	}
 
 	target_vol = MAX(gpu_dvfs_get_voltage(target_clk) + platform->voltage_margin, platform->cold_min_vol);
-	target_vol = target_vol < (int) platform->table[0].voltage ? target_vol : (int) platform->table[0].voltage;
 
 	prev_clk = gpu_get_cur_clock(platform);
 #ifdef CONFIG_EXYNOS_CL_DVFS_G3D
@@ -205,7 +203,7 @@
 		exynos7420_cl_dvfs_start(ID_G3D);
 #endif
 	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "pending clk[%d -> %d], vol[%d (margin : %d)]\n",
-		prev_clk, gpu_get_cur_clock(platform), gpu_get_cur_voltage(platform), platform->voltage_margin);
+			prev_clk, gpu_get_cur_clock(platform), gpu_get_cur_voltage(platform), platform->voltage_margin);
 
 	return ret;
 }
@@ -393,32 +391,25 @@
 		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: DVFS is disabled\n", __func__);
 		return;
 	}
-
-	if (kbdev->pm.metrics.timer_active && !enable) {
-/* MALI_SEC_INTEGRATION */
-#if 0
-		hrtimer_cancel(&kbdev->pm.metrics.timer);
-#else
+#ifdef CONFIG_MALI_DVFS_USER_GOVERNOR
+	if (platform->udvfs_enable)
+		return;
+#endif
+	if (kbdev->pm.backend.metrics.timer_active && !enable) {
 		cancel_delayed_work(platform->delayed_work);
 		flush_workqueue(platform->dvfs_wq);
-#endif
-	} else if (!kbdev->pm.metrics.timer_active && enable) {
-/* MALI_SEC_INTEGRATION */
-#if 0
-		hrtimer_start(&kbdev->pm.metrics.timer, HR_TIMER_DELAY_MSEC(platform->polling_speed), HRTIMER_MODE_REL);
-#else
+	} else if (!kbdev->pm.backend.metrics.timer_active && enable) {
 		queue_delayed_work_on(0, platform->dvfs_wq,
 				platform->delayed_work, msecs_to_jiffies(platform->polling_speed));
-#endif
 		spin_lock_irqsave(&platform->gpu_dvfs_spinlock, flags);
 		platform->down_requirement = platform->table[platform->step].down_staycount;
 		platform->interactive.delay_count = 0;
 		spin_unlock_irqrestore(&platform->gpu_dvfs_spinlock, flags);
 	}
 
-	spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
-	kbdev->pm.metrics.timer_active = enable;
-	spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+	spin_lock_irqsave(&kbdev->pm.backend.metrics.lock, flags);
+	kbdev->pm.backend.metrics.timer_active = enable;
+	spin_unlock_irqrestore(&kbdev->pm.backend.metrics.lock, flags);
 }
 
 int gpu_dvfs_on_off(bool enable)
@@ -484,7 +475,7 @@
 
 int gpu_dvfs_update_time_in_state(int clock)
 {
-#ifdef CONFIG_MALI_DEBUG_SYS
+#if defined(CONFIG_MALI_DEBUG_SYS) && defined(CONFIG_MALI_DVFS)
 	struct kbase_device *kbdev = pkbdev;
 	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
 
@@ -526,6 +517,24 @@
 	return -1;
 }
 
+int gpu_dvfs_get_level_clock(int clock)
+{
+	struct kbase_device *kbdev = pkbdev;
+	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
+	int i, min, max;
+
+	DVFS_ASSERT(platform);
+
+	min = gpu_dvfs_get_level(platform->gpu_min_clock);
+	max = gpu_dvfs_get_level(platform->gpu_max_clock);
+
+	for (i = max; i <= min; i++)
+		if (clock - (int)(platform->table[i].clock) >= 0)
+			return platform->table[i].clock;
+
+	return -1;
+}
+
 int gpu_dvfs_get_voltage(int clock)
 {
 	struct kbase_device *kbdev = pkbdev;
@@ -577,3 +586,593 @@
 
 	return platform->table_size;
 }
+
+#ifdef CONFIG_MALI_DVFS_USER
+#define JOB_GET_SIZE_VAL 0xFFFFFFFF
+static bool gpu_dvfs_check_valid_job(gpu_dvfs_job *job)
+{
+	struct kbase_device *kbdev = pkbdev;
+	struct exynos_context *platform;
+	bool valid = true;
+#ifdef CONFIG_PWRCAL
+	struct dvfs_rate_volt rate_volt[48];
+	int table_size;
+#endif
+
+	platform = kbdev ? (struct exynos_context *) kbdev->platform_context:NULL;
+	if (platform == NULL)
+		return false;
+
+	switch(job->type)
+	{
+		case DVFS_REQ_GET_DVFS_TABLE:
+			if (job->data_size == JOB_GET_SIZE_VAL)
+				break;
+			if (job->data_size != (platform->table_size * sizeof(gpu_dvfs_info))) {
+				valid = false;
+			}
+			break;
+		case DVFS_REQ_GET_DVFS_ATTR:
+			if (job->data_size == JOB_GET_SIZE_VAL)
+				break;
+			if (job->data_size != gpu_get_config_attr_size()) {
+				valid = false;
+			}
+			break;
+		case DVFS_REQ_HWC_DUMP:
+			if (job->data_size != sizeof(gpu_dvfs_hwc_data)) {
+				valid = false;
+			}
+			break;
+		case DVFS_REQ_HWC_SETUP:
+			if (job->data_size != sizeof(gpu_dvfs_hwc_setup)) {
+				valid = false;
+			}
+			break;
+		case DVFS_REQ_GET_UTILIZATION:
+			if (job->data_size != sizeof(gpu_dvfs_env_data)) {
+				valid = false;
+			}
+			break;
+		case DVFS_REQ_GET_MIF_TABLE:
+#ifdef CONFIG_PWRCAL
+			if (job->data_size == JOB_GET_SIZE_VAL)
+				break;
+			table_size = cal_dfs_get_rate_asv_table(dvfs_mif, rate_volt);
+			if (job->data_size != sizeof(unsigned int) * table_size) {
+				GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to get MIF Table size\n", __func__);
+				valid = false;
+			}
+#else
+			valid = false;
+#endif
+			break;
+		case DVFS_REQ_GET_INT_TABLE:
+#ifdef CONFIG_PWRCAL
+			if (job->data_size == JOB_GET_SIZE_VAL)
+				break;
+			table_size = cal_dfs_get_rate_asv_table(dvfs_int, rate_volt);
+			if (job->data_size != sizeof(unsigned int) * table_size) {
+				GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to get INT Table size\n", __func__);
+				valid = false;
+			}
+#else
+			valid = false;
+#endif
+			break;
+		case DVFS_REQ_GET_ATLAS_TABLE:
+#ifdef CONFIG_PWRCAL
+			if (job->data_size == JOB_GET_SIZE_VAL)
+				break;
+			table_size = cal_dfs_get_rate_asv_table(dvfs_big, rate_volt);
+			if (job->data_size != sizeof(unsigned int) * table_size) {
+				GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to get Big Table size\n", __func__);
+				valid = false;
+			}
+#else
+			valid = false;
+#endif
+			break;
+		case DVFS_REQ_GET_APOLLO_TABLE:
+#ifdef CONFIG_PWRCAL
+			if (job->data_size == JOB_GET_SIZE_VAL)
+				break;
+			table_size = cal_dfs_get_rate_asv_table(dvfs_little, rate_volt);
+			if (job->data_size != sizeof(unsigned int) * table_size) {
+				GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to get Little Table size\n", __func__);
+				valid = false;
+			}
+#else
+			valid = false;
+#endif
+			break;
+		case DVFS_REQ_SET_LEVEL:
+		case DVFS_REQ_GET_LEVEL:
+		case DVFS_REQ_GET_GPU_MIN_LOCK:
+		case DVFS_REQ_SET_GPU_MIN_LOCK:
+		case DVFS_REQ_GET_GPU_MAX_LOCK:
+		case DVFS_REQ_SET_GPU_MAX_LOCK:
+		case DVFS_REQ_GET_ATLAS_MIN_LOCK:
+		case DVFS_REQ_SET_ATLAS_MIN_LOCK:
+		case DVFS_REQ_GET_APOLLO_MIN_LOCK:
+		case DVFS_REQ_SET_APOLLO_MIN_LOCK:
+		case DVFS_REQ_GET_MIF_MIN_LOCK:
+		case DVFS_REQ_SET_MIF_MIN_LOCK:
+		case DVFS_REQ_GET_INT_MIN_LOCK:
+		case DVFS_REQ_SET_INT_MIN_LOCK:
+			if (job->data_size != sizeof(int)) {
+				valid = false;
+			}
+			break;
+		case DVFS_REQ_REGISTER_CTX:
+			break;
+		default:
+			valid = false;
+			break;
+	}
+	return valid;
+}
+#ifdef CONFIG_MALI_DVFS_USER_GOVERNOR
+static inline void gpu_dvfs_notify_info(base_jd_event_code event)
+{
+	struct kbase_device *kbdev = pkbdev;
+	struct kbase_jd_atom *katom;
+	struct exynos_context *platform;
+	struct kbase_jd_context *jctx;
+	struct kbase_context *dvfs_kctx;
+
+	platform = kbdev ? (struct exynos_context *) kbdev->platform_context:NULL;
+	if (platform == NULL)
+		return;
+
+	dvfs_kctx = platform->dvfs_kctx;
+	if (dvfs_kctx == NULL)
+		return;
+
+	if (platform->udvfs_enable == false)
+		return;
+
+	jctx = &dvfs_kctx->jctx;
+	katom = &jctx->atoms[platform->atom_idx++];
+	if (platform->atom_idx == DVFS_USER_NOTIFIER_ATOM_NUMBER_MAX)
+		platform->atom_idx = DVFS_USER_NOTIFIER_ATOM_NUMBER_BASE;
+
+	katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+	katom->kctx = dvfs_kctx;
+	katom->extres = NULL;
+	katom->coreref_state = KBASE_ATOM_COREREF_STATE_NO_CORES_REQUESTED;
+	katom->core_req = BASE_JD_REQ_SOFT_DVFS;
+	katom->event_code = event;
+	kbase_event_post(platform->dvfs_kctx, katom);
+	return;
+}
+
+void gpu_dvfs_check_destroy_context(struct kbase_context *kctx)
+{
+	struct kbase_device *kbdev = pkbdev;
+	struct exynos_context *platform;
+
+	platform = kbdev ? (struct exynos_context *) kbdev->platform_context:NULL;
+	if (platform == NULL)
+		return;
+
+	mutex_lock(&platform->gpu_process_job_lock);
+	if (platform->dvfs_kctx == kctx)
+	{
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u,"gpu_dvfs_check_destroy_context %p\n", kctx);
+		platform->dvfs_kctx = NULL;
+		kfree(platform->mif_table);
+		kfree(platform->int_table);
+		kfree(platform->atlas_table);
+		kfree(platform->apollo_table);
+	}
+	mutex_unlock(&platform->gpu_process_job_lock);
+}
+
+
+void gpu_dvfs_notify_poweroff(void)
+{
+	GPU_LOG(DVFS_DEBUG, DUMMY, 0u, 0u,"gpu_dvfs_notify_poweroff\n");
+	gpu_dvfs_notify_info(BASE_JD_EVENT_DVFS_INFO_POWER_OFF);
+	return;
+}
+
+void gpu_dvfs_notify_poweron(void)
+{
+	GPU_LOG(DVFS_DEBUG, DUMMY, 0u, 0u,"gpu_dvfs_notify_poweron\n");
+	gpu_dvfs_notify_info(BASE_JD_EVENT_DVFS_INFO_POWER_ON);
+	return;
+}
+#endif
+static void __user *
+get_compat_pointer(struct kbase_context *kctx, const union kbase_pointer *p)
+{
+#ifdef CONFIG_COMPAT
+	if (kctx->is_compat)
+		return compat_ptr(p->compat_value);
+	else
+#endif
+		return p->value;
+}
+
+bool gpu_dvfs_process_job(void *pkatom)
+{
+	struct kbase_jd_atom *katom = (struct kbase_jd_atom *)pkatom;
+	struct kbase_device *kbdev = pkbdev;
+	struct exynos_context *platform;
+	gpu_dvfs_job dvfs_job;
+	gpu_dvfs_job *job;
+	void __user *data;
+	void __user *job_addr;
+	int clock, cur_clock, i;
+	int level, step;
+	unsigned int ret_val = 0;
+
+	platform = kbdev ? (struct exynos_context *) kbdev->platform_context:NULL;
+	if (platform == NULL)
+		return false;
+
+	mutex_lock(&platform->gpu_process_job_lock);
+
+	job = &dvfs_job;
+
+	job_addr = get_compat_pointer(katom->kctx, (union kbase_pointer *)&katom->jc);
+	if (copy_from_user(&dvfs_job, job_addr, sizeof(gpu_dvfs_job)) != 0)
+		goto out;
+
+	data = (gpu_dvfs_job __user *)get_compat_pointer(katom->kctx, (union kbase_pointer *)&job->data);
+
+	job->event = DVFS_JOB_EVENT_ERROR;
+	katom->core_req |= BASEP_JD_REQ_EVENT_NEVER;
+
+	if (job->data_size && data == NULL) {
+		mutex_unlock(&platform->gpu_process_job_lock);
+		return false;
+	}
+
+	if (gpu_dvfs_check_valid_job(job) == false) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "gpu_dvfs_check_valid_job %d, %u\n", job->type, job->data_size);
+		mutex_unlock(&platform->gpu_process_job_lock);
+		return false;
+	}
+
+	job->event = DVFS_JOB_EVENT_DONE;
+
+	switch(job->type)
+	{
+		case DVFS_REQ_REGISTER_CTX:
+			if (platform->dvfs_kctx == NULL) {
+				platform->atom_idx = DVFS_USER_NOTIFIER_ATOM_NUMBER_BASE;
+			}
+			platform->dvfs_kctx = katom->kctx;
+			update_cal_table();
+			GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "DVFS_REQ_REGISTER_CTX 0x%p\n", katom->kctx);
+			break;
+		case DVFS_REQ_GET_DVFS_TABLE:
+			if (job->data_size == JOB_GET_SIZE_VAL) {
+				ret_val = platform->table_size * sizeof(gpu_dvfs_info);
+				if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+					goto out;
+				break;
+			}
+			if (copy_to_user(data,  platform->table, job->data_size) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_GET_DVFS_ATTR:
+			if (job->data_size == JOB_GET_SIZE_VAL) {
+				ret_val = gpu_get_config_attr_size();
+				if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+					goto out;
+				break;
+			}
+			if (copy_to_user(data,  gpu_get_config_attributes(), job->data_size) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_GET_UTILIZATION:
+			gpu_dvfs_calculate_env_data(pkbdev);
+			mutex_unlock(&kbdev->pm.lock);
+			if (copy_to_user(data, &platform->env_data, job->data_size) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_GET_MIF_TABLE:
+#ifdef CONFIG_PWRCAL
+			if (job->data_size == JOB_GET_SIZE_VAL) {
+				ret_val = sizeof(unsigned int) * platform->mif_table_size;
+				if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+					goto out;
+				break;
+			}
+			for (i = 0; i < platform->mif_table_size; i++) {
+				ret_val = platform->mif_table[i];
+				if (copy_to_user(&((unsigned int*)data)[i], &ret_val, sizeof(int)) != 0)
+					goto out;
+			}
+#else
+			goto out;
+#endif
+			break;
+		case DVFS_REQ_GET_INT_TABLE:
+#ifdef CONFIG_PWRCAL
+			if (job->data_size == JOB_GET_SIZE_VAL) {
+				ret_val = sizeof(unsigned int) * platform->int_table_size;
+				if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+					goto out;
+				break;
+			}
+			for (i = 0; i < platform->int_table_size; i++) {
+				ret_val = platform->int_table[i];
+				if (copy_to_user(&((unsigned int*)data)[i], &ret_val, sizeof(int)) != 0)
+					goto out;
+			}
+#else
+			goto out;
+#endif
+			break;
+		case DVFS_REQ_GET_ATLAS_TABLE:
+#ifdef CONFIG_PWRCAL
+			if (job->data_size == JOB_GET_SIZE_VAL) {
+				ret_val = sizeof(unsigned int) * platform->atlas_table_size;
+				if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+					goto out;
+				break;
+			}
+			for (i = 0; i < platform->atlas_table_size; i++) {
+				ret_val = platform->atlas_table[i];
+				if (copy_to_user(&((unsigned int*)data)[i], &ret_val, sizeof(int)) != 0)
+					goto out;
+			}
+#else
+			goto out;
+#endif
+			break;
+		case DVFS_REQ_GET_APOLLO_TABLE:
+#ifdef CONFIG_PWRCAL
+			if (job->data_size == JOB_GET_SIZE_VAL) {
+				ret_val = sizeof(unsigned int) * platform->apollo_table_size;
+				if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+					goto out;
+				break;
+			}
+			for (i = 0; i < platform->apollo_table_size; i++) {
+				ret_val = platform->apollo_table[i];
+				if (copy_to_user(&((unsigned int*)data)[i], &ret_val, sizeof(int)) != 0)
+					goto out;
+			}
+#else
+			goto out;
+#endif
+			break;
+		case DVFS_REQ_SET_LEVEL:
+			if (copy_from_user(&level, data, sizeof(int)) != 0)
+				goto out;
+			clock = gpu_dvfs_get_clock(level);
+			gpu_set_target_clk_vol(clock, true);
+			cur_clock = platform->cur_clock;
+
+			/* set clock successfully */
+			if (clock != cur_clock) {
+				if ((platform->max_lock > 0) || (platform->min_lock > 0)) {
+					job->event = DVFS_JOB_EVENT_LOCKED;
+				} else {
+					job->event = DVFS_JOB_EVENT_ERROR;
+				}
+			}
+
+			ret_val = gpu_dvfs_get_level(cur_clock);
+			if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_GET_GPU_MIN_LOCK:
+			step = gpu_dvfs_get_level(platform->min_lock);
+			if(step == -1)
+				ret_val = gpu_dvfs_get_level(platform->gpu_min_clock);
+			else
+				ret_val = step;
+			if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+				goto out;
+
+			break;
+		case DVFS_REQ_SET_GPU_MIN_LOCK:
+			if (copy_from_user(&level, data, sizeof(int)) != 0)
+				goto out;
+
+			if(level == -1)
+			{
+				gpu_dvfs_clock_lock(GPU_DVFS_MIN_UNLOCK, USER_LOCK, 0);
+				ret_val = gpu_dvfs_get_level(platform->cur_clock);
+				if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+					goto out;
+				break;
+			}
+
+			clock = gpu_dvfs_get_clock(level);
+			if ((clock < platform->gpu_min_clock) || (clock > platform->gpu_max_clock)) {
+				GPU_LOG(DVFS_WARNING, DUMMY, 0u, 0u, "%s: invalid clock value (%d)\n", __func__, clock);
+				mutex_unlock(&platform->gpu_process_job_lock);
+				return -ENOENT;
+			}
+
+			if (clock > platform->gpu_max_clock)
+				clock = platform->gpu_max_clock;
+
+			if ((clock == platform->gpu_min_clock) || clock == 0)
+				gpu_dvfs_clock_lock(GPU_DVFS_MIN_UNLOCK, USER_LOCK, 0);
+			else
+				gpu_dvfs_clock_lock(GPU_DVFS_MIN_LOCK, USER_LOCK, clock);
+
+			ret_val = gpu_dvfs_get_level(clock);
+			if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_GET_GPU_MAX_LOCK:
+			ret_val = gpu_dvfs_get_level(platform->gpu_max_clock);
+			if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_SET_GPU_MAX_LOCK:
+			if (copy_from_user(&level, data, sizeof(int)) != 0)
+				goto out;
+			if(level == -1)
+			{
+				gpu_dvfs_clock_lock(GPU_DVFS_MAX_UNLOCK, USER_LOCK, 0);
+				ret_val = gpu_dvfs_get_level(platform->cur_clock);
+				if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+					goto out;
+				break;
+			}
+
+			clock = gpu_dvfs_get_clock(level);
+			if ((clock > platform->gpu_max_clock) || (clock < platform->gpu_min_clock)) {
+				GPU_LOG(DVFS_WARNING, DUMMY, 0u, 0u, "%s: invalid clock value (%d)\n", __func__, clock);
+				mutex_unlock(&platform->gpu_process_job_lock);
+				return -ENOENT;
+			}
+
+			if ((clock == platform->gpu_max_clock) || clock == 0)
+				gpu_dvfs_clock_lock(GPU_DVFS_MAX_UNLOCK, USER_LOCK, 0);
+			else
+				gpu_dvfs_clock_lock(GPU_DVFS_MAX_LOCK, USER_LOCK, clock);
+
+			ret_val = gpu_dvfs_get_level(clock);
+			if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_GET_ATLAS_MIN_LOCK:
+			ret_val = platform->apollo_min_step;
+			if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_SET_ATLAS_MIN_LOCK:
+			if (copy_from_user(&step, data, sizeof(int)) != 0)
+				goto out;
+			platform->atlas_min_step = step;
+			gpu_atlas_min_pmqos(platform, step == -1 ? 0 : platform->atlas_min_step);
+			break;
+		case DVFS_REQ_GET_APOLLO_MIN_LOCK:
+			ret_val = platform->apollo_min_step;
+			if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_SET_APOLLO_MIN_LOCK:
+			if (copy_from_user(&step, data, sizeof(int)) != 0)
+				goto out;
+			platform->apollo_min_step = step;
+			gpu_apollo_min_pmqos(platform, step == -1 ? 0 : platform->apollo_min_step);
+			break;
+		case DVFS_REQ_GET_MIF_MIN_LOCK:
+			ret_val = platform->mif_min_step;
+			if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_SET_MIF_MIN_LOCK:
+			if (copy_from_user(&step, data, sizeof(int)) != 0)
+				goto out;
+			platform->mif_min_step = step;
+			gpu_mif_min_pmqos(platform, step == -1 ? 0 : platform->mif_min_step);
+			break;
+		case DVFS_REQ_GET_INT_MIN_LOCK:
+			ret_val = platform->int_min_step;
+			if (copy_to_user(data, &ret_val, sizeof(int)) != 0)
+				goto out;
+			break;
+		case DVFS_REQ_SET_INT_MIN_LOCK:
+			if (copy_from_user(&step, data, sizeof(int)) != 0)
+				goto out;
+			platform->int_min_step = step;
+			gpu_int_min_pmqos(platform, step == -1 ? 0 : platform->int_min_step);
+			break;
+		case DVFS_REQ_HWC_SETUP:
+		{
+			gpu_dvfs_hwc_setup hwc_setup;
+			if (copy_from_user(&hwc_setup, data, sizeof(gpu_dvfs_hwc_setup)) != 0)
+				goto out;
+			printk("DVFS_REQ_HWC_SETUP !!! %d\n", hwc_setup.jm_bm);
+#ifdef CONFIG_MALI_DVFS_USER_GOVERNOR
+			if (hwc_setup.profile_mode)
+				gpu_dvfs_notify_info(BASE_JD_EVENT_DVFS_INFO_PROFILE_MODE_ON);
+			else
+				gpu_dvfs_notify_info(BASE_JD_EVENT_DVFS_INFO_PROFILE_MODE_OFF);
+#endif
+			mutex_lock(&kbdev->pm.lock);
+#ifdef CONFIG_MALI_SEC_HWCNT
+			platform->hwcnt_choose_jm = kbdev->hwcnt.suspended_state.jm_bm = hwc_setup.jm_bm;
+			platform->hwcnt_choose_tiler = kbdev->hwcnt.suspended_state.tiler_bm = hwc_setup.tiler_bm;
+			platform->hwcnt_choose_shader = kbdev->hwcnt.suspended_state.shader_bm = hwc_setup.sc_bm;
+			platform->hwcnt_choose_mmu_l2 = kbdev->hwcnt.suspended_state.mmu_l2_bm = hwc_setup.memory_bm;
+#endif
+			mutex_unlock(&kbdev->pm.lock);
+		}
+			break;
+		case DVFS_REQ_HWC_DUMP:
+			gpu_dvfs_calculate_env_data(pkbdev);
+			platform->hwc_data.data[HWC_DATA_CLOCK] = platform->cur_clock;
+			platform->hwc_data.data[HWC_DATA_UTILIZATION] = platform->env_data.utilization;
+
+			if (copy_to_user(data, &platform->hwc_data, job->data_size) != 0)
+				goto out;
+			break;
+		default:
+			break;
+	}
+
+	mutex_unlock(&platform->gpu_process_job_lock);
+
+	if (copy_to_user(job_addr, &dvfs_job, sizeof(gpu_dvfs_job)) != 0)
+		return false;
+
+	return true;
+
+out:
+	job->event = 0x1234;
+	mutex_unlock(&platform->gpu_process_job_lock);
+	if (copy_to_user(job_addr, &dvfs_job, sizeof(gpu_dvfs_job)) != 0)
+		return false;
+
+	return false;
+}
+
+#ifdef CONFIG_PWRCAL
+bool update_cal_table()
+{
+	struct kbase_device *kbdev = pkbdev;
+	struct exynos_context *platform;
+	struct dvfs_rate_volt rate_volt[48];
+	int table_size, i;
+
+	platform = kbdev ? (struct exynos_context *) kbdev->platform_context:NULL;
+	if (platform == NULL)
+		return false;
+
+	/* update mif table */
+	table_size = cal_dfs_get_rate_asv_table(dvfs_mif, rate_volt);
+	platform->mif_table = kmalloc(sizeof(int) * table_size, GFP_KERNEL);
+	for (i = 0; i < table_size; i++) {
+		platform->mif_table[i] = rate_volt[i].rate;
+	}
+	platform->mif_table_size = table_size;
+	/* update little table */
+	table_size = cal_dfs_get_rate_asv_table(dvfs_little, rate_volt);
+	platform->apollo_table = kmalloc(sizeof(int) * table_size, GFP_KERNEL);
+	for (i = 0; i < table_size; i++) {
+		platform->apollo_table[i] = rate_volt[i].rate;
+	}
+	platform->apollo_table_size = table_size;
+	/* update big table */
+	table_size = cal_dfs_get_rate_asv_table(dvfs_big, rate_volt);
+	platform->atlas_table = kmalloc(sizeof(int) * table_size, GFP_KERNEL);
+	for (i = 0; i < table_size; i++) {
+		platform->atlas_table[i] = rate_volt[i].rate;
+	}
+	platform->atlas_table_size = table_size;
+	/* update int table */
+	table_size = cal_dfs_get_rate_asv_table(dvfs_int, rate_volt);
+	platform->int_table = kmalloc(sizeof(int) * table_size, GFP_KERNEL);
+	for (i = 0; i < table_size; i++) {
+		platform->int_table[i] = rate_volt[i].rate;
+	}
+	platform->int_table_size = table_size;
+
+	return true;
+}
+#endif
+#endif
diff -Nur r5p0/platform/gpu_dvfs_governor.c r15p0/platform/exynos/gpu_dvfs_governor.c
--- r5p0/platform/gpu_dvfs_governor.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_dvfs_governor.c	2017-07-20 16:14:04.620559419 +0200
@@ -89,9 +89,12 @@
 	if ((platform->step > gpu_dvfs_get_level(platform->gpu_max_clock)) &&
 			(utilization > platform->table[platform->step].max_threshold)) {
 		platform->step--;
-#ifdef MALI_SEC_HWCNT
+#ifdef CONFIG_MALI_SEC_HWCNT
 		if ((!platform->hwcnt_bt_clk) && (platform->table[platform->step].clock > platform->gpu_max_clock_limit))
 			platform->step = gpu_dvfs_get_level(platform->gpu_max_clock_limit);
+#else
+		if (platform->table[platform->step].clock > platform->gpu_max_clock_limit)
+			platform->step = gpu_dvfs_get_level(platform->gpu_max_clock_limit);
 #endif
 		platform->down_requirement = platform->table[platform->step].down_staycount;
 	} else if ((platform->step < gpu_dvfs_get_level(platform->gpu_min_clock)) && (utilization < platform->table[platform->step].min_threshold)) {
@@ -128,9 +131,12 @@
 			platform->step--;
 			platform->interactive.delay_count = 0;
 		}
-#ifdef MALI_SEC_HWCNT
+#ifdef CONFIG_MALI_SEC_HWCNT
 		if ((!platform->hwcnt_bt_clk) && (platform->table[platform->step].clock > platform->gpu_max_clock_limit))
 			platform->step = gpu_dvfs_get_level(platform->gpu_max_clock_limit);
+#else
+		if (platform->table[platform->step].clock > platform->gpu_max_clock_limit)
+			platform->step = gpu_dvfs_get_level(platform->gpu_max_clock_limit);
 #endif
 		platform->down_requirement = platform->table[platform->step].down_staycount;
 	} else if ((platform->step < gpu_dvfs_get_level(platform->gpu_min_clock))
@@ -241,6 +247,11 @@
 	gpu_dvfs_get_next_level(platform, utilization);
 	spin_unlock_irqrestore(&platform->gpu_dvfs_spinlock, flags);
 
+#ifdef CONFIG_MALI_SEC_CL_BOOST
+	if (kbdev->pm.backend.metrics.is_full_compute_util)
+		platform->step = gpu_dvfs_get_level(platform->gpu_max_clock_limit);
+#endif
+
 #ifdef CONFIG_CPU_THERMAL_IPA
 	ipa_mali_dvfs_requested(platform->table[platform->step].clock);
 #endif /* CONFIG_CPU_THERMAL_IPA */
diff -Nur r5p0/platform/gpu_dvfs_handler.c r15p0/platform/exynos/gpu_dvfs_handler.c
--- r5p0/platform/gpu_dvfs_handler.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_dvfs_handler.c	2017-07-20 16:14:04.000000000 +0200
@@ -88,6 +88,13 @@
 	if (!platform->dvfs_status)
 		platform->dvfs_status = true;
 
+#ifdef CONFIG_MALI_DVFS_USER
+	platform->mif_min_step = -1;
+	platform->int_min_step = -1;
+	platform->apollo_min_step = -1;
+	platform->atlas_min_step = -1;
+	proactive_pm_qos_command(platform, GPU_CONTROL_PM_QOS_INIT);
+#endif
 	gpu_pm_qos_command(platform, GPU_CONTROL_PM_QOS_INIT);
 
 	gpu_set_target_clk_vol(platform->table[platform->step].clock, false);
@@ -106,6 +113,9 @@
 		platform->dvfs_status = false;
 
 	gpu_pm_qos_command(platform, GPU_CONTROL_PM_QOS_DEINIT);
+#ifdef CONFIG_MALI_DVFS_USER
+	proactive_pm_qos_command(platform, GPU_CONTROL_PM_QOS_DEINIT);
+#endif
 
 	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "dvfs handler de-initialized\n");
 	return 0;
diff -Nur r5p0/platform/gpu_dvfs_handler.h r15p0/platform/exynos/gpu_dvfs_handler.h
--- r5p0/platform/gpu_dvfs_handler.h	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_dvfs_handler.h	2017-07-20 16:14:04.620559419 +0200
@@ -80,5 +80,12 @@
 
 int gpu_pm_qos_command(struct exynos_context *platform, gpu_pmqos_state state);
 int gpu_mif_pmqos(struct exynos_context *platform, int mem_freq);
-
+#ifdef CONFIG_MALI_DVFS_USER
+int proactive_pm_qos_command(struct exynos_context *platform, gpu_pmqos_state state);
+int gpu_mif_min_pmqos(struct exynos_context *platform, int mem_step);
+int gpu_int_min_pmqos(struct exynos_context *platform, int int_step);
+int gpu_apollo_min_pmqos(struct exynos_context *platform, int apollo_step);
+int gpu_atlas_min_pmqos(struct exynos_context *platform, int atlas_step);
+int gpu_dvfs_update_hwc(struct kbase_device *kbdev);
+#endif
 #endif /* _GPU_DVFS_HANDLER_H_ */
diff -Nur r5p0/platform/gpu_exynos5433.c r15p0/platform/exynos/gpu_exynos5433.c
--- r5p0/platform/gpu_exynos5433.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_exynos5433.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,677 +0,0 @@
-/* drivers/gpu/arm/.../platform/gpu_exynos5433.c
- *
- * Copyright 2011 by S.LSI. Samsung Electronics Inc.
- * San#24, Nongseo-Dong, Giheung-Gu, Yongin, Korea
- *
- * Samsung SoC Mali-T Series DVFS driver
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software FoundatIon.
- */
-
-/**
- * @file gpu_exynos5433.c
- * DVFS
- */
-
-#include <mali_kbase.h>
-
-#include <linux/regulator/driver.h>
-#include <linux/pm_qos.h>
-#include <linux/mfd/samsung/core.h>
-
-#include <mach/asv-exynos.h>
-#include <mach/asv-exynos_cal.h>
-#include <mach/pm_domains.h>
-#if defined(CONFIG_EXYNOS5433_BTS)
-#include <mach/bts.h>
-#endif /* CONFIG_EXYNOS5433_BTS */
-
-#include <linux/sec_debug.h>
-
-#include "mali_kbase_platform.h"
-#include "gpu_dvfs_handler.h"
-#include "gpu_control.h"
-
-extern struct kbase_device *pkbdev;
-
-#define CPU_MAX PM_QOS_CLUSTER1_FREQ_MAX_DEFAULT_VALUE
-#define G3D_RBB_VALUE	0x8d
-
-#define GPU_OSC_CLK	24000
-
-/*  clk,vol,abb,min,max,down stay,time_in_state,pm_qos mem,pm_qos int,pm_qos cpu_kfc_min,pm_qos cpu_egl_max */
-static gpu_dvfs_info gpu_dvfs_table_default[] = {
-	{700, 1150000, 0, 98, 100, 1, 0, 921000, 400000, 1300000, 1300000},
-	{600, 1150000, 0, 98,  99, 1, 0, 921000, 400000, 1300000, 1300000},
-	{550, 1125000, 0, 98,  99, 1, 0, 825000, 400000, 1300000, 1800000},
-	{500, 1075000, 0, 98,  99, 1, 0, 825000, 400000, 1300000, 1800000},
-	{420, 1025000, 0, 80,  99, 1, 0, 667000, 200000,  900000, 1800000},
-	{350, 1025000, 0, 80,  90, 1, 0, 543000, 160000,       0, CPU_MAX},
-	{266, 1000000, 0, 80,  90, 3, 0, 413000, 133000,       0, CPU_MAX},
-	{160, 1000000, 0,  0,  90, 1, 0, 272000, 133000,       0, CPU_MAX},
-};
-
-static int mif_min_table[] = {
-	78000, 109000, 136000,
-	167000, 222000, 272000,
-	413000, 543000, 667000,
-	825000,
-};
-
-static int available_max_clock[] = {GPU_L2, GPU_L2, GPU_L0, GPU_L0, GPU_L0};
-
-static gpu_attribute gpu_config_attributes[] = {
-	{GPU_MAX_CLOCK, 700},
-	{GPU_MAX_CLOCK_LIMIT, 600},
-	{GPU_MIN_CLOCK, 160},
-	{GPU_DVFS_START_CLOCK, 266},
-	{GPU_DVFS_BL_CONFIG_CLOCK, 266},
-	{GPU_GOVERNOR_START_CLOCK_DEFAULT, 266},
-	{GPU_GOVERNOR_START_CLOCK_STATIC, 266},
-	{GPU_GOVERNOR_START_CLOCK_BOOSTER, 266},
-	{GPU_GOVERNOR_TABLE_DEFAULT, (uintptr_t)&gpu_dvfs_table_default},
-	{GPU_GOVERNOR_TABLE_STATIC, (uintptr_t)&gpu_dvfs_table_default},
-	{GPU_GOVERNOR_TABLE_BOOSTER, (uintptr_t)&gpu_dvfs_table_default},
-	{GPU_GOVERNOR_TABLE_SIZE_DEFAULT, GPU_DVFS_TABLE_LIST_SIZE(gpu_dvfs_table_default)},
-	{GPU_GOVERNOR_TABLE_SIZE_STATIC, GPU_DVFS_TABLE_LIST_SIZE(gpu_dvfs_table_default)},
-	{GPU_GOVERNOR_TABLE_SIZE_BOOSTER, GPU_DVFS_TABLE_LIST_SIZE(gpu_dvfs_table_default)},
-	{GPU_DEFAULT_VOLTAGE, 937500},
-	{GPU_COLD_MINIMUM_VOL, 0},
-	{GPU_VOLTAGE_OFFSET_MARGIN, 37500},
-	{GPU_TMU_CONTROL, 1},
-	{GPU_TEMP_THROTTLING1, 420},
-	{GPU_TEMP_THROTTLING2, 350},
-	{GPU_TEMP_THROTTLING3, 266},
-	{GPU_TEMP_THROTTLING4, 160},
-	{GPU_TEMP_TRIPPING, 160},
-	{GPU_BOOST_MIN_LOCK, 600},
-	{GPU_BOOST_EGL_MIN_LOCK, 1600000},
-	{GPU_POWER_COEFF, 460}, /* all core on param */
-	{GPU_DVFS_TIME_INTERVAL, 5},
-	{GPU_DEFAULT_WAKEUP_LOCK, 1},
-	{GPU_BUS_DEVFREQ, 1},
-	{GPU_DYNAMIC_ABB, 1},
-	{GPU_EARLY_CLK_GATING, 0},
-	{GPU_DVS, 1},
-	{GPU_PERF_GATHERING, 0},
-#ifdef MALI_SEC_HWCNT
-	{GPU_HWCNT_GATHERING, 1},
-	{GPU_HWCNT_GPR, 1},
-	{GPU_HWCNT_DUMP_PERIOD, 50}, /* ms */
-	{GPU_HWCNT_CHOOSE_JM , 0},
-	{GPU_HWCNT_CHOOSE_SHADER , 0x560},
-	{GPU_HWCNT_CHOOSE_TILER , 0},
-	{GPU_HWCNT_CHOOSE_L3_CACHE , 0},
-	{GPU_HWCNT_CHOOSE_MMU_L2 , 0},
-#endif
-	{GPU_RUNTIME_PM_DELAY_TIME, 50},
-	{GPU_DVFS_POLLING_TIME, 100},
-	{GPU_DEBUG_LEVEL, DVFS_WARNING},
-	{GPU_TRACE_LEVEL, TRACE_ALL},
-};
-
-int gpu_dvfs_decide_max_clock(struct exynos_context *platform)
-{
-	int table_id;
-	int level;
-
-	if (!platform)
-		return -1;
-
-	table_id = cal_get_table_ver();
-
-	if (table_id < 0)
-		return -1;
-
-	if (table_id >= GPU_DVFS_TABLE_LIST_SIZE(available_max_clock))
-		table_id = GPU_DVFS_TABLE_LIST_SIZE(available_max_clock)-1;
-
-	level = available_max_clock[table_id];
-
-	platform->gpu_max_clock = MIN(platform->gpu_max_clock, platform->table[level].clock);
-
-	if (is_max_limit_sample())
-		platform->gpu_max_clock = MIN(platform->gpu_max_clock, platform->table[GPU_L3].clock);
-
-	return 0;
-}
-
-void *gpu_get_config_attributes(void)
-{
-	return &gpu_config_attributes;
-}
-
-struct clk *fin_pll;
-struct clk *fout_g3d_pll;
-struct clk *aclk_g3d;
-struct clk *mout_g3d_pll;
-struct clk *dout_aclk_g3d;
-
-#ifdef CONFIG_REGULATOR
-struct regulator *g3d_regulator;
-#endif /* CONFIG_REGULATOR */
-
-int gpu_is_power_on(void)
-{
-	return ((__raw_readl(EXYNOS_PMU_G3D_STATUS) & POWER_ON_STATUS) == POWER_ON_STATUS) ? 1 : 0;
-}
-
-int gpu_power_init(struct kbase_device *kbdev)
-{
-	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
-
-	if (!platform)
-		return -ENODEV;
-
-	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "power initialized\n");
-
-	return 0;
-}
-
-int gpu_get_cur_clock(struct exynos_context *platform)
-{
-	if (!platform)
-		return -ENODEV;
-
-	if (!aclk_g3d) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: clock is not initialized\n", __func__);
-		return -1;
-	}
-
-	return clk_get_rate(aclk_g3d)/MHZ;
-}
-
-int gpu_is_clock_on(void)
-{
-	return __clk_is_enabled(aclk_g3d);
-}
-
-static int gpu_clock_on(struct exynos_context *platform)
-{
-	int ret = 0;
-	if (!platform)
-		return -ENODEV;
-
-#ifdef CONFIG_MALI_RT_PM
-	if (platform->exynos_pm_domain)
-		mutex_lock(&platform->exynos_pm_domain->access_lock);
-#endif /* CONFIG_MALI_RT_PM */
-
-	if (!gpu_is_power_on()) {
-		GPU_LOG(DVFS_WARNING, DUMMY, 0u, 0u, "%s: can't set clock on in power off status\n", __func__);
-		ret = -1;
-		goto err_return;
-	}
-
-	if (platform->clk_g3d_status == 1) {
-		ret = 0;
-		goto err_return;
-	}
-
-	if (aclk_g3d) {
-		(void) clk_prepare_enable(aclk_g3d);
-		GPU_LOG(DVFS_DEBUG, LSI_CLOCK_ON, 0u, 0u, "clock is enabled\n");
-	}
-
-	platform->clk_g3d_status = 1;
-
-err_return:
-#ifdef CONFIG_MALI_RT_PM
-	if (platform->exynos_pm_domain)
-		mutex_unlock(&platform->exynos_pm_domain->access_lock);
-#endif /* CONFIG_MALI_RT_PM */
-	return ret;
-}
-
-static int gpu_clock_off(struct exynos_context *platform)
-{
-	int ret = 0;
-
-	if (!platform)
-		return -ENODEV;
-
-#ifdef CONFIG_MALI_RT_PM
-	if (platform->exynos_pm_domain)
-		mutex_lock(&platform->exynos_pm_domain->access_lock);
-#endif /* CONFIG_MALI_RT_PM */
-
-	if (!gpu_is_power_on()) {
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%s: can't set clock off in power off status\n", __func__);
-		ret = -1;
-		goto err_return;
-	}
-
-	if (platform->clk_g3d_status == 0) {
-		ret = 0;
-		goto err_return;
-	}
-
-	if (aclk_g3d) {
-		(void)clk_disable_unprepare(aclk_g3d);
-		GPU_LOG(DVFS_DEBUG, LSI_CLOCK_OFF, 0u, 0u, "clock is disabled\n");
-	}
-
-	platform->clk_g3d_status = 0;
-
-err_return:
-#ifdef CONFIG_MALI_RT_PM
-	if (platform->exynos_pm_domain)
-		mutex_unlock(&platform->exynos_pm_domain->access_lock);
-#endif /* CONFIG_MALI_RT_PM */
-	return ret;
-}
-
-int gpu_register_dump(void)
-{
-	if (gpu_is_power_on()) {
-		/* G3D PMU */
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x105C4064, __raw_readl(EXYNOS5433_G3D_STATUS),
-							"REG_DUMP: EXYNOS5433_G3D_STATUS %x\n", __raw_readl(EXYNOS_PMU_G3D_STATUS));
-		/* G3D PLL */
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0000, __raw_readl(EXYNOS5430_G3D_PLL_LOCK),
-							"REG_DUMP: EXYNOS5433_G3D_PLL_LOCK %x\n", __raw_readl(EXYNOS5430_G3D_PLL_LOCK));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0100, __raw_readl(EXYNOS5430_G3D_PLL_CON0),
-							"REG_DUMP: EXYNOS5433_G3D_PLL_CON0 %x\n", __raw_readl(EXYNOS5430_G3D_PLL_CON0));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0104, __raw_readl(EXYNOS5430_G3D_PLL_CON1),
-							"REG_DUMP: EXYNOS5433_G3D_PLL_CON1 %x\n", __raw_readl(EXYNOS5430_G3D_PLL_CON1));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA010c, __raw_readl(EXYNOS5430_G3D_PLL_FREQ_DET),
-							"REG_DUMP: EXYNOS5433_G3D_PLL_FREQ_DET %x\n", __raw_readl(EXYNOS5430_G3D_PLL_FREQ_DET));
-
-		/* G3D SRC */
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0200, __raw_readl(EXYNOS5430_SRC_SEL_G3D),
-							"REG_DUMP: EXYNOS5430_SRC_SEL_G3D %x\n", __raw_readl(EXYNOS5430_SRC_SEL_G3D));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0300, __raw_readl(EXYNOS5430_SRC_ENABLE_G3D),
-							"REG_DUMP: EXYNOS5430_SRC_ENABLE_G3D %x\n", __raw_readl(EXYNOS5430_SRC_ENABLE_G3D));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0400, __raw_readl(EXYNOS5430_SRC_STAT_G3D),
-							"REG_DUMP: EXYNOS5430_SRC_STAT_G3D %x\n", __raw_readl(EXYNOS5430_SRC_STAT_G3D));
-
-		/* G3D DIV */
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0600, __raw_readl(EXYNOS5430_DIV_G3D),
-							"REG_DUMP: EXYNOS5430_DIV_G3D %x\n", __raw_readl(EXYNOS5430_DIV_G3D));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0604, __raw_readl(EXYNOS5430_DIV_G3D_PLL_FREQ_DET),
-							"REG_DUMP: EXYNOS5430_DIV_G3D_PLL_FREQ_DET %x\n", __raw_readl(EXYNOS5430_DIV_G3D_PLL_FREQ_DET));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0700, __raw_readl(EXYNOS5430_DIV_STAT_G3D),
-							"REG_DUMP: EXYNOS5430_DIV_STAT_G3D %x\n", __raw_readl(EXYNOS5430_DIV_STAT_G3D));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0704, __raw_readl(EXYNOS5430_DIV_STAT_G3D_PLL_FREQ_DET),
-							"REG_DUMP: EXYNOS5430_DIV_STAT_G3D_PLL_FREQ_DET %x\n", __raw_readl(EXYNOS5430_DIV_STAT_G3D_PLL_FREQ_DET));
-
-		/* G3D ENABLE */
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0800, __raw_readl(EXYNOS5430_ENABLE_ACLK_G3D),
-							"REG_DUMP: EXYNOS5430_ENABLE_ACLK_G3D %x\n", __raw_readl(EXYNOS5430_ENABLE_ACLK_G3D));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0900, __raw_readl(EXYNOS5430_ENABLE_PCLK_G3D),
-							"REG_DUMP: EXYNOS5430_ENABLE_PCLK_G3D %x\n", __raw_readl(EXYNOS5430_ENABLE_PCLK_G3D));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0A00, __raw_readl(EXYNOS5430_ENABLE_SCLK_G3D),
-							"REG_DUMP: EXYNOS5430_ENABLE_SCLK_G3D %x\n", __raw_readl(EXYNOS5430_ENABLE_SCLK_G3D));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0B00, __raw_readl(EXYNOS5430_ENABLE_IP_G3D0),
-							"REG_DUMP: EXYNOS5430_ENABLE_IP_G3D0 %x\n", __raw_readl(EXYNOS5430_ENABLE_IP_G3D0));
-		GPU_LOG(DVFS_DEBUG, LSI_REGISTER_DUMP, 0x14AA0B0A, __raw_readl(EXYNOS5430_ENABLE_IP_G3D1),
-							"REG_DUMP: EXYNOS5430_ENABLE_IP_G3D1 %x\n", __raw_readl(EXYNOS5430_ENABLE_IP_G3D1));
-	}
-
-	return 0;
-}
-
-static int gpu_set_clock(struct exynos_context *platform, int clk)
-{
-	long g3d_rate_prev = -1;
-	unsigned long g3d_rate = clk * MHZ;
-	int ret = 0;
-
-	if (aclk_g3d == 0)
-		return -1;
-
-#ifdef CONFIG_MALI_RT_PM
-	if (platform->exynos_pm_domain)
-		mutex_lock(&platform->exynos_pm_domain->access_lock);
-#endif /* CONFIG_MALI_RT_PM */
-
-	if (!gpu_is_power_on()) {
-		ret = -1;
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%s: can't set clock in the power-off state!\n", __func__);
-		goto err;
-	}
-
-	if (!gpu_is_clock_on()) {
-		ret = -1;
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%s: can't set clock in the clock-off state! %d\n", __func__, __raw_readl(EXYNOS5430_ENABLE_ACLK_G3D));
-		goto err;
-	}
-
-	g3d_rate_prev = clk_get_rate(aclk_g3d);
-
-	/* if changed the VPLL rate, set rate for VPLL and wait for lock time */
-	if (g3d_rate != g3d_rate_prev) {
-		/*change here for future stable clock changing*/
-		ret = clk_set_parent(mout_g3d_pll, fin_pll);
-		if (ret < 0) {
-			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_set_parent [mout_g3d_pll]\n", __func__);
-			goto err;
-		}
-
-		if (g3d_rate_prev != GPU_OSC_CLK)
-			sec_debug_aux_log(SEC_DEBUG_AUXLOG_CPU_BUS_CLOCK_CHANGE,
-				"[GPU] %7d <= %7d", g3d_rate / 1000, g3d_rate_prev / 1000);
-
-		/*change g3d pll*/
-		ret = clk_set_rate(fout_g3d_pll, g3d_rate);
-		if (ret < 0) {
-			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_set_rate [fout_g3d_pll]\n", __func__);
-			goto err;
-		}
-
-		/*restore parent*/
-		ret = clk_set_parent(mout_g3d_pll, fout_g3d_pll);
-		if (ret < 0) {
-			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_set_parent [mout_g3d_pll]\n", __func__);
-			goto err;
-		}
-	}
-
-	platform->cur_clock = gpu_get_cur_clock(platform);
-
-	if (platform->cur_clock != clk_get_rate(fout_g3d_pll)/MHZ)
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "clock value is wrong (aclk_g3d: %d, fout_g3d_pll: %d)\n",
-				platform->cur_clock, (int) clk_get_rate(fout_g3d_pll)/MHZ);
-
-	if (g3d_rate != g3d_rate_prev)
-		GPU_LOG(DVFS_DEBUG, LSI_CLOCK_VALUE, g3d_rate/MHZ, platform->cur_clock, "clock set: %d, clock get: %d\n", (int) g3d_rate/MHZ, platform->cur_clock);
-err:
-#ifdef CONFIG_MALI_RT_PM
-	if (platform->exynos_pm_domain)
-		mutex_unlock(&platform->exynos_pm_domain->access_lock);
-#endif /* CONFIG_MALI_RT_PM */
-	return ret;
-}
-
-static int gpu_set_clock_to_osc(struct exynos_context *platform)
-{
-	int ret = 0;
-
-#ifdef CONFIG_MALI_RT_PM
-	if (platform->exynos_pm_domain)
-		mutex_lock(&platform->exynos_pm_domain->access_lock);
-#endif /* CONFIG_MALI_RT_PM */
-
-	if (!gpu_is_power_on()) {
-		ret = -1;
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%s: can't control clock in the power-off state!\n", __func__);
-		goto err;
-	}
-
-	if (!gpu_is_clock_on()) {
-		ret = -1;
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%s: can't control clock in the clock-off state!\n", __func__);
-		goto err;
-	}
-
-	/* change the mux to osc */
-	ret = clk_set_parent(mout_g3d_pll, fin_pll);
-	if (ret < 0) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_set_parent [mout_g3d_pll]\n", __func__);
-		goto err;
-	}
-
-	GPU_LOG(DVFS_DEBUG, LSI_CLOCK_VALUE, platform->cur_clock, gpu_get_cur_clock(platform),
-		"clock set to soc: %d (%d)\n", gpu_get_cur_clock(platform), platform->cur_clock);
-err:
-#ifdef CONFIG_MALI_RT_PM
-	if (platform->exynos_pm_domain)
-		mutex_unlock(&platform->exynos_pm_domain->access_lock);
-#endif /* CONFIG_MALI_RT_PM */
-	return ret;
-}
-
-static int gpu_set_clock_pre(struct exynos_context *platform, int clk, bool is_up)
-{
-	if (!platform)
-		return -ENODEV;
-
-#if defined(CONFIG_EXYNOS5433_BTS)
-	if (!is_up) {
-		if (clk < platform->table[GPU_L4].clock)
-			bts_scen_update(TYPE_G3D_SCENARIO, 0);
-		else
-			bts_scen_update(TYPE_G3D_SCENARIO, 1);
-	}
-#endif /* CONFIG_EXYNOS5433_BTS */
-
-	return 0;
-}
-
-static int gpu_set_clock_post(struct exynos_context *platform, int clk, bool is_up)
-{
-	if (!platform)
-		return -ENODEV;
-
-#if defined(CONFIG_EXYNOS5433_BTS)
-	if (is_up) {
-		if (clk < platform->table[GPU_L4].clock)
-			bts_scen_update(TYPE_G3D_SCENARIO, 0);
-		else
-			bts_scen_update(TYPE_G3D_SCENARIO, 1);
-	}
-#endif /* CONFIG_EXYNOS5433_BTS */
-
-	return 0;
-}
-
-static int gpu_get_clock(struct kbase_device *kbdev)
-{
-	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
-	if (!platform)
-		return -ENODEV;
-
-	KBASE_DEBUG_ASSERT(kbdev != NULL);
-
-	fin_pll = clk_get(kbdev->dev, "fin_pll");
-	if (IS_ERR(fin_pll) || (fin_pll == NULL)) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_get [fin_pll]\n", __func__);
-		return -1;
-	}
-
-	fout_g3d_pll = clk_get(NULL, "fout_g3d_pll");
-	if (IS_ERR(fout_g3d_pll)) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_get [fout_g3d_pll]\n", __func__);
-		return -1;
-	}
-
-	aclk_g3d = clk_get(kbdev->dev, "aclk_g3d");
-	if (IS_ERR(aclk_g3d)) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_get [aclk_g3d]\n", __func__);
-		return -1;
-	}
-
-	dout_aclk_g3d = clk_get(kbdev->dev, "dout_aclk_g3d");
-	if (IS_ERR(dout_aclk_g3d)) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_get [dout_aclk_g3d]\n", __func__);
-		return -1;
-	}
-
-	mout_g3d_pll = clk_get(kbdev->dev, "mout_g3d_pll");
-	if (IS_ERR(mout_g3d_pll)) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_get [mout_g3d_pll]\n", __func__);
-		return -1;
-	}
-
-	return 0;
-}
-
-int gpu_clock_init(struct kbase_device *kbdev)
-{
-	int ret;
-
-	KBASE_DEBUG_ASSERT(kbdev != NULL);
-
-	ret = gpu_get_clock(kbdev);
-	if (ret < 0)
-		return -1;
-
-	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "clock initialized\n");
-
-	return 0;
-}
-
-int gpu_get_cur_voltage(struct exynos_context *platform)
-{
-	int ret = 0;
-#ifdef CONFIG_REGULATOR
-	if (!g3d_regulator) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: regulator is not initialized\n", __func__);
-		return -1;
-	}
-
-	ret = regulator_get_voltage(g3d_regulator);
-#endif /* CONFIG_REGULATOR */
-	return ret;
-}
-
-static int gpu_set_voltage(struct exynos_context *platform, int vol)
-{
-	if (gpu_get_cur_voltage(platform) == vol)
-		return 0;
-
-	if (!gpu_is_power_on()) {
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%s: can't set voltage in the power-off state!\n", __func__);
-		return -1;
-	}
-
-#ifdef CONFIG_REGULATOR
-	if (!g3d_regulator) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: regulator is not initialized\n", __func__);
-		return -1;
-	}
-
-	if (regulator_set_voltage(g3d_regulator, vol, vol) != 0) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to set voltage, voltage: %d\n", __func__, vol);
-		return -1;
-	}
-#endif /* CONFIG_REGULATOR */
-
-	platform->cur_voltage = gpu_get_cur_voltage(platform);
-
-	GPU_LOG(DVFS_DEBUG, LSI_VOL_VALUE, vol, platform->cur_voltage, "voltage set: %d, voltage get:%d\n", vol, platform->cur_voltage);
-
-	return 0;
-}
-
-static int gpu_set_voltage_pre(struct exynos_context *platform, bool is_up)
-{
-	if (!platform)
-		return -ENODEV;
-
-	if (!is_up && platform->dynamic_abb_status)
-		set_match_abb(ID_G3D, gpu_dvfs_get_cur_asv_abb());
-
-	return 0;
-}
-
-static int gpu_set_voltage_post(struct exynos_context *platform, bool is_up)
-{
-	if (!platform)
-		return -ENODEV;
-
-	if (is_up && platform->dynamic_abb_status)
-		set_match_abb(ID_G3D, gpu_dvfs_get_cur_asv_abb());
-
-	return 0;
-}
-
-static struct gpu_control_ops ctr_ops = {
-	.is_power_on = gpu_is_power_on,
-	.set_voltage = gpu_set_voltage,
-	.set_voltage_pre = gpu_set_voltage_pre,
-	.set_voltage_post = gpu_set_voltage_post,
-	.set_clock_to_osc = gpu_set_clock_to_osc,
-	.set_clock = gpu_set_clock,
-	.set_clock_pre = gpu_set_clock_pre,
-	.set_clock_post = gpu_set_clock_post,
-	.enable_clock = gpu_clock_on,
-	.disable_clock = gpu_clock_off,
-};
-
-struct gpu_control_ops *gpu_get_control_ops(void)
-{
-	return &ctr_ops;
-}
-
-#ifdef CONFIG_REGULATOR
-int gpu_enable_dvs(struct exynos_context *platform)
-{
-	if (!platform->dvs_status)
-		return 0;
-
-#if defined(CONFIG_REGULATOR_S2MPS13)
-	if (s2m_set_dvs_pin(true) != 0) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to enable dvs\n", __func__);
-		return -1;
-	}
-#endif /* CONFIG_REGULATOR_S2MPS13 */
-
-	if (!cal_get_fs_abb()) {
-		if (set_match_abb(ID_G3D, G3D_RBB_VALUE)) {
-			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to restore RBB setting\n", __func__);
-			return -1;
-		}
-	}
-
-	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "dvs is enabled (vol: %d)\n", gpu_get_cur_voltage(platform));
-	return 0;
-}
-
-int gpu_disable_dvs(struct exynos_context *platform)
-{
-	if (!platform->dvs_status)
-		return 0;
-
-	if (!cal_get_fs_abb()) {
-		if (set_match_abb(ID_G3D, gpu_dvfs_get_cur_asv_abb())) {
-			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to restore RBB setting\n", __func__);
-			return -1;
-		}
-	}
-
-#if defined(CONFIG_REGULATOR_S2MPS13)
-	if (s2m_set_dvs_pin(false) != 0) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to disable dvs\n", __func__);
-		return -1;
-	}
-#endif /* CONFIG_REGULATOR_S2MPS13 */
-
-	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "dvs is disabled (vol: %d)\n", gpu_get_cur_voltage(platform));
-	return 0;
-}
-
-int gpu_regulator_init(struct exynos_context *platform)
-{
-	int gpu_voltage = 0;
-
-	g3d_regulator = regulator_get(NULL, "vdd_g3d");
-	if (IS_ERR(g3d_regulator)) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to get vdd_g3d regulator, 0x%p\n", __func__, g3d_regulator);
-		g3d_regulator = NULL;
-		return -1;
-	}
-
-	gpu_voltage = get_match_volt(ID_G3D, platform->gpu_dvfs_config_clock*1000);
-
-	if (gpu_voltage == 0)
-		gpu_voltage = platform->gpu_default_vol;
-
-	if (gpu_set_voltage(platform, gpu_voltage) != 0) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to set voltage [%d]\n", __func__, gpu_voltage);
-		return -1;
-	}
-
-	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "regulator initialized\n");
-
-	return 0;
-}
-#endif /* CONFIG_REGULATOR */
-
-int *get_mif_table(int *size)
-{
-	*size = ARRAY_SIZE(mif_min_table);
-	return mif_min_table;
-}
diff -Nur r5p0/platform/gpu_exynos7420.c r15p0/platform/exynos/gpu_exynos7420.c
--- r5p0/platform/gpu_exynos7420.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_exynos7420.c	2017-07-25 18:24:55.753690731 +0200
@@ -41,29 +41,27 @@
 #endif
 #endif
 
-#define CPU_MAX PM_QOS_CLUSTER1_FREQ_MAX_DEFAULT_VALUE
-
 #ifdef CONFIG_EXYNOS_NOC_DEBUGGING
 void __iomem *g3d0_outstanding_regs;
 void __iomem *g3d1_outstanding_regs;
 #endif /* CONFIG_EXYNOS_NOC_DEBUGGING */
 
-/*  clk,vol,abb,min,max,down stay, pm_qos mem, pm_qos int, pm_qos cpu_kfc_min, pm_qos cpu_egl_max */
+/*  clock, voltage, asv_abb, min_threshold, max_threshold, down_staycount, time, mem_freq, int_freq, cpu_freq (cl0) */
 static gpu_dvfs_info gpu_dvfs_table_default[] = {
-	{772, 900000, 0, 98, 100, 1, 0, 1552000, 400000, 1500000, 1300000},
-	{700, 900000, 0, 98,  99, 1, 0, 1552000, 400000, 1500000, 1300000},
-	{600, 900000, 0, 78,  85, 1, 0, 1552000, 413000, 1500000, 1300000},
-	{544, 900000, 0, 78,  85, 1, 0, 1026000, 413000, 1500000, 1800000},
-	{420, 900000, 0, 78,  85, 1, 0, 1026000, 267000,  900000, 1800000},
-	{350, 900000, 0, 78,  85, 1, 0,  543000, 200000,       0, CPU_MAX},
-	{266, 900000, 0, 78,  85, 1, 0,  416000, 160000,       0, CPU_MAX},
+	{ 772, 900000, 0, 98, 100, 1, 0, 1552000, 400000, 1500000 },
+	{ 700, 840000, 0, 98,  99, 1, 0, 1552000, 400000, 1200000 },
+	{ 600, 780000, 0, 78,  85, 1, 0, 1552000, 413000,  900000 },
+	{ 544, 730000, 0, 78,  85, 1, 0, 1026000, 413000,  600000 },
+	{ 420, 680000, 0, 78,  85, 1, 0, 1026000, 267000,       0 },
+	{ 350, 640000, 0, 78,  85, 1, 0,  543000, 200000,       0 },
+	{ 266, 600000, 0, 78,  85, 1, 0,  416000, 160000,       0 },
 };
 
 static int mif_min_table[] = {
 	 100000,  133000,  167000,
 	 276000,  348000,  416000,
 	 543000,  632000,  828000,
-	1026000, 1264000, 1456000,
+	1026000, 1264000, 1464000,
 	1552000,
 };
 
@@ -74,7 +72,7 @@
 
 static gpu_attribute gpu_config_attributes[] = {
 	{GPU_MAX_CLOCK, 772},
-	{GPU_MAX_CLOCK_LIMIT, 700},
+	{GPU_MAX_CLOCK_LIMIT, 772},
 	{GPU_MIN_CLOCK, 266},
 	{GPU_DVFS_START_CLOCK, 266},
 	{GPU_DVFS_BL_CONFIG_CLOCK, 266},
@@ -98,12 +96,12 @@
 	{GPU_COLD_MINIMUM_VOL, 0},
 	{GPU_VOLTAGE_OFFSET_MARGIN, 37500},
 	{GPU_TMU_CONTROL, 1},
-	{GPU_TEMP_THROTTLING1, 544},
-	{GPU_TEMP_THROTTLING2, 350},
-	{GPU_TEMP_THROTTLING3, 266},
-	{GPU_TEMP_THROTTLING4, 266},
+	{GPU_TEMP_THROTTLING1, 600},
+	{GPU_TEMP_THROTTLING2, 544},
+	{GPU_TEMP_THROTTLING3, 420},
+	{GPU_TEMP_THROTTLING4, 350},
 	{GPU_TEMP_TRIPPING, 266},
-	{GPU_POWER_COEFF, 443}, /* all core on param */
+	{GPU_POWER_COEFF, 900}, /* all core on param */
 	{GPU_DVFS_TIME_INTERVAL, 5},
 	{GPU_DEFAULT_WAKEUP_LOCK, 1},
 	{GPU_BUS_DEVFREQ, 1},
@@ -111,7 +109,8 @@
 	{GPU_EARLY_CLK_GATING, 0},
 	{GPU_DVS, 1},
 	{GPU_PERF_GATHERING, 0},
-#ifdef MALI_SEC_HWCNT
+#ifdef CONFIG_MALI_SEC_HWCNT
+	{GPU_HWCNT_PROFILE, 0},
 	{GPU_HWCNT_GATHERING, 1},
 	{GPU_HWCNT_POLLING_TIME, 90},
 	{GPU_HWCNT_UP_STEP, 3},
@@ -128,22 +127,40 @@
 	{GPU_DVFS_POLLING_TIME, 30},
 	{GPU_PMQOS_INT_DISABLE, 1},
 	{GPU_PMQOS_MIF_MAX_CLOCK, 1464000},
-	{GPU_PMQOS_MIF_MAX_CLOCK_BASE, 700},
+	{GPU_PMQOS_MIF_MAX_CLOCK_BASE, 772},
 	{GPU_CL_DVFS_START_BASE, 266},
 	{GPU_DEBUG_LEVEL, DVFS_WARNING},
 	{GPU_TRACE_LEVEL, TRACE_ALL},
+#ifdef CONFIG_MALI_DVFS_USER
+	{GPU_UDVFS_ENABLE, 1},
+#endif
 };
 
+extern void preload_balance_init(struct kbase_device *kbdev);
+int gpu_device_specific_init(struct kbase_device *kbdev)
+{
+	preload_balance_init(kbdev);
+
+	return 1;
+}
+
 int gpu_dvfs_decide_max_clock(struct exynos_context *platform)
 {
 	if (!platform)
 		return -1;
-    if (cal_get_table_ver() == 12)
+	if (cal_get_table_ver() == 12)
 		platform->gpu_max_clock = platform->table[GPU_L2].clock;
 
 	return 0;
 }
 
+#ifdef CONFIG_MALI_DVFS_USER
+unsigned int gpu_get_config_attr_size(void)
+{
+	return sizeof(gpu_config_attributes);
+}
+#endif
+
 void *gpu_get_config_attributes(void)
 {
 	return &gpu_config_attributes;
@@ -554,7 +571,6 @@
 	level = gpu_dvfs_get_level(gpu_get_cur_clock(platform));
 	exynos7420_cl_dvfs_stop(ID_G3D, level);
 #endif /* CONFIG_EXYNOS_CL_DVFS_G3D */
-
 	/* Do not need to enable dvs during suspending */
 	if (!pkbdev->pm.suspending) {
 		if (s2m_set_dvs_pin(true) != 0) {
@@ -588,7 +604,6 @@
 
 int gpu_regulator_init(struct exynos_context *platform)
 {
-	int gpu_voltage = 0;
 
 	g3d_regulator = regulator_get(NULL, "vdd_g3d");
 	if (IS_ERR(g3d_regulator)) {
@@ -597,16 +612,6 @@
 		return -1;
 	}
 
-	gpu_voltage = get_match_volt(ID_G3D, platform->gpu_dvfs_config_clock*1000);
-
-	if (gpu_voltage == 0)
-		gpu_voltage = platform->gpu_default_vol;
-
-	if (gpu_set_voltage(platform, gpu_voltage) != 0) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to set voltage [%d]\n", __func__, gpu_voltage);
-		return -1;
-	}
-
 	if (platform->dvs_status)
 		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "dvs GPIO PMU status enable\n");
 
diff -Nur r5p0/platform/gpu_exynos7890.c r15p0/platform/exynos/gpu_exynos7890.c
--- r5p0/platform/gpu_exynos7890.c	1970-01-01 01:00:00.000000000 +0100
+++ r15p0/platform/exynos/gpu_exynos7890.c	2017-07-20 16:14:04.620559419 +0200
@@ -0,0 +1,579 @@
+/* drivers/gpu/t6xx/kbase/src/platform/gpu_exynos7890.c
+ *
+ * Copyright 2011 by S.LSI. Samsung Electronics Inc.
+ * San#24, Nongseo-Dong, Giheung-Gu, Yongin, Korea
+ *
+ * Samsung SoC Mali-T604 DVFS driver
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software FoundatIon.
+ */
+
+/**
+ * @file gpu_exynos7890.c
+ * DVFS
+ */
+
+#include <mali_kbase.h>
+
+#include <linux/regulator/driver.h>
+#include <linux/pm_qos.h>
+#include <linux/delay.h>
+#include <linux/smc.h>
+
+#include <mach/apm-exynos.h>
+#include <mach/asv-exynos.h>
+#ifdef MALI64_PORTING
+#include <mach/asv-exynos7_cal.h>
+#endif
+#include <mach/pm_domains.h>
+#include <mach/regs-pmu.h>
+
+#include "mali_kbase_platform.h"
+#include "gpu_dvfs_handler.h"
+#include "gpu_dvfs_governor.h"
+#include "gpu_control.h"
+#include "../mali_midg_regmap.h"
+
+extern struct kbase_device *pkbdev;
+#ifdef CONFIG_REGULATOR
+#if defined(CONFIG_REGULATOR_S2MPS16)
+extern int s2m_get_dvs_is_on(void);
+#endif
+#endif
+
+#define CPU_MAX PM_QOS_CLUSTER1_FREQ_MAX_DEFAULT_VALUE
+
+#ifdef CONFIG_EXYNOS_BUSMONITOR
+void __iomem *g3d0_outstanding_regs;
+void __iomem *g3d1_outstanding_regs;
+#endif /* CONFIG_EXYNOS_BUSMONITOR */
+
+/*  clk,vol,abb,min,max,down stay, pm_qos mem, pm_qos int, pm_qos cpu_kfc_min, pm_qos cpu_egl_max */
+static gpu_dvfs_info gpu_dvfs_table_default[] = {
+	{772, 850000, 0, 98, 100, 1, 0, 1552000, 400000, 1500000, 1300000},
+	{700, 800000, 0, 98, 100, 1, 0, 1552000, 400000, 1500000, 1300000},
+	{600, 800000, 0, 78,  85, 1, 0, 1552000, 413000, 1500000, 1300000},
+	{544, 800000, 0, 78,  85, 1, 0, 1026000, 413000, 1500000, 1800000},
+	{420, 800000, 0, 78,  85, 1, 0, 1026000, 267000,  900000, 1800000},
+	{350, 800000, 0, 78,  85, 1, 0,  543000, 200000,       0, CPU_MAX},
+	{266, 800000, 0, 78,  85, 1, 0,  416000, 160000,       0, CPU_MAX},
+};
+
+static int mif_min_table[] = {
+	 100000,  133000,  167000,
+	 276000,  348000,  416000,
+	 543000,  632000,  828000,
+	1026000, 1264000, 1456000,
+	1552000,
+};
+
+static int hpm_freq_table[] = {
+	/* 772, 700, 600, 544, 420, 350, 266 */
+	3, 3, 3, 3, 2, 2, 2,
+};
+
+static gpu_attribute gpu_config_attributes[] = {
+	{GPU_MAX_CLOCK, 772},
+	{GPU_MAX_CLOCK_LIMIT, 700},
+	{GPU_MIN_CLOCK, 266},
+	{GPU_DVFS_START_CLOCK, 266},
+	{GPU_DVFS_BL_CONFIG_CLOCK, 266},
+	{GPU_GOVERNOR_TYPE, G3D_DVFS_GOVERNOR_INTERACTIVE},
+	{GPU_GOVERNOR_START_CLOCK_DEFAULT, 266},
+	{GPU_GOVERNOR_START_CLOCK_INTERACTIVE, 266},
+	{GPU_GOVERNOR_START_CLOCK_STATIC, 266},
+	{GPU_GOVERNOR_START_CLOCK_BOOSTER, 266},
+	{GPU_GOVERNOR_TABLE_DEFAULT, (uintptr_t)&gpu_dvfs_table_default},
+	{GPU_GOVERNOR_TABLE_INTERACTIVE, (uintptr_t)&gpu_dvfs_table_default},
+	{GPU_GOVERNOR_TABLE_STATIC, (uintptr_t)&gpu_dvfs_table_default},
+	{GPU_GOVERNOR_TABLE_BOOSTER, (uintptr_t)&gpu_dvfs_table_default},
+	{GPU_GOVERNOR_TABLE_SIZE_DEFAULT, GPU_DVFS_TABLE_LIST_SIZE(gpu_dvfs_table_default)},
+	{GPU_GOVERNOR_TABLE_SIZE_INTERACTIVE, GPU_DVFS_TABLE_LIST_SIZE(gpu_dvfs_table_default)},
+	{GPU_GOVERNOR_TABLE_SIZE_STATIC, GPU_DVFS_TABLE_LIST_SIZE(gpu_dvfs_table_default)},
+	{GPU_GOVERNOR_TABLE_SIZE_BOOSTER, GPU_DVFS_TABLE_LIST_SIZE(gpu_dvfs_table_default)},
+	{GPU_GOVERNOR_INTERACTIVE_HIGHSPEED_CLOCK, 420},
+	{GPU_GOVERNOR_INTERACTIVE_HIGHSPEED_LOAD, 95},
+	{GPU_GOVERNOR_INTERACTIVE_HIGHSPEED_DELAY, 0},
+	{GPU_DEFAULT_VOLTAGE, 900000},
+	{GPU_COLD_MINIMUM_VOL, 0},
+	{GPU_VOLTAGE_OFFSET_MARGIN, 37500},
+	{GPU_TMU_CONTROL, 1},
+	{GPU_TEMP_THROTTLING1, 544},
+	{GPU_TEMP_THROTTLING2, 350},
+	{GPU_TEMP_THROTTLING3, 266},
+	{GPU_TEMP_THROTTLING4, 266},
+	{GPU_TEMP_TRIPPING, 266},
+	{GPU_POWER_COEFF, 443}, /* all core on param */
+	{GPU_DVFS_TIME_INTERVAL, 5},
+	{GPU_DEFAULT_WAKEUP_LOCK, 1},
+	{GPU_BUS_DEVFREQ, 1},
+	{GPU_DYNAMIC_ABB, 0},
+	{GPU_EARLY_CLK_GATING, 0},
+	{GPU_DVS, 1},
+	{GPU_PERF_GATHERING, 0},
+#ifdef CONFIG_MALI_SEC_HWCNT
+	{GPU_HWCNT_GATHERING, 1},
+	{GPU_HWCNT_GPR, 1},
+	{GPU_HWCNT_DUMP_PERIOD, 50}, /* ms */
+	{GPU_HWCNT_CHOOSE_JM , 0},
+	{GPU_HWCNT_CHOOSE_SHADER , 0x560},
+	{GPU_HWCNT_CHOOSE_TILER , 0},
+	{GPU_HWCNT_CHOOSE_L3_CACHE , 0},
+	{GPU_HWCNT_CHOOSE_MMU_L2 , 0},
+#endif
+	{GPU_RUNTIME_PM_DELAY_TIME, 50},
+	{GPU_DVFS_POLLING_TIME, 30},
+	{GPU_PMQOS_INT_DISABLE, 1},
+	{GPU_PMQOS_MIF_MAX_CLOCK, 1456000},
+	{GPU_PMQOS_MIF_MAX_CLOCK_BASE, 700},
+	{GPU_CL_DVFS_START_BASE, 700},
+	{GPU_DEBUG_LEVEL, DVFS_WARNING},
+	{GPU_TRACE_LEVEL, TRACE_ALL},
+};
+
+extern void preload_balance_init(struct kbase_device *kbdev);
+int gpu_device_specific_init(struct kbase_device *kbdev)
+{
+	preload_balance_init(kbdev);
+
+	return 1;
+}
+
+int gpu_dvfs_decide_max_clock(struct exynos_context *platform)
+{
+	if (!platform)
+		return -1;
+
+	return 0;
+}
+
+void *gpu_get_config_attributes(void)
+{
+	return &gpu_config_attributes;
+}
+
+uintptr_t gpu_get_max_freq(void)
+{
+	return gpu_get_attrib_data(gpu_config_attributes, GPU_MAX_CLOCK) * 1000;
+}
+
+uintptr_t gpu_get_min_freq(void)
+{
+	return gpu_get_attrib_data(gpu_config_attributes, GPU_MIN_CLOCK) * 1000;
+}
+
+struct clk *fin_pll;
+struct clk *fout_g3d_pll;
+struct clk *aclk_g3d;
+struct clk *mout_g3d;
+struct clk *sclk_hpm_g3d;
+#ifdef CONFIG_REGULATOR
+struct regulator *g3d_regulator;
+#endif /* CONFIG_REGULATOR */
+
+int gpu_is_power_on(void)
+{
+//	return ((__raw_readl(EXYNOS_PMU_G3D_STATUS) & LOCAL_PWR_CFG) == LOCAL_PWR_CFG) ? 1 : 0;
+	return 1;
+}
+
+int gpu_power_init(struct kbase_device *kbdev)
+{
+	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
+
+	if (!platform)
+		return -ENODEV;
+
+	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "power initialized\n");
+
+	return 0;
+}
+
+int gpu_get_cur_clock(struct exynos_context *platform)
+{
+	if (!platform)
+		return -ENODEV;
+
+	if (!aclk_g3d) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: clock is not initialized\n", __func__);
+		return -1;
+	}
+
+	return clk_get_rate(aclk_g3d)/MHZ;
+}
+
+int gpu_register_dump(void)
+{
+	if (gpu_is_power_on() && !s2m_get_dvs_is_on()) {
+		/* MCS Value check */
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP,  0x10051224 , __raw_readl(EXYNOS7420_VA_SYSREG + 0x1224),
+				"REG_DUMP: G3D_EMA_RF2_UHD_CON %x\n", __raw_readl(EXYNOS7420_VA_SYSREG + 0x1224));
+		/* G3D PMU */
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x105C4100, __raw_readl(EXYNOS_PMU_G3D_CONFIGURATION),
+				"REG_DUMP: EXYNOS_PMU_G3D_CONFIGURATION %x\n", __raw_readl(EXYNOS_PMU_G3D_CONFIGURATION));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x105C4104, __raw_readl(EXYNOS_PMU_G3D_STATUS),
+				"REG_DUMP: EXYNOS_PMU_G3D_STATUS %x\n", __raw_readl(EXYNOS_PMU_G3D_STATUS));
+		/* G3D PLL */
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x105C6100, __raw_readl(EXYNOS_PMU_GPU_DVS_CTRL),
+				"REG_DUMP: EXYNOS_PMU_GPU_DVS_CTRL %x\n", __raw_readl(EXYNOS_PMU_GPU_DVS_CTRL));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x10576104, __raw_readl(EXYNOS_PMU_GPU_DVS_STATUS),
+				"REG_DUMP: GPU_DVS_STATUS %x\n", __raw_readl(EXYNOS_PMU_GPU_DVS_STATUS));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x10051234, __raw_readl(EXYNOS7420_VA_SYSREG + 0x1234),
+				"REG_DUMP: G3D_G3DCFG_REG0 %x\n", __raw_readl(EXYNOS7420_VA_SYSREG + 0x1234));
+
+#ifdef CONFIG_EXYNOS_BUSMONITOR
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14A002F0, __raw_readl(g3d0_outstanding_regs + 0x2F0),
+				"REG_DUMP: read outstanding %x\n", __raw_readl(g3d0_outstanding_regs + 0x2F0));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14A003F0, __raw_readl(g3d0_outstanding_regs + 0x3F0),
+				"REG_DUMP: write outstanding %x\n", __raw_readl(g3d0_outstanding_regs + 0x3F0));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14A202F0, __raw_readl(g3d1_outstanding_regs + 0x2F0),
+				"REG_DUMP: read outstanding %x\n", __raw_readl(g3d1_outstanding_regs + 0x2F0));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14A203F0, __raw_readl(g3d1_outstanding_regs + 0x3F0),
+				"REG_DUMP: write outstanding %x\n", __raw_readl(g3d1_outstanding_regs + 0x3F0));
+#endif /* CONFIG_EXYNOS_BUSMONITOR */
+
+		/* G3D PLL */
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14AA0000, __raw_readl(G3D_LOCK),
+				"REG_DUMP: EXYNOS7890_G3D_PLL_LOCK %x\n", __raw_readl(G3D_LOCK));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14AA0100, __raw_readl(G3D_CON),
+				"REG_DUMP: EXYNOS7890_G3D_PLL_CON0 %x\n", __raw_readl(G3D_CON));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14AA0104, __raw_readl(G3D_CON1),
+				"REG_DUMP: EXYNOS7890_G3D_PLL_CON1 %x\n", __raw_readl(G3D_CON1));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14AA0108, __raw_readl(G3D_CON2),
+				"REG_DUMP: EXYNOS7890_G3D_PLL_CON2 %x\n", __raw_readl(G3D_CON2));
+
+		/* G3D SRC */
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14AA0200, __raw_readl(EXYNOS7420_MUX_SEL_G3D),
+				"REG_DUMP: EXYNOS7890_SRC_SEL_G3D %x\n", __raw_readl(EXYNOS7420_MUX_SEL_G3D));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14AA0300, __raw_readl(EXYNOS7420_MUX_ENABLE_G3D),
+				"REG_DUMP: EXYNOS7890_SRC_ENABLE_G3D %x\n", __raw_readl(EXYNOS7420_MUX_ENABLE_G3D));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14AA0400, __raw_readl(EXYNOS7420_MUX_STAT_G3D),
+				"REG_DUMP: EXYNOS7890_SRC_STAT_G3D %x\n", __raw_readl(EXYNOS7420_MUX_STAT_G3D));
+
+		/* G3D DIV */
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14AA0600, __raw_readl(EXYNOS7420_DIV_G3D),
+				"REG_DUMP: EXYNOS7890_DIV_G3D %x\n", __raw_readl(EXYNOS7420_DIV_G3D));
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14AA0700, __raw_readl(EXYNOS7420_DIV_STAT_G3D),
+				"REG_DUMP: EXYNOS7890_DIV_STAT_G3D %x\n", __raw_readl(EXYNOS7420_DIV_STAT_G3D));
+
+		/* G3D ENABLE */
+		GPU_LOG(DVFS_WARNING, LSI_REGISTER_DUMP, 0x14AA0B00, __raw_readl(EXYNOS7420_CLK_ENABLE_IP_G3D),
+				"REG_DUMP: EXYNOS7890_ENABLE_IP_G3D %x\n", __raw_readl(EXYNOS7420_CLK_ENABLE_IP_G3D));
+	} else {
+		GPU_LOG(DVFS_WARNING, DUMMY, 0u, 0u, "%s: Power Status %d, DVS Status %d\n", __func__, gpu_is_power_on(), s2m_get_dvs_is_on());
+	}
+
+	return 0;
+}
+
+static int gpu_set_clock(struct exynos_context *platform, int clk)
+{
+	long g3d_rate_prev = -1;
+	unsigned long g3d_rate = clk * MHZ;
+	int ret = 0;
+	int level = 0;
+
+	if (aclk_g3d == 0)
+		return -1;
+
+#ifdef CONFIG_MALI_RT_PM
+	if (platform->exynos_pm_domain)
+		mutex_lock(&platform->exynos_pm_domain->access_lock);
+#endif /* CONFIG_MALI_RT_PM */
+
+	if (!gpu_is_power_on()) {
+		ret = -1;
+		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%s: can't set clock in the power-off state!\n", __func__);
+		goto err;
+	}
+
+	g3d_rate_prev = clk_get_rate(aclk_g3d);
+
+	/* if changed the VPLL rate, set rate for VPLL and wait for lock time */
+	if (g3d_rate != g3d_rate_prev) {
+
+		ret = clk_set_parent(mout_g3d, fin_pll);
+		if (ret < 0) {
+			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_set_parent [fin_pll]\n", __func__);
+			goto err;
+		}
+
+		/*change g3d pll*/
+		ret = clk_set_rate(fout_g3d_pll, g3d_rate);
+		if (ret < 0) {
+			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_set_rate [fout_g3d_pll]\n", __func__);
+			goto err;
+		}
+
+		level = gpu_dvfs_get_level(g3d_rate/MHZ);
+		if (level < 0) {
+			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to gpu_dvfs_get_level \n", __func__);
+			goto err;
+		}
+
+		ret = clk_set_rate(sclk_hpm_g3d, (clk_get_rate(aclk_g3d)/hpm_freq_table[level]));
+		if(ret < 0)
+			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_set_rate [sclk_hpm_g3d]\n", __func__);
+
+		ret = clk_set_parent(mout_g3d, fout_g3d_pll);
+		if (ret < 0) {
+			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_set_parent [fout_g3d_pll]\n", __func__);
+			goto err;
+		}
+
+		g3d_rate_prev = g3d_rate;
+	}
+
+	platform->cur_clock = gpu_get_cur_clock(platform);
+
+	if (platform->cur_clock != clk_get_rate(fout_g3d_pll)/MHZ)
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "clock value is wrong (aclk_g3d: %d, fout_g3d_pll: %d)\n",
+				platform->cur_clock, (int) clk_get_rate(fout_g3d_pll)/MHZ);
+	GPU_LOG(DVFS_DEBUG, LSI_CLOCK_VALUE, g3d_rate/MHZ, platform->cur_clock,
+		"clock set: %ld, clock get: %d\n", g3d_rate/MHZ, platform->cur_clock);
+err:
+#ifdef CONFIG_MALI_RT_PM
+	if (platform->exynos_pm_domain)
+		mutex_unlock(&platform->exynos_pm_domain->access_lock);
+#endif /* CONFIG_MALI_RT_PM */
+	return ret;
+}
+
+static int gpu_get_clock(struct kbase_device *kbdev)
+{
+	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
+	if (!platform)
+		return -ENODEV;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	fin_pll = clk_get(kbdev->dev, "fin_pll");
+	if (IS_ERR(fin_pll) || (fin_pll == NULL)) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_get [fin_pll]\n", __func__);
+		return -1;
+	}
+
+	fout_g3d_pll = clk_get(kbdev->dev, "fout_g3d_pll");
+	if (IS_ERR(fout_g3d_pll)) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_get [fout_g3d_pll]\n", __func__);
+		return -1;
+	}
+
+	aclk_g3d = clk_get(kbdev->dev, "aclk_g3d");
+	if (IS_ERR(aclk_g3d)) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_get [aclk_g3d]\n", __func__);
+		return -1;
+	}
+
+	mout_g3d = clk_get(kbdev->dev, "mout_g3d");
+	if (IS_ERR(mout_g3d)) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_get [mout_g3d]\n", __func__);
+		return -1;
+	}
+
+	sclk_hpm_g3d = clk_get(kbdev->dev, "sclk_hpm_g3d");
+	if (IS_ERR(sclk_hpm_g3d)) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to clk_get [sclk_hpm_g3d]\n", __func__);
+		return -1;
+	}
+
+	__raw_writel(0x1, EXYNOS7420_MUX_SEL_G3D);
+
+	return 0;
+}
+
+int gpu_clock_init(struct kbase_device *kbdev)
+{
+	int ret;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	ret = gpu_get_clock(kbdev);
+	if (ret < 0)
+		return -1;
+
+#ifdef CONFIG_EXYNOS_BUSMONITOR
+	g3d0_outstanding_regs = ioremap(0x14A00000, SZ_1K);
+	g3d1_outstanding_regs = ioremap(0x14A20000, SZ_1K);
+#endif /* CONFIG_EXYNOS_BUSMONITOR */
+
+	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "clock initialized\n");
+
+	return 0;
+}
+
+int gpu_get_cur_voltage(struct exynos_context *platform)
+{
+	int ret = 0;
+#ifdef CONFIG_REGULATOR
+	if (!g3d_regulator) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: regulator is not initialized\n", __func__);
+		return -1;
+	}
+
+	ret = regulator_get_voltage(g3d_regulator);
+#endif /* CONFIG_REGULATOR */
+	return ret;
+}
+
+static int gpu_set_voltage(struct exynos_context *platform, int vol)
+{
+	if (gpu_get_cur_voltage(platform) == vol)
+		return 0;
+
+	if (!gpu_is_power_on()) {
+		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%s: can't set voltage in the power-off state!\n", __func__);
+		return -1;
+	}
+
+#ifdef CONFIG_REGULATOR
+	if (!g3d_regulator) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: regulator is not initialized\n", __func__);
+		return -1;
+	}
+
+#ifdef CONFIG_EXYNOS_CL_DVFS_G3D
+	regulator_sync_voltage(g3d_regulator);
+#endif /* CONFIG_EXYNOS_CL_DVFS_G3D */
+
+	if (regulator_set_voltage(g3d_regulator, vol, vol) != 0) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to set voltage, voltage: %d\n", __func__, vol);
+		return -1;
+	}
+#endif /* CONFIG_REGULATOR */
+
+	platform->cur_voltage = gpu_get_cur_voltage(platform);
+
+	GPU_LOG(DVFS_DEBUG, LSI_VOL_VALUE, vol, platform->cur_voltage, "voltage set: %d, voltage get:%d\n", vol, platform->cur_voltage);
+
+	return 0;
+}
+
+static int gpu_set_voltage_pre(struct exynos_context *platform, bool is_up)
+{
+	if (!platform)
+		return -ENODEV;
+
+	if (!is_up && platform->dynamic_abb_status)
+		set_match_abb(ID_G3D, gpu_dvfs_get_cur_asv_abb());
+
+	return 0;
+}
+
+static int gpu_set_voltage_post(struct exynos_context *platform, bool is_up)
+{
+	if (!platform)
+		return -ENODEV;
+
+	if (is_up && platform->dynamic_abb_status)
+		set_match_abb(ID_G3D, gpu_dvfs_get_cur_asv_abb());
+
+	return 0;
+}
+
+static struct gpu_control_ops ctr_ops = {
+	.is_power_on = gpu_is_power_on,
+	.set_voltage = gpu_set_voltage,
+	.set_voltage_pre = gpu_set_voltage_pre,
+	.set_voltage_post = gpu_set_voltage_post,
+	.set_clock_to_osc = NULL,
+	.set_clock = gpu_set_clock,
+	.set_clock_pre = NULL,
+	.set_clock_post = NULL,
+	.enable_clock = NULL,
+	.disable_clock = NULL,
+};
+
+struct gpu_control_ops *gpu_get_control_ops(void)
+{
+	return &ctr_ops;
+}
+
+#ifdef CONFIG_REGULATOR
+extern int s2m_set_dvs_pin(bool gpio_val);
+int gpu_enable_dvs(struct exynos_context *platform)
+{
+#ifdef CONFIG_EXYNOS_CL_DVFS_G3D
+	int level = 0;
+#endif /* CONFIG_EXYNOS_CL_DVFS_G3D */
+
+	if (!platform->dvs_status)
+		return 0;
+
+	if (!gpu_is_power_on()) {
+		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%s: can't set dvs in the power-off state!\n", __func__);
+		return -1;
+	}
+
+#if defined(CONFIG_REGULATOR_S2MPS16)
+#ifdef CONFIG_EXYNOS_CL_DVFS_G3D
+	level = gpu_dvfs_get_level(gpu_get_cur_clock(platform));
+	exynos7890_cl_dvfs_stop(ID_G3D, level);
+#endif /* CONFIG_EXYNOS_CL_DVFS_G3D */
+
+	/* Do not need to enable dvs during suspending */
+	if (!pkbdev->pm.suspending) {
+		if (s2m_set_dvs_pin(true) != 0) {
+			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to enable dvs\n", __func__);
+			return -1;
+		}
+	}
+#endif /* CONFIG_REGULATOR_S2MPS16 */
+
+	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "dvs is enabled (vol: %d)\n", gpu_get_cur_voltage(platform));
+	return 0;
+}
+
+int gpu_disable_dvs(struct exynos_context *platform)
+{
+	if (!platform->dvs_status)
+		return 0;
+
+#if defined(CONFIG_REGULATOR_S2MPS16)
+	if (s2m_set_dvs_pin(false) != 0) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to disable dvs\n", __func__);
+		return -1;
+	}
+#endif /* CONFIG_REGULATOR_S2MPS16 */
+
+	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "dvs is disabled (vol: %d)\n", gpu_get_cur_voltage(platform));
+	return 0;
+}
+
+int gpu_regulator_init(struct exynos_context *platform)
+{
+	int gpu_voltage = 0;
+
+	g3d_regulator = regulator_get(NULL, "vdd_g3d");
+	if (IS_ERR(g3d_regulator)) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to get vdd_g3d regulator, 0x%p\n", __func__, g3d_regulator);
+		g3d_regulator = NULL;
+		return -1;
+	}
+
+	gpu_voltage = get_match_volt(ID_G3D, platform->gpu_dvfs_config_clock*1000);
+
+	if (gpu_voltage == 0)
+		gpu_voltage = platform->gpu_default_vol;
+
+	if (gpu_set_voltage(platform, gpu_voltage) != 0) {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: failed to set voltage [%d]\n", __func__, gpu_voltage);
+		return -1;
+	}
+
+	if (platform->dvs_status)
+		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "dvs GPIO PMU status enable\n");
+
+	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "regulator initialized\n");
+
+	return 0;
+}
+#endif /* CONFIG_REGULATOR */
+
+int *get_mif_table(int *size)
+{
+	*size = ARRAY_SIZE(mif_min_table);
+	return mif_min_table;
+}
diff -Nur r5p0/platform/gpu_hwcnt.c r15p0/platform/exynos/gpu_hwcnt.c
--- r5p0/platform/gpu_hwcnt.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_hwcnt.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,543 +0,0 @@
-/* drivers/gpu/arm/.../platform/gpu_hwcnt.c
- *
- * Copyright 2011 by S.LSI. Samsung Electronics Inc.
- * San#24, Nongseo-Dong, Giheung-Gu, Yongin, Korea
- *
- * Samsung SoC Mali-T Series DVFS driver
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software FoundatIon.
- */
-
-/**
- * @file gpu_hwcnt.c
- * DVFS
- */
-#ifdef MALI_SEC_HWCNT
-
-#include <mali_kbase.h>
-#include <mali_kbase_mem_linux.h>
-#include "mali_kbase_platform.h"
-#include "gpu_hwcnt.h"
-
-mali_error exynos_gpu_hwcnt_update(struct kbase_device *kbdev)
-{
-	mali_error err = MALI_ERROR_FUNCTION_FAILED;
-	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
-	static int polling_period = 0;
-
-	KBASE_DEBUG_ASSERT(kbdev);
-
-	if ((!kbdev->hwcnt.enable_for_utilization) ||(!kbdev->hwcnt.is_init)) {
-		err = MALI_ERROR_NONE;
-		goto out;
-	}
-
-	if (kbdev->js_data.runpool_irq.secure_mode == MALI_TRUE) {
-		err = MALI_ERROR_NONE;
-		goto out;
-	}
-
-	polling_period -= platform->polling_speed;
-	if (polling_period > 0) {
-		err = MALI_ERROR_NONE;
-		goto out;
-	}
-
-#ifdef MALI_SEC_HWCNT_DUMP_DVFS_THREAD
-	if (kbdev->hwcnt.is_powered && kbdev->hwcnt.kctx) {
-		polling_period = platform->hwcnt_polling_speed;
-		err = hwcnt_dump(kbdev->hwcnt.kctx);
-		if (err != MALI_ERROR_NONE) {
-			GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "hwcnt dump error in %s %d \n", __FUNCTION__, err);
-			goto out;
-		}
-	}
-#endif
-	if (kbdev->hwcnt.is_powered && kbdev->hwcnt.kctx) {
-		hwcnt_get_utilization_resource(kbdev);
-		hwcnt_utilization_equation(kbdev);
-	} else {
-		hwcnt_value_clear(kbdev);
-	}
-
-out:
-	return err;
-}
-
-void hwcnt_value_clear(struct kbase_device *kbdev)
-{
-	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
-	platform->hwcnt_bt_clk = FALSE;
-	kbdev->hwcnt.cnt_for_bt_start = 0;
-	kbdev->hwcnt.cnt_for_bt_stop = 0;
-	kbdev->hwcnt.resources.arith_words = 0;
-	kbdev->hwcnt.resources.ls_issues = 0;
-	kbdev->hwcnt.resources.tex_issues = 0;
-}
-
-void hwcnt_utilization_equation(struct kbase_device *kbdev)
-{
-	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
-	int total_util;
-	unsigned int debug_util;
-
-	KBASE_DEBUG_ASSERT(kbdev);
-
-	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%d, %d, %d\n",
-			kbdev->hwcnt.resources.arith_words, kbdev->hwcnt.resources.ls_issues,
-			kbdev->hwcnt.resources.tex_issues);
-
-	total_util = kbdev->hwcnt.resources.arith_words * 25 +
-		kbdev->hwcnt.resources.ls_issues * 40 + kbdev->hwcnt.resources.tex_issues * 35;
-
-	debug_util = (kbdev->hwcnt.resources.arith_words << 24)
-		| (kbdev->hwcnt.resources.ls_issues << 16) | (kbdev->hwcnt.resources.tex_issues << 8);
-
-	if ((kbdev->hwcnt.resources.arith_words > 0) &&
-			(kbdev->hwcnt.resources.ls_issues == 0) && (kbdev->hwcnt.resources.tex_issues == 0)) {
-		platform->hwcnt_bt_clk = FALSE;
-		kbdev->hwcnt.cnt_for_bt_start = 0;
-		memset(kbdev->hwcnt.acc_buffer, 0, HWC_ACC_BUFFER_SIZE);
-		GPU_LOG(DVFS_WARNING, DUMMY, 0u, 0u,
-			"hwcnt info (clock %d, voltage %d\n", platform->cur_clock, platform->cur_voltage);
-		return;
-	}
-
-	if ((kbdev->hwcnt.resources.arith_words * 10 > kbdev->hwcnt.resources.ls_issues * 14) &&
-			(kbdev->hwcnt.resources.ls_issues < 40) && (total_util > 1000) && (total_util < 4800) &&
-			platform->cur_clock >= platform->gpu_max_clock_limit) {
-		kbdev->hwcnt.cnt_for_bt_start++;
-		kbdev->hwcnt.cnt_for_bt_stop = 0;
-		if (kbdev->hwcnt.cnt_for_bt_start > platform->hwcnt_up_step) {
-			platform->hwcnt_bt_clk = TRUE;
-			kbdev->hwcnt.cnt_for_bt_start = 0;
-		}
-	} else {
-		kbdev->hwcnt.cnt_for_bt_stop++;
-		kbdev->hwcnt.cnt_for_bt_start = 0;
-		if (kbdev->hwcnt.cnt_for_bt_stop > platform->hwcnt_down_step) {
-			platform->hwcnt_bt_clk = FALSE;
-			kbdev->hwcnt.cnt_for_bt_stop = 0;
-		}
-	}
-
-	if (platform->hwcnt_bt_clk == TRUE)
-		GPU_LOG(DVFS_INFO, LSI_HWCNT_BT_ON, 0u, debug_util, "hwcnt bt on\n");
-	else
-		GPU_LOG(DVFS_INFO, LSI_HWCNT_BT_OFF, 0u, debug_util, "hwcnt bt off\n");
-
-	memset(kbdev->hwcnt.acc_buffer, 0, HWC_ACC_BUFFER_SIZE);
-}
-
-mali_error hwcnt_get_utilization_resource(struct kbase_device *kbdev)
-{
-	int num_cores;
-	int mem_offset;
-	unsigned int *acc_addr;
-	u32 arith_words = 0, ls_issues = 0, tex_issues = 0, tripipe_active = 0, div_tripipe_active;
-	mali_error err = MALI_ERROR_FUNCTION_FAILED;
-	int i;
-
-	if (kbdev->gpu_props.num_core_groups != 1) {
-		/* num of core groups must be 1 on T76X */
-		goto out;
-	}
-
-	acc_addr = (unsigned int*)kbdev->hwcnt.acc_buffer;
-
-	num_cores = kbdev->gpu_props.num_cores;
-
-	if (num_cores <= 4)
-		mem_offset = MALI_SIZE_OF_HWCBLK * 3;
-	else
-		mem_offset = MALI_SIZE_OF_HWCBLK * 4;
-
-	for (i=0; i < num_cores; i++)
-	{
-		if ( i == 3 && (num_cores == 5 || num_cores == 6) )
-			mem_offset += MALI_SIZE_OF_HWCBLK;
-		tripipe_active += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_TRIPIPE_ACTIVE);
-		arith_words += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_ARITH_WORDS);
-		ls_issues += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_LS_ISSUES);
-		tex_issues += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_TEX_ISSUES);
-	}
-
-	div_tripipe_active = tripipe_active / 100;
-
-	if (div_tripipe_active) {
-		kbdev->hwcnt.resources.arith_words = arith_words / div_tripipe_active;
-		kbdev->hwcnt.resources.ls_issues = ls_issues / div_tripipe_active;
-		kbdev->hwcnt.resources.tex_issues = tex_issues / div_tripipe_active;
-	}
-
-	err = MALI_ERROR_NONE;
-
-out:
-	return err;
-}
-
-mali_error hwcnt_get_gpr_resource(struct kbase_device *kbdev, struct kbase_uk_hwcnt_gpr_dump *dump)
-{
-	int num_cores;
-	int mem_offset;
-	unsigned int* acc_addr;
-	u32 shader_20 = 0, shader_21 = 0;
-	mali_error err = MALI_ERROR_FUNCTION_FAILED;
-	int i;
-
-	KBASE_DEBUG_ASSERT(kbdev);
-
-	if (kbdev->gpu_props.num_core_groups != 1) {
-		/* num of core groups must be 1 on T76X */
-		goto out;
-	}
-
-	num_cores = kbdev->gpu_props.num_cores;
-	acc_addr = (unsigned int*)kbdev->hwcnt.acc_buffer;
-
-	if (num_cores <= 4)
-		mem_offset = MALI_SIZE_OF_HWCBLK * 3;
-	else
-		mem_offset = MALI_SIZE_OF_HWCBLK * 4;
-
-	for (i=0; i < num_cores; i++)
-	{
-		if ( i == 3 && (num_cores == 5 || num_cores == 6) )
-			mem_offset += MALI_SIZE_OF_HWCBLK;
-		shader_20 += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_SHADER_20);
-		shader_21 += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_SHADER_21);
-	}
-
-	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "[%d] [%d]\n", shader_20, shader_21);
-
-	dump->shader_20 = shader_20;
-	dump->shader_21 = shader_21;
-
-	memset(kbdev->hwcnt.acc_buffer, 0, HWC_ACC_BUFFER_SIZE);
-	err = MALI_ERROR_NONE;
-
-out:
-	return err;
-}
-
-void hwcnt_accumulate_resource(struct kbase_device *kbdev)
-{
-	unsigned int *addr, *acc_addr;
-	int i;
-
-	KBASE_DEBUG_ASSERT(kbdev);
-
-	if ((!kbdev->hwcnt.kctx) || (!kbdev->hwcnt.acc_buffer))
-		return;
-
-	addr = (unsigned int *)kbdev->hwcnt.kspace_addr;
-	acc_addr = (unsigned int*)kbdev->hwcnt.acc_buffer;
-
-	__flush_dcache_area((void *)addr, HWC_ACC_BUFFER_SIZE);
-
-	/* following copy code will be optimized soon */
-	for (i=0; i < HWC_ACC_BUFFER_SIZE / 4; i++)
-	{
-		*(acc_addr + i) += *(addr + i);
-	}
-}
-
-mali_error hwcnt_dump(struct kbase_context *kctx)
-{
-	mali_error err = MALI_ERROR_FUNCTION_FAILED;
-	struct kbase_device *kbdev;
-
-	if (kctx == NULL) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "kctx is NULL error in %s %d \n", __FUNCTION__, err);
-		goto out;
-	}
-
-	kbdev = kctx->kbdev;
-
-	if ((!kbdev->hwcnt.kctx) || (!kbdev->hwcnt.is_init) || (!kbdev->hwcnt.is_powered )) {
-		err = MALI_ERROR_NONE;
-		goto out;
-	}
-
-	err = kbase_instr_hwcnt_dump(kbdev->hwcnt.kctx);
-
-	if (err != MALI_ERROR_NONE) {
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "kbase_instr_hwcnt_dump error in %s %d \n", __FUNCTION__, err);
-		goto out;
-	}
-
-	err = kbase_instr_hwcnt_clear(kbdev->hwcnt.kctx);
-
-	if (err != MALI_ERROR_NONE) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "kbase_instr_hwcnt_clear error in %s %d \n", __FUNCTION__, err);
-		goto out;
-	}
-
-	hwcnt_accumulate_resource(kbdev);
-
-	err = MALI_ERROR_NONE;
-
- out:
-	return err;
-}
-
-void hwcnt_start(struct kbase_device *kbdev)
-{
-	struct kbase_context *kctx;
-	KBASE_DEBUG_ASSERT(kbdev);
-
-	kbdev->hwcnt.is_stoped = FALSE;
-
-	if (kbdev->hwcnt.suspended_kctx) {
-		kctx = kbdev->hwcnt.suspended_kctx;
-		kbdev->hwcnt.suspended_kctx = NULL;
-	} else {
-		kctx = kbdev->hwcnt.kctx;
-		if (kctx == NULL)
-		{
-			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "hwcnt exception!!!! Both suspended_ktctx and kctx are NULL\n");
-			BUG();
-		}
-	}
-
-	if (kctx) {
-		mali_error err;
-		err = kbase_instr_hwcnt_enable_internal_sec(kbdev, kctx, &kbdev->hwcnt.suspended_state, false);
-
-		if (err != MALI_ERROR_NONE)
-		{
-			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "Failed to restore instrumented hardware counters on resume\n");
-			kbdev->hwcnt.kctx = kctx;
-		}
-	}
-}
-
-void hwcnt_stop(struct kbase_device *kbdev)
-{
-	struct kbase_context *kctx;
-	KBASE_DEBUG_ASSERT(kbdev);
-
-	kctx = kbdev->hwcnt.kctx;
-	kbdev->hwcnt.is_stoped = TRUE;
-
-	if (kctx == NULL)
-	{
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "hwcnt exception!!!! kctx is NULL\n");
-		return;
-	}
-
-	kbdev->hwcnt.suspended_kctx = kctx;
-
-	/* Relevant state was saved into hwcnt.suspended_state when enabling the
-	 * counters */
-
-	if (kctx)
-	{
-		KBASE_DEBUG_ASSERT(kctx->jctx.sched_info.ctx.flags & KBASE_CTX_FLAG_PRIVILEGED);
-		kbase_instr_hwcnt_disable_sec(kctx);
-	}
-}
-
-/**
- * @brief Configure HW counters collection
- */
-mali_error hwcnt_setup(struct kbase_context *kctx, struct kbase_uk_hwcnt_setup *setup)
-{
-	mali_error err = MALI_ERROR_FUNCTION_FAILED;
-	struct kbase_device *kbdev;
-	struct exynos_context *platform;
-
-	KBASE_DEBUG_ASSERT(NULL != kctx);
-
-	kbdev = kctx->kbdev;
-	KBASE_DEBUG_ASSERT(NULL != kbdev);
-
-	if (NULL == setup) {
-		/* Bad parameter - abort */
-		goto out;
-	}
-
-	platform = (struct exynos_context *) kbdev->platform_context;
-
-	if ((!platform->hwcnt_gathering_status) && (!platform->hwcnt_gpr_status)) {
-		err = MALI_ERROR_NONE;
-		goto out;
-	}
-
-	if ((platform->hwcnt_gpr_status) && (setup->padding == HWC_MODE_GPR_EN)) {
-
-		if (!kbdev->hwcnt.is_init) {
-			err = MALI_ERROR_NONE;
-			goto out;
-		}
-
-		mutex_lock(&kbdev->hwcnt.mlock);
-
-		kbdev->hwcnt.s_enable_for_utilization = kbdev->hwcnt.enable_for_utilization;
-		kbdev->hwcnt.enable_for_utilization = FALSE;
-		kbdev->hwcnt.enable_for_gpr = TRUE;
-		kbdev->hwcnt.kctx_gpr = kctx;
-		platform->hwcnt_bt_clk = 0;
-
-		KBASE_TRACE_ADD_EXYNOS(kbdev, LSI_HWCNT_ON_GPR, NULL, NULL, 0u, (long unsigned int)kbdev->hwcnt.kspace_addr);
-
-		err = MALI_ERROR_NONE;
-
-		mutex_unlock(&kbdev->hwcnt.mlock);
-		goto out;
-	} else if ((platform->hwcnt_gpr_status) && (setup->padding == HWC_MODE_GPR_DIS)) {
-		if (!kbdev->hwcnt.is_init) {
-			err = MALI_ERROR_NONE;
-			goto out;
-		}
-
-		mutex_lock(&kbdev->hwcnt.mlock);
-
-		KBASE_TRACE_ADD_EXYNOS(kbdev, LSI_HWCNT_OFF_GPR, NULL, NULL, 0u, (long unsigned int)kbdev->hwcnt.kspace_addr);
-
-		kbdev->hwcnt.enable_for_gpr = FALSE;
-		kbdev->hwcnt.enable_for_utilization = kbdev->hwcnt.s_enable_for_utilization;
-		kbdev->hwcnt.kctx_gpr = NULL;
-		platform->hwcnt_bt_clk = 0;
-		err = MALI_ERROR_NONE;
-
-		mutex_unlock(&kbdev->hwcnt.mlock);
-		goto out;
-	}
-
-	if ((setup->dump_buffer != 0ULL) && (setup->padding == HWC_MODE_UTILIZATION)) {
-		err = kbase_instr_hwcnt_enable_internal_sec(kbdev, kctx, setup, true);
-		mutex_lock(&kbdev->hwcnt.mlock);
-
-		if (kbdev->hwcnt.kctx)
-			hwcnt_stop(kbdev);
-
-		kbdev->hwcnt.enable_for_utilization = TRUE;
-		platform->hwcnt_bt_clk = 0;
-		kbdev->hwcnt.cnt_for_stop = 0;
-		kbdev->hwcnt.cnt_for_bt_stop = 0;
-		mutex_unlock(&kbdev->hwcnt.mlock);
-	}
-
-out:
-	return err;
-}
-
-void exynos_hwcnt_init(struct kbase_device *kbdev)
-{
-	struct kbase_uk_hwcnt_setup setup_arg;
-	struct kbase_context *kctx;
-	struct kbase_uk_mem_alloc mem;
-	struct kbase_va_region *reg;
-	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
-
-	if (platform->hwcnt_gathering_status == false)
-		goto out;
-
-	kctx = kbase_create_context(kbdev, false);
-
-	if (kctx) {
-		kbdev->hwcnt.kctx = kctx;
-	} else {
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "hwcnt error!, hwcnt_init is failed\n");
-		goto out;
-	}
-
-	mem.va_pages = mem.commit_pages = mem.extent = 1;
-	mem.flags = BASE_MEM_PROT_GPU_WR | BASE_MEM_PROT_CPU_RD | BASE_MEM_HINT_CPU_RD;
-
-	reg = kbase_mem_alloc(kctx, mem.va_pages, mem.commit_pages, mem.extent, &mem.flags, &mem.gpu_va, &mem.va_alignment);
-
-#if defined(CONFIG_64BIT)
-	kbase_gpu_vm_lock(kctx);
-	if (MALI_ERROR_NONE != kbase_gpu_mmap(kctx, reg, 0, 1, 1)) {
-		kbase_gpu_vm_unlock(kctx);
-		platform->hwcnt_gathering_status = false;
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "exynos_hwcnt_init error!mmap fail\n");
-		kbase_mem_free(kbdev->hwcnt.kctx, kbdev->hwcnt.suspended_state.dump_buffer);
-		goto out;
-	}
-	kbase_gpu_vm_unlock(kctx);
-#endif
-
-	kctx->kbdev->hwcnt.phy_addr = reg->alloc->pages[0];
-	kctx->kbdev->hwcnt.enable_for_utilization = FALSE;
-	kctx->kbdev->hwcnt.enable_for_gpr = FALSE;
-	kctx->kbdev->hwcnt.suspended_kctx = NULL;
-	kctx->kbdev->hwcnt.timeout = (unsigned int)msecs_to_jiffies(100);
-	kctx->kbdev->hwcnt.is_powered = FALSE;
-	kctx->kbdev->hwcnt.is_stoped = TRUE;
-	mutex_init(&kbdev->hwcnt.mlock);
-
-#if defined(CONFIG_64BIT)
-	setup_arg.dump_buffer = reg->start_pfn << PAGE_SHIFT;
-#else
-	setup_arg.dump_buffer = mem.gpu_va;
-#endif
-	setup_arg.jm_bm =  platform->hwcnt_choose_jm;
-	setup_arg.shader_bm = platform->hwcnt_choose_shader;
-	setup_arg.tiler_bm =  platform->hwcnt_choose_tiler;
-	setup_arg.l3_cache_bm =  platform->hwcnt_choose_l3_cache;
-	setup_arg.mmu_l2_bm =  platform->hwcnt_choose_mmu_l2;
-	setup_arg.padding = HWC_MODE_UTILIZATION;
-
-	kctx->kbdev->hwcnt.kspace_addr = kbase_kmap_from_physical_address(kbdev);
-
-	if (MALI_ERROR_NONE != hwcnt_setup(kctx, &setup_arg)) {
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "hwcnt_setup is failed\n");
-		goto out;
-	}
-
-	kctx->kbdev->hwcnt.acc_buffer = kmalloc(HWC_ACC_BUFFER_SIZE, GFP_KERNEL);
-
-	if (kctx->kbdev->hwcnt.acc_buffer)
-		memset(kctx->kbdev->hwcnt.acc_buffer, 0, HWC_ACC_BUFFER_SIZE);
-	else
-		goto out;
-
-	kbdev->hwcnt.is_init = TRUE;
-	if(kbdev->pm.pm_current_policy->id == KBASE_PM_POLICY_ID_ALWAYS_ON) {
-		mutex_lock(&kbdev->hwcnt.mlock);
-		if (!kbdev->hwcnt.kctx)
-			hwcnt_start(kbdev);
-		mutex_unlock(&kbdev->hwcnt.mlock);
-	}
-	return;
-out:
-	kbdev->hwcnt.is_init = FALSE;
-	return;
-}
-
-void exynos_hwcnt_remove(struct kbase_device *kbdev)
-{
-	struct exynos_context *platform;
-
-	if (!kbdev->hwcnt.is_init)
-		return;
-
-	if (kbdev->hwcnt.kctx && kbdev->hwcnt.suspended_state.dump_buffer)
-		kbase_mem_free(kbdev->hwcnt.kctx, kbdev->hwcnt.suspended_state.dump_buffer);
-
-	if (kbdev->hwcnt.acc_buffer)
-		kfree(kbdev->hwcnt.acc_buffer);
-
-	platform = (struct exynos_context *) kbdev->platform_context;
-
-	kbdev->hwcnt.enable_for_gpr = FALSE;
-	kbdev->hwcnt.enable_for_utilization = FALSE;
-	kbdev->hwcnt.kctx_gpr = NULL;
-	kbdev->hwcnt.kctx = NULL;
-	kbdev->hwcnt.is_stoped = TRUE;
-	kbdev->hwcnt.is_init = FALSE;
-	platform->hwcnt_bt_clk = 0;
-
-	if (kbdev->hwcnt.kspace_addr) {
-		kbase_kunmap_from_physical_address(kbdev);
-		kbdev->hwcnt.kspace_addr = 0;
-	}
-
-	mutex_destroy(&kbdev->hwcnt.mlock);
-}
-#endif
diff -Nur r5p0/platform/gpu_hwcnt.h r15p0/platform/exynos/gpu_hwcnt.h
--- r5p0/platform/gpu_hwcnt.h	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_hwcnt.h	1970-01-01 01:00:00.000000000 +0100
@@ -1,108 +0,0 @@
-/* drivers/gpu/arm/.../platform/gpu_hwcnt.h
- *
- * Copyright 2011 by S.LSI. Samsung Electronics Inc.
- * San#24, Nongseo-Dong, Giheung-Gu, Yongin, Korea
- *
- * Samsung SoC Mali-T Series DVFS driver
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software FoundatIon.
- */
-
-/**
- * @file gpu_hwcnt.h
- * DVFS
- */
-#ifdef MALI_SEC_HWCNT
-
-#ifndef __GPU_HWCNT_H
-#define __GPU_HWCNT_H __FILE__
-
-#include <platform/mali_kbase_platform.h>
-
-#define MALI_SEC_HWCNT_DUMP_DVFS_THREAD
-
-#define MALI_SIZE_OF_HWCBLK 64
-
-enum HWCNT_OFFSET {
-	OFFSET_SHADER_20 = 20,
-	OFFSET_SHADER_21 = 21,
-	OFFSET_TRIPIPE_ACTIVE = 26,
-	OFFSET_ARITH_WORDS = 27,
-	OFFSET_LS_ISSUES = 32,
-	OFFSET_TEX_ISSUES = 42,
-};
-
-#define LV1_SHIFT       20
-#define LV2_BASE_MASK       0x3ff
-#define LV2_PT_MASK     0xff000
-#define LV2_SHIFT       12
-#define LV1_DESC_MASK       0x3
-#define LV2_DESC_MASK       0x2
-
-#define HWC_MODE_UTILIZATION 0x80510010
-#define HWC_MODE_GPR_EN 0x80510001
-#define HWC_MODE_GPR_DIS 0x80510002
-
-#define HWC_ACC_BUFFER_SIZE     4096		// bytes
-
-static inline unsigned long kbase_virt_to_phys(struct mm_struct *mm, unsigned long vaddr)
-{
-	unsigned long *pgd;
-	unsigned long *lv1d, *lv2d;
-
-	pgd = (unsigned long *)mm->pgd;
-
-	lv1d = pgd + (vaddr >> LV1_SHIFT);
-
-	if ((*lv1d & LV1_DESC_MASK) != 0x1) {
-		printk("invalid LV1 descriptor, "
-				"pgd %p lv1d 0x%lx vaddr 0x%lx\n",
-				pgd, *lv1d, vaddr);
-		return 0;
-	}
-
-	lv2d = (unsigned long *)phys_to_virt(*lv1d & ~LV2_BASE_MASK) +
-		((vaddr & LV2_PT_MASK) >> LV2_SHIFT);
-
-	if ((*lv2d & LV2_DESC_MASK) != 0x2) {
-		printk("invalid LV2 descriptor, "
-				"pgd %p lv2d 0x%lx vaddr 0x%lx\n",
-				pgd, *lv2d, vaddr);
-		return 0;
-	}
-
-	return (*lv2d & PAGE_MASK) | (vaddr & (PAGE_SIZE-1));
-}
-
-static inline void* kbase_kmap_from_physical_address(struct kbase_device *kbdev)
-{
-	return kmap(pfn_to_page(PFN_DOWN(kbdev->hwcnt.phy_addr)));
-}
-
-static inline void kbase_kunmap_from_physical_address(struct kbase_device *kbdev)
-{
-	return kunmap(pfn_to_page(PFN_DOWN(kbdev->hwcnt.phy_addr)));
-}
-
-extern mali_error kbase_instr_hwcnt_util_dump(struct kbase_device *kbdev);
-
-mali_error exynos_gpu_hwcnt_update(struct kbase_device *kbdev);
-
-bool hwcnt_check_conditions(struct kbase_device *kbdev);
-void hwcnt_value_clear(struct kbase_device *kbdev);
-void hwcnt_utilization_equation(struct kbase_device *kbdev);
-mali_error hwcnt_get_utilization_resource(struct kbase_device *kbdev);
-mali_error hwcnt_get_gpr_resource(struct kbase_device *kbdev, struct kbase_uk_hwcnt_gpr_dump *dump);
-extern void hwcnt_accumulate_resource(struct kbase_device *kbdev);
-mali_error hwcnt_dump(struct kbase_context *kctx);
-void hwcnt_start(struct kbase_device *kbdev);
-void hwcnt_stop(struct kbase_device *kbdev);
-mali_error hwcnt_setup(struct kbase_context *kctx, struct kbase_uk_hwcnt_setup *setup);
-void exynos_hwcnt_init(struct kbase_device *kbdev);
-void exynos_hwcnt_remove(struct kbase_device *kbdev);
-
-#endif /* __GPU_HWCNT_H */
-
-#endif
diff -Nur r5p0/platform/gpu_hwcnt_sec.c r15p0/platform/exynos/gpu_hwcnt_sec.c
--- r5p0/platform/gpu_hwcnt_sec.c	1970-01-01 01:00:00.000000000 +0100
+++ r15p0/platform/exynos/gpu_hwcnt_sec.c	2017-07-20 16:14:04.620559419 +0200
@@ -0,0 +1,351 @@
+/* drivers/gpu/arm/.../platform/gpu_hwcnt_sec.c
+ *
+ * Copyright 2011 by S.LSI. Samsung Electronics Inc.
+ * San#24, Nongseo-Dong, Giheung-Gu, Yongin, Korea
+ *
+ * Samsung SoC Mali-T Series DVFS driver
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software FoundatIon.
+ */
+
+/**
+ * @file gpu_hwcnt_sec.c
+ * DVFS
+ */
+#include <mali_kbase.h>
+#include <mali_kbase_mem_linux.h>
+#include <backend/gpu/mali_kbase_pm_policy.h>
+#include <mali_kbase_vinstr.h>
+#include "mali_kbase_platform.h"
+#include "gpu_hwcnt_sec.h"
+
+
+/* MALI_SEC_INTEGRATION */
+void dvfs_hwcnt_attach(void *dev)
+{
+	struct kbase_device * kbdev = (struct kbase_device *)dev;
+	struct exynos_context *platform;
+	struct kbase_vinstr_client *vinstr_cli;
+	u32 bitmap[4];
+
+	// if secure rendering set this flag, do not attach sec hwcnt
+	if (kbdev->hwcnt.is_hwcnt_force_stop == true)
+		return;
+
+	platform = (struct exynos_context *) kbdev->platform_context;
+
+	bitmap[SHADER_HWCNT_BM] = platform->hwcnt_choose_shader;
+	bitmap[TILER_HWCNT_BM] = platform->hwcnt_choose_tiler;
+	bitmap[MMU_L2_HWCNT_BM] = platform->hwcnt_choose_mmu_l2;
+	bitmap[JM_HWCNT_BM] = platform->hwcnt_choose_jm;
+
+	vinstr_cli = kbasep_vinstr_attach_client_sec(kbdev->vinstr_ctx, 0, bitmap, (void *)(long)kbdev->hwcnt.hwcnt_fd, NULL);
+	if (vinstr_cli == NULL || kbdev->hwcnt.kctx == NULL)
+	{
+		GPU_LOG(DVFS_WARNING, DUMMY, 0u, 0u, "skip attach hwcnt \n");
+		return;
+	}
+	kbdev->hwcnt.kctx->vinstr_cli = vinstr_cli;
+
+	kbase_vinstr_disable(kbdev->vinstr_ctx);
+	kbase_pm_context_idle(kbdev);
+
+	kbdev->hwcnt.is_hwcnt_enable = false;
+	kbdev->hwcnt.is_hwcnt_gpr_enable = false;
+	kbdev->hwcnt.is_hwcnt_force_stop = false;
+	kbdev->hwcnt.is_hwcnt_attach = true;
+}
+
+void dvfs_hwcnt_update(void *dev)
+{
+	struct kbase_device * kbdev = (struct kbase_device *)dev;
+	struct exynos_context *platform;
+
+	platform = (struct exynos_context *) kbdev->platform_context;
+
+	if ((!platform->hwcnt_gathering_status) && (!platform->hwcnt_gpr_status))
+		return;
+
+	if (kbdev->hwcnt.kctx != NULL && kbdev->hwcnt.is_hwcnt_attach == true && kbdev->hwcnt.is_hwcnt_enable == true) {
+		kbase_vinstr_hwc_dump(kbdev->hwcnt.kctx->vinstr_cli, 0);
+	}
+}
+
+void dvfs_hwcnt_detach(void *dev)
+{
+	struct kbase_device * kbdev = (struct kbase_device *)dev;
+
+	if (kbdev->hwcnt.kctx != NULL) {
+		// disable hwcnt before detach
+		dvfs_hwcnt_disable(dev);
+
+		kbase_vinstr_detach_client_sec(kbdev->hwcnt.kctx->vinstr_cli);
+		kbdev->hwcnt.is_hwcnt_attach = false;
+		kbdev->hwcnt.is_hwcnt_enable = false;
+		dvfs_hwcnt_clear_tripipe(kbdev);
+		kbdev->hwcnt.is_hwcnt_gpr_enable = false;
+	}
+
+}
+
+void dvfs_hwcnt_force_start(void *dev)
+{
+	struct kbase_device * kbdev = (struct kbase_device *)dev;
+
+	kbdev->hwcnt.is_hwcnt_force_stop = false;
+	kbdev->hwcnt.is_hwcnt_attach = true;
+}
+
+void dvfs_hwcnt_force_stop(void *dev)
+{
+	struct kbase_device * kbdev = (struct kbase_device *)dev;
+
+	if (kbdev->hwcnt.is_hwcnt_attach == true)
+		dvfs_hwcnt_disable(dev);
+
+	kbdev->hwcnt.is_hwcnt_force_stop = true;
+	kbdev->hwcnt.is_hwcnt_attach = false;
+}
+
+void dvfs_hwcnt_enable(void *dev)
+{
+	struct kbase_device *kbdev = (struct kbase_device *)dev;
+	struct exynos_context *platform;
+
+	if (kbdev->hwcnt.is_hwcnt_attach == false)
+		return;
+
+	platform = (struct exynos_context *) kbdev->platform_context;
+	if (platform->dvs_is_enabled == false && kbdev->hwcnt.is_hwcnt_enable == false)
+	{
+		if (!kbase_vinstr_enable(kbdev->vinstr_ctx))
+			kbdev->hwcnt.is_hwcnt_enable = true;
+		else
+			kbdev->hwcnt.backend.state = KBASE_INSTR_STATE_DISABLED;
+	}
+}
+
+void dvfs_hwcnt_disable(void *dev)
+{
+	struct kbase_device * kbdev = (struct kbase_device *)dev;
+	struct exynos_context *platform;
+
+	if (kbdev->hwcnt.is_hwcnt_attach == false)
+		return;
+
+	platform = (struct exynos_context *) kbdev->platform_context;
+	if (kbdev->hwcnt.kctx != NULL && platform->dvs_is_enabled == false && kbdev->hwcnt.is_hwcnt_enable == true)
+	{
+		kbase_vinstr_disable(kbdev->vinstr_ctx);
+		kbdev->hwcnt.is_hwcnt_enable = false;
+	}
+}
+
+void dvfs_hwcnt_get_resource(struct kbase_device *kbdev)
+{
+	int num_cores;
+	int mem_offset;
+	unsigned int *acc_addr;
+	u64 arith_words = 0, ls_issues = 0, tex_issues = 0, tripipe_active = 0, div_tripipe_active = 0;
+#ifdef CONFIG_MALI_SEC_HWCNT_VERT
+	u64 gpu_active = 0, js0_active = 0, tiler_active = 0, external_read_bits = 0;
+#endif
+	int i;
+
+	struct exynos_context *platform;
+
+	platform = (struct exynos_context *) kbdev->platform_context;
+
+	if (!platform->hwcnt_gathering_status)
+		return;
+
+	if (kbdev->gpu_props.num_core_groups != 1) {
+		/* num of core groups must be 1 on T76X */
+		return;
+	}
+
+	acc_addr = (unsigned int*)kbase_vinstr_get_addr(kbdev);
+
+	num_cores = kbdev->gpu_props.num_cores;
+
+	if (num_cores <= 4)
+		mem_offset = MALI_SIZE_OF_HWCBLK * 3;
+	else
+		mem_offset = MALI_SIZE_OF_HWCBLK * 4;
+
+#ifdef CONFIG_MALI_SEC_HWCNT_VERT
+	gpu_active = *(acc_addr + 6);
+	js0_active = *(acc_addr + 10);
+	tiler_active = *(acc_addr + MALI_SIZE_OF_HWCBLK + 45);
+	external_read_bits = *(acc_addr + 2*MALI_SIZE_OF_HWCBLK + 31);
+#endif
+
+	for (i=0; i < num_cores; i++)
+	{
+		if ( i == 3 && (num_cores == 5 || num_cores == 6) )
+			mem_offset += MALI_SIZE_OF_HWCBLK;
+		tripipe_active += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_TRIPIPE_ACTIVE);
+		arith_words += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_ARITH_WORDS);
+		ls_issues += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_LS_ISSUES);
+		tex_issues += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_TEX_ISSUES);
+	}
+
+	div_tripipe_active = tripipe_active / 100;
+
+	kbdev->hwcnt.resources.arith_words += arith_words;
+	kbdev->hwcnt.resources.ls_issues += ls_issues;
+	kbdev->hwcnt.resources.tex_issues += tex_issues;
+	kbdev->hwcnt.resources.tripipe_active += tripipe_active;
+#ifdef CONFIG_MALI_SEC_HWCNT_VERT
+	kbdev->hwcnt.resources.gpu_active += gpu_active;
+	kbdev->hwcnt.resources.js0_active += js0_active;
+	kbdev->hwcnt.resources.tiler_active += tiler_active;
+	kbdev->hwcnt.resources.external_read_bits += external_read_bits;
+#endif
+}
+
+void dvfs_hwcnt_clear_tripipe(struct kbase_device *kbdev)
+{
+	kbdev->hwcnt.resources.arith_words = 0;
+	kbdev->hwcnt.resources.ls_issues  = 0;
+	kbdev->hwcnt.resources.tex_issues = 0;
+	kbdev->hwcnt.resources.tripipe_active = 0;
+#ifdef CONFIG_MALI_SEC_HWCNT_VERT
+	kbdev->hwcnt.resources.gpu_active = 0;
+	kbdev->hwcnt.resources.js0_active = 0;
+	kbdev->hwcnt.resources.tiler_active = 0;
+	kbdev->hwcnt.resources.external_read_bits = 0;
+#endif
+}
+
+void dvfs_hwcnt_utilization_equation(struct kbase_device *kbdev)
+{
+	struct exynos_context *platform;
+	int total_util;
+	unsigned int debug_util;
+	u64 arith, ls, tex, tripipe;
+#ifdef CONFIG_MALI_SEC_HWCNT_VERT
+	static int vertex_count = 0;
+	static int vertex_inc = 0;
+#endif
+
+	platform = (struct exynos_context *) kbdev->platform_context;
+
+	tripipe = kbdev->hwcnt.resources.tripipe_active / 100;
+
+	arith = kbdev->hwcnt.resources.arith_words / tripipe;
+	ls = kbdev->hwcnt.resources.ls_issues / tripipe;
+	tex = kbdev->hwcnt.resources.tex_issues / tripipe;
+
+	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%llu, %llu, %llu, %llu\n", tripipe, arith, ls, tex);
+#ifdef CONFIG_MALI_SEC_HWCNT_VERT
+	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "%llu, %llu, %llu, %llu\n", kbdev->hwcnt.resources.gpu_active, kbdev->hwcnt.resources.js0_active, kbdev->hwcnt.resources.tiler_active, kbdev->hwcnt.resources.external_read_bits);
+#endif
+
+	total_util = arith * 25 + ls * 40 + tex * 35;
+	debug_util = (arith << 24) | (ls << 16) | (tex << 8);
+
+	if ((arith > 0) && (ls == 0) && (tex == 0)) {
+		platform->hwcnt_bt_clk = false;
+		kbdev->hwcnt.cnt_for_bt_start = 0;
+		return;
+	}
+	if ((arith * 10 > ls * 14) && (ls < 40) && (total_util > 1000) && (total_util < 4800) &&
+			(platform->cur_clock >= platform->gpu_max_clock_limit)) {
+		kbdev->hwcnt.cnt_for_bt_start++;
+		kbdev->hwcnt.cnt_for_bt_stop = 0;
+		if (kbdev->hwcnt.cnt_for_bt_start > platform->hwcnt_up_step) {
+			platform->hwcnt_bt_clk = true;
+			kbdev->hwcnt.cnt_for_bt_start = 0;
+		}
+	}
+	else {
+		kbdev->hwcnt.cnt_for_bt_stop++;
+		kbdev->hwcnt.cnt_for_bt_start = 0;
+		if (kbdev->hwcnt.cnt_for_bt_stop > platform->hwcnt_down_step) {
+			platform->hwcnt_bt_clk = false;
+			kbdev->hwcnt.cnt_for_bt_stop = 0;
+		}
+	}
+	if (platform->hwcnt_bt_clk == true)
+		GPU_LOG(DVFS_INFO, LSI_HWCNT_BT_ON, 0u, debug_util, "hwcnt bt on\n");
+	else
+		GPU_LOG(DVFS_INFO, LSI_HWCNT_BT_OFF, 0u, debug_util, "hwcnt bt off\n");
+#ifdef CONFIG_MALI_SEC_HWCNT_VERT
+	if ((kbdev->hwcnt.resources.external_read_bits > 8500000) && ((kbdev->hwcnt.resources.js0_active * 100 / kbdev->hwcnt.resources.gpu_active) > 95)
+			&& (kbdev->hwcnt.resources.tiler_active < 20000000)) {
+		vertex_inc++;
+		if( vertex_inc > 4) {
+			platform->hwcnt_allow_vertex_throttle = 1;
+			vertex_count = 10;
+			vertex_inc = 5;
+		}
+	}
+	else {
+		vertex_count--;
+		if(vertex_count < 0) {
+			platform->hwcnt_allow_vertex_throttle = 0;
+			vertex_count = 0;
+			vertex_inc = 0;
+		}
+	}
+#endif
+	kbdev->hwcnt.resources_log = kbdev->hwcnt.resources;
+
+	dvfs_hwcnt_clear_tripipe(kbdev);
+}
+
+void dvfs_hwcnt_gpr_enable(struct kbase_device *kbdev, bool flag)
+{
+	kbdev->hwcnt.is_hwcnt_gpr_enable = flag;
+}
+
+void dvfs_hwcnt_get_gpr_resource(struct kbase_device *kbdev, struct kbase_uk_hwcnt_gpr_dump *dump)
+{
+	int num_cores;
+	int mem_offset;
+	unsigned int *acc_addr;
+	u32 shader_20 = 0, shader_21 = 0;
+	int i;
+
+	struct exynos_context *platform;
+
+	platform = (struct exynos_context *) kbdev->platform_context;
+
+	/* set default values */
+	dump->shader_20 = 0xF;
+	dump->shader_21 = 0x1;
+
+	if (!platform->hwcnt_gpr_status)
+		return;
+
+	if (!kbdev->hwcnt.is_hwcnt_gpr_enable)
+		return;
+
+	if (kbdev->gpu_props.num_core_groups != 1)
+		return;
+
+	acc_addr = (unsigned int*)kbase_vinstr_get_addr(kbdev);
+
+	num_cores = kbdev->gpu_props.num_cores;
+
+	if (num_cores <= 4)
+		mem_offset = MALI_SIZE_OF_HWCBLK * 3;
+	else
+		mem_offset = MALI_SIZE_OF_HWCBLK * 4;
+
+	for (i=0; i < num_cores; i++)
+	{
+		if ( i == 3 && (num_cores == 5 || num_cores == 6) )
+			mem_offset += MALI_SIZE_OF_HWCBLK;
+		shader_20 += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_SHADER_20);
+		shader_21 += *(acc_addr + mem_offset + MALI_SIZE_OF_HWCBLK * i + OFFSET_SHADER_21);
+	}
+
+	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "[%d] [%d]\n", shader_20, shader_21);
+
+	dump->shader_20 = shader_20;
+	dump->shader_21 = shader_21;
+}
diff -Nur r5p0/platform/gpu_hwcnt_sec.h r15p0/platform/exynos/gpu_hwcnt_sec.h
--- r5p0/platform/gpu_hwcnt_sec.h	1970-01-01 01:00:00.000000000 +0100
+++ r15p0/platform/exynos/gpu_hwcnt_sec.h	2017-07-20 16:14:04.620559419 +0200
@@ -0,0 +1,47 @@
+/* drivers/gpu/arm/.../platform/gpu_hwcnt_sec.h
+ *
+ * Copyright 2011 by S.LSI. Samsung Electronics Inc.
+ * San#24, Nongseo-Dong, Giheung-Gu, Yongin, Korea
+ *
+ * Samsung SoC Mali-T Series DVFS driver
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software FoundatIon.
+ */
+
+/**
+ * @file gpu_hwcnt_sec.h
+ * DVFS
+ */
+#ifndef __GPU_HWCNT_SEC_H
+#define __GPU_HWCNT_SEC_H __FILE__
+
+#define MALI_SIZE_OF_HWCBLK 64
+
+#define HWC_MODE_GPR_EN 0x80510001
+#define HWC_MODE_GPR_DIS 0x80510002
+
+enum HWCNT_OFFSET {
+	OFFSET_SHADER_20 = 20,
+	OFFSET_SHADER_21 = 21,
+	OFFSET_TRIPIPE_ACTIVE = 26,
+	OFFSET_ARITH_WORDS = 27,
+	OFFSET_LS_ISSUES = 32,
+	OFFSET_TEX_ISSUES = 42,
+};
+
+void dvfs_hwcnt_attach(void *dev);
+void dvfs_hwcnt_update(void *dev);
+void dvfs_hwcnt_detach(void *dev);
+void dvfs_hwcnt_enable(void *dev);
+void dvfs_hwcnt_disable(void *dev);
+void dvfs_hwcnt_force_start(void *dev);
+void dvfs_hwcnt_force_stop(void *dev);
+void dvfs_hwcnt_gpr_enable(struct kbase_device *kbdev, bool flag);
+void dvfs_hwcnt_get_gpr_resource(struct kbase_device *kbdev, struct kbase_uk_hwcnt_gpr_dump *dump);
+void dvfs_hwcnt_get_resource(struct kbase_device *kbdev);
+void dvfs_hwcnt_clear_tripipe(struct kbase_device *kbdev);
+void dvfs_hwcnt_utilization_equation(struct kbase_device *kbdev);
+
+#endif
diff -Nur r5p0/platform/gpu_integration_callbacks.c r15p0/platform/exynos/gpu_integration_callbacks.c
--- r5p0/platform/gpu_integration_callbacks.c	1970-01-01 01:00:00.000000000 +0100
+++ r15p0/platform/exynos/gpu_integration_callbacks.c	2017-07-20 16:14:04.000000000 +0200
@@ -0,0 +1,1174 @@
+/* drivers/gpu/arm/.../platform/gpu_integration_callbacks.c
+ *
+ * Copyright 2011 by S.LSI. Samsung Electronics Inc.
+ * San#24, Nongseo-Dong, Giheung-Gu, Yongin, Korea
+ *
+ * Samsung SoC Mali-T Series DDK porting layer
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software FoundatIon.
+ */
+
+/**
+ * @file gpu_integration_callbacks.c
+ * DDK porting layer.
+ */
+
+#include <mali_kbase.h>
+#include <mali_midg_regmap.h>
+#include <mali_kbase_sync.h>
+
+#include <linux/pm_qos.h>
+#include <linux/sched.h>
+
+#include <mali_kbase_gpu_memory_debugfs.h>
+#include <backend/gpu/mali_kbase_device_internal.h>
+
+/* MALI_SEC_INTEGRATION */
+#define KBASE_REG_CUSTOM_TMEM       (1ul << 19)
+#define KBASE_REG_CUSTOM_PMEM       (1ul << 20)
+
+#define ENTRY_TYPE_MASK     3ULL
+#define ENTRY_IS_ATE        1ULL
+#define ENTRY_IS_INVAL      2ULL
+#define ENTRY_IS_PTE        3ULL
+
+#define ENTRY_ATTR_BITS (7ULL << 2)	/* bits 4:2 */
+#define ENTRY_RD_BIT (1ULL << 6)
+#define ENTRY_WR_BIT (1ULL << 7)
+#define ENTRY_SHARE_BITS (3ULL << 8)	/* bits 9:8 */
+#define ENTRY_ACCESS_BIT (1ULL << 10)
+#define ENTRY_NX_BIT (1ULL << 54)
+
+#define ENTRY_FLAGS_MASK (ENTRY_ATTR_BITS | ENTRY_RD_BIT | ENTRY_WR_BIT | \
+		ENTRY_SHARE_BITS | ENTRY_ACCESS_BIT | ENTRY_NX_BIT)
+
+/*
+* peak_flops: 100/85
+* sobel: 100/50
+*/
+#define COMPUTE_JOB_WEIGHT (10000/50)
+
+#ifdef CONFIG_SENSORS_SEC_THERMISTOR
+extern int sec_therm_get_ap_temperature(void);
+#endif
+
+#ifdef CONFIG_SCHED_HMP
+extern int set_hmp_boost(int enable);
+#endif
+
+#ifdef CONFIG_USE_VSYNC_SKIP
+void decon_extra_vsync_wait_set(int);
+void decon_extra_vsync_wait_add(int);
+#endif
+
+#ifdef MALI_SEC_SEPERATED_UTILIZATION
+/** Notify the Power Management Metrics System that the GPU active state might
+ * have changed.
+ *
+ * If it has indeed changed since the last time the Metrics System was
+ * notified, then it calculates the active/idle time. Otherwise, it does
+ * nothing. For example, the caller can signal going idle when the last non-hw
+ * counter context deschedules, and then signals going idle again when the
+ * hwcounter context itself also deschedules.
+ *
+ * If there is only one context left running and that is HW counters
+ * collection, then the caller should set @p is_active to false. This has
+ * a side effect that counter collecting contexts that also run jobs will be
+ * invisible to utilization metrics. Note that gator cannot run jobs itself, so
+ * is unaffected by this.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid
+ *                  pointer)
+ * @param is_active Indicator that GPU must be recorded active (true), or
+ *                  idle (false)
+ */
+void gpu_pm_record_state(void *dev, bool is_active);
+#endif
+
+extern int gpu_register_dump(void);
+
+void gpu_create_context(void *ctx)
+{
+	struct kbase_context *kctx;
+	char current_name[sizeof(current->comm)];
+
+	kctx = (struct kbase_context *)ctx;
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	kctx->ctx_status = CTX_UNINITIALIZED;
+	kctx->ctx_need_qos = false;
+
+	get_task_comm(current_name, current);
+	strncpy((char *)(&kctx->name), current_name, CTX_NAME_SIZE);
+
+	kctx->ctx_status = CTX_INITIALIZED;
+
+	/* MALI_SEC_SECURE_RENDERING */
+#ifdef CONFIG_MALI_EXYNOS_SECURE_RENDERING
+	kctx->enabled_TZASC = false;
+#endif
+
+	kctx->destroying_context = false;
+}
+
+void gpu_destroy_context(void *ctx)
+{
+	struct kbase_context *kctx;
+	struct kbase_device *kbdev;
+
+	kctx = (struct kbase_context *)ctx;
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	kctx->destroying_context = true;
+
+	/* MALI_SEC_SECURE_RENDERING */
+#ifdef CONFIG_MALI_EXYNOS_SECURE_RENDERING
+	if (kbdev->protected_mode_support == true &&
+	    kctx->enabled_TZASC == true &&
+	    kbdev->protected_ops != NULL) {
+
+#ifdef CONFIG_MALI_SEC_ASP_SECURE_BUF_CTRL
+		kbdev->sec_sr_info.secure_flags_crc_asp = 0;
+#endif
+		kctx->enabled_TZASC = false;
+		GPU_LOG(DVFS_WARNING, LSI_GPU_SECURE, 0u, 0u, "%s: disable the protection mode, kctx : %p\n", __func__, kctx);
+
+#ifdef CONFIG_MALI_SEC_HWCNT
+		mutex_lock(&kbdev->hwcnt.mlock);
+		if(kbdev->vendor_callbacks->hwcnt_force_start)
+			kbdev->vendor_callbacks->hwcnt_force_start(kbdev);
+		mutex_unlock(&kbdev->hwcnt.mlock);
+#endif
+	}
+#endif
+
+	kctx->ctx_status = CTX_DESTROYED;
+
+	if (kctx->ctx_need_qos)
+	{
+		int i, policy_count;
+		const struct kbase_pm_policy *const *policy_list;
+		struct exynos_context *platform;
+		platform = (struct exynos_context *) kbdev->platform_context;
+#ifdef CONFIG_MALI_DVFS
+		gpu_dvfs_boost_lock(GPU_DVFS_BOOST_UNSET);
+#endif
+#ifdef CONFIG_SCHED_HMP
+		/* set policy back */
+		policy_count = kbase_pm_list_policies(&policy_list);
+		if (platform->cur_policy){
+			for (i = 0; i < policy_count; i++) {
+				if (sysfs_streq(policy_list[i]->name, platform->cur_policy->name)) {
+					kbase_pm_set_policy(kbdev, policy_list[i]);
+					break;
+				}
+			}
+		}
+		else{
+			for (i = 0; i < policy_count; i++) {
+				if (sysfs_streq(policy_list[i]->name, "coarse_demand")) {
+					kbase_pm_set_policy(kbdev, policy_list[i]);
+					break;
+				}
+			}
+		}
+		set_hmp_boost(0);
+		set_hmp_aggressive_up_migration(false);
+		set_hmp_aggressive_yield(false);
+#endif
+	}
+}
+
+/* MALI_SEC_INTEGRATION */
+/**
+ * enum mali_error - Mali error codes shared with userspace
+ *
+ * This is subset of those common Mali errors that can be returned to userspace.
+ * Values of matching user and kernel space enumerators MUST be the same.
+ * MALI_ERROR_NONE is guaranteed to be 0.
+ */
+enum mali_error {
+	MALI_ERROR_NONE = 0,
+	MALI_ERROR_OUT_OF_GPU_MEMORY,
+	MALI_ERROR_OUT_OF_MEMORY,
+	MALI_ERROR_FUNCTION_FAILED,
+};
+
+int gpu_vendor_dispatch(struct kbase_context *kctx, void * const args, u32 args_size)
+{
+	struct kbase_device *kbdev;
+	union uk_header *ukh = args;
+	u32 id;
+
+	KBASE_DEBUG_ASSERT(ukh != NULL);
+
+	kbdev = kctx->kbdev;
+	id = ukh->id;
+	ukh->ret = 0;	/* Be optimistic */
+
+	switch(id)
+	{
+#ifdef CONFIG_MALI_SEC_HWCNT
+	case KBASE_FUNC_TMU_SKIP:
+		{
+/* MALI_SEC_INTEGRATION */
+#ifdef CONFIG_SENSORS_SEC_THERMISTOR
+#ifdef CONFIG_USE_VSYNC_SKIP
+			struct kbase_uk_tmu_skip *tskip = args;
+			int thermistor = sec_therm_get_ap_temperature();
+			u32 i, t_index;
+			tskip->num_ratiometer = MIN(tskip->num_ratiometer, TMU_INDEX_MAX);
+			t_index = tskip->num_ratiometer;
+
+			for (i = 0; i < tskip->num_ratiometer; i++)
+				if (thermistor >= tskip->temperature[i])
+					t_index = i;
+
+			if (t_index < tskip->num_ratiometer) {
+				decon_extra_vsync_wait_add(tskip->skip_count[t_index]);
+				ukh->ret = MALI_ERROR_NONE;
+			} else {
+				decon_extra_vsync_wait_set(0);
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			}
+
+#endif /* CONFIG_USE_VSYNC_SKIP */
+#endif /* CONFIG_SENSORS_SEC_THERMISTOR */
+			break;
+		}
+#endif
+
+	case KBASE_FUNC_CREATE_SURFACE:
+		{
+			kbase_mem_set_max_size(kctx);
+			break;
+		}
+
+	case KBASE_FUNC_DESTROY_SURFACE:
+		{
+			kbase_mem_free_list_cleanup(kctx);
+			break;
+		}
+
+	case KBASE_FUNC_SET_MIN_LOCK :
+		{
+#ifdef CONFIG_MALI_DVFS
+			struct kbase_uk_custom_command *kgp = (struct kbase_uk_custom_command *)args;
+#endif /* CONFIG_MALI_DVFS */
+			int i, policy_count;
+			const struct kbase_pm_policy *const *policy_list;
+			struct exynos_context *platform;
+			platform = (struct exynos_context *) kbdev->platform_context;
+			if (!kctx->ctx_need_qos) {
+				kctx->ctx_need_qos = true;
+#ifdef CONFIG_SCHED_HMP
+				/* set policy to always_on */
+				policy_count = kbase_pm_list_policies(&policy_list);
+				platform->cur_policy = kbase_pm_get_policy(kbdev);
+				for (i = 0; i < policy_count; i++) {
+					if (sysfs_streq(policy_list[i]->name, "always_on")) {
+						kbase_pm_set_policy(kbdev, policy_list[i]);
+						break;
+					}
+				}
+				/* set hmp boost */
+				set_hmp_boost(1);
+				set_hmp_aggressive_up_migration(true);
+				set_hmp_aggressive_yield(true);
+#endif
+			}
+#ifdef CONFIG_MALI_DVFS
+			if (kgp->padding) {
+				platform->boost_egl_min_lock = kgp->padding;
+				gpu_pm_qos_command(platform, GPU_CONTROL_PM_QOS_EGL_SET);
+			} else {
+				gpu_dvfs_boost_lock(GPU_DVFS_BOOST_SET);
+			}
+#endif /* CONFIG_MALI_DVFS */
+			break;
+		}
+
+	case KBASE_FUNC_UNSET_MIN_LOCK :
+		{
+#ifdef CONFIG_MALI_DVFS
+			struct kbase_uk_custom_command *kgp = (struct kbase_uk_custom_command*)args;
+#endif /* CONFIG_MALI_DVFS */
+			int i, policy_count;
+			const struct kbase_pm_policy *const *policy_list;
+			struct exynos_context *platform;
+			platform = (struct exynos_context *) kbdev->platform_context;
+			if (kctx->ctx_need_qos) {
+				kctx->ctx_need_qos = false;
+#ifdef CONFIG_SCHED_HMP
+				/* set policy back */
+				policy_count = kbase_pm_list_policies(&policy_list);
+				for (i = 0; i < policy_count; i++) {
+					if (sysfs_streq(policy_list[i]->name, platform->cur_policy->name)) {
+						kbase_pm_set_policy(kbdev, policy_list[i]);
+						break;
+					}
+				}
+				platform->cur_policy = NULL;
+				/* unset hmp boost */
+				set_hmp_boost(0);
+				set_hmp_aggressive_up_migration(false);
+				set_hmp_aggressive_yield(false);
+#endif /* CONFIG_SCHED_HMP */
+#ifdef CONFIG_MALI_DVFS
+				if (kgp->padding) {
+					platform->boost_egl_min_lock = 0;
+					gpu_pm_qos_command(platform, GPU_CONTROL_PM_QOS_EGL_RESET);
+				} else {
+					gpu_dvfs_boost_lock(GPU_DVFS_BOOST_UNSET);
+				}
+#endif /* CONFIG_MALI_DVFS */
+			}
+			break;
+		}
+
+	/* MALI_SEC_SECURE_RENDERING */
+#ifdef CONFIG_MALI_EXYNOS_SECURE_RENDERING
+	case KBASE_FUNC_SECURE_WORLD_RENDERING :
+	{
+		if (kbdev->protected_mode_support == true &&
+		    kctx->enabled_TZASC == false &&
+		    kbdev->protected_ops != NULL) {
+
+#ifdef CONFIG_MALI_SEC_ASP_SECURE_BUF_CTRL
+			struct kbase_uk_custom_command *kgp = (struct kbase_uk_custom_command*)args;
+			kbdev->sec_sr_info.secure_flags_crc_asp = kgp->flags;
+
+			if (!kgp->flags) {
+				GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong operation! ASP enabled. But, flags is ZERO\n", __func__);
+				BUG();
+			}
+			GPU_LOG(DVFS_WARNING, LSI_GPU_SECURE, 0u, 0u, "%s: enable the protection mode, kctx : %p, flags : %llX\n", __func__, kctx, kgp->flags);
+#else
+			GPU_LOG(DVFS_WARNING, LSI_GPU_SECURE, 0u, 0u, "%s: enable the protection mode, kctx : %p, NO use ASP feature.\n", __func__, kctx);
+#endif
+			kctx->enabled_TZASC = true;
+
+#ifdef CONFIG_MALI_SEC_HWCNT
+			mutex_lock(&kbdev->hwcnt.mlock);
+			if(kbdev->vendor_callbacks->hwcnt_force_stop)
+				kbdev->vendor_callbacks->hwcnt_force_stop(kbdev);
+			mutex_unlock(&kbdev->hwcnt.mlock);
+#endif
+		} else {
+			GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong operation! DDK cannot support Secure Rendering\n", __func__);
+		}
+		break;
+	}
+
+	/* MALI_SEC_SECURE_RENDERING */
+	case KBASE_FUNC_NON_SECURE_WORLD_RENDERING :
+	{
+		if (kbdev->protected_mode_support == true &&
+		    kctx->enabled_TZASC == true &&
+		    kbdev->protected_ops != NULL) {
+
+#ifdef CONFIG_MALI_SEC_ASP_SECURE_BUF_CTRL
+			kbdev->sec_sr_info.secure_flags_crc_asp = 0;
+#endif
+			kctx->enabled_TZASC = false;
+			GPU_LOG(DVFS_WARNING, LSI_GPU_SECURE, 0u, 0u, "%s: disable the protection mode, kctx : %p\n", __func__, kctx);
+
+#ifdef CONFIG_MALI_SEC_HWCNT
+			mutex_lock(&kbdev->hwcnt.mlock);
+			if(kbdev->vendor_callbacks->hwcnt_force_start)
+				kbdev->vendor_callbacks->hwcnt_force_start(kbdev);
+			mutex_unlock(&kbdev->hwcnt.mlock);
+#endif
+		} else {
+			GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong operation! DDK cannot support Secure Rendering\n", __func__);
+		}
+		break;
+	}
+#endif
+
+	/* MALI_SEC_INTEGRATION */
+#ifdef CONFIG_MALI_SEC_HWCNT
+	case KBASE_FUNC_HWCNT_UTIL_SETUP:
+	{
+		struct kbase_uk_hwcnt_setup *setup = args;
+
+		if (setup->padding == HWC_MODE_GPR_EN)
+			dvfs_hwcnt_gpr_enable(kbdev, true);
+		else
+			dvfs_hwcnt_gpr_enable(kbdev, false);
+
+		break;
+	}
+	case KBASE_FUNC_HWCNT_GPR_DUMP:
+	{
+		struct kbase_uk_hwcnt_gpr_dump *dump = args;
+
+		mutex_lock(&kbdev->hwcnt.mlock);
+#ifdef CONFIG_MALI_EXYNOS_SECURE_RENDERING
+		if (kbdev->protected_mode == true) {
+			mutex_unlock(&kbdev->hwcnt.mlock);
+			dev_err(kbdev->dev, "cannot support ioctl %u in secure mode", id);
+			break;
+		}
+#endif
+		if (kbdev->hwcnt.is_hwcnt_attach == true && kbdev->hwcnt.is_hwcnt_gpr_enable == true) {
+			if (kbdev->vendor_callbacks->hwcnt_update) {
+				kbdev->vendor_callbacks->hwcnt_update(kbdev);
+				dvfs_hwcnt_get_gpr_resource(kbdev, dump);
+			}
+		}
+		else {
+			dump->shader_20 = 0xF;
+			dump->shader_21 = 0x1;
+		}
+
+		mutex_unlock(&kbdev->hwcnt.mlock);
+		break;
+	}
+	case KBASE_FUNC_VSYNC_SKIP:
+		{
+/* MALI_SEC_INTEGRATION */
+#ifdef CONFIG_USE_VSYNC_SKIP
+			struct kbase_uk_vsync_skip *vskip = args;
+
+			/* increment vsync skip variable that is used in fimd driver */
+			KBASE_TRACE_ADD_EXYNOS(kbdev, LSI_HWCNT_VSYNC_SKIP, NULL, NULL, 0u, vskip->skip_count);
+
+			if (vskip->skip_count == 0) {
+				decon_extra_vsync_wait_set(0);
+			} else {
+				decon_extra_vsync_wait_add(vskip->skip_count);
+			}
+#endif /* CONFIG_USE_VSYNC_SKIP */
+			break;
+		}
+#endif
+	default:
+		break;
+	}
+
+	return 0;
+
+}
+
+#include <mali_kbase_gpu_memory_debugfs.h>
+int gpu_memory_seq_show(struct seq_file *sfile, void *data)
+{
+	ssize_t ret = 0;
+	struct list_head *entry;
+	const struct list_head *kbdev_list;
+	size_t free_size = 0;
+	size_t each_free_size = 0;
+
+	kbdev_list = kbase_dev_list_get();
+	list_for_each(entry, kbdev_list) {
+		struct kbase_device *kbdev = NULL;
+		struct kbasep_kctx_list_element *element;
+
+		kbdev = list_entry(entry, struct kbase_device, entry);
+		/* output the total memory usage and cap for this device */
+		mutex_lock(&kbdev->kctx_list_lock);
+		list_for_each_entry(element, &kbdev->kctx_list, link) {
+			spin_lock(&(element->kctx->mem_pool.pool_lock));
+			free_size += element->kctx->mem_pool.cur_size;
+			spin_unlock(&(element->kctx->mem_pool.pool_lock));
+		}
+		mutex_unlock(&kbdev->kctx_list_lock);
+		ret = seq_printf(sfile, "===========================================================\n");
+		ret = seq_printf(sfile, " %16s  %18s  %20s\n", \
+				"dev name", \
+				"total used pages", \
+				"total shrink pages");
+		ret = seq_printf(sfile, "-----------------------------------------------------------\n");
+		ret = seq_printf(sfile, " %16s  %18u  %20zu\n", \
+				kbdev->devname, \
+				atomic_read(&(kbdev->memdev.used_pages)), \
+				free_size);
+		ret = seq_printf(sfile, "===========================================================\n\n");
+		ret = seq_printf(sfile, "%28s     %20s  %16s  %12s\n", \
+				"context name", \
+				"context addr", \
+				"used pages", \
+				"shrink pages");
+		ret = seq_printf(sfile, "====================================================");
+		ret = seq_printf(sfile, "========================================\n");
+		mutex_lock(&kbdev->kctx_list_lock);
+		list_for_each_entry(element, &kbdev->kctx_list, link) {
+			/* output the memory usage and cap for each kctx
+			* opened on this device */
+
+			spin_lock(&(element->kctx->mem_pool.pool_lock));
+			each_free_size = element->kctx->mem_pool.cur_size;
+			spin_unlock(&(element->kctx->mem_pool.pool_lock));
+			ret = seq_printf(sfile, "  (%24s), %s-0x%p    %12u  %10zu\n", \
+					element->kctx->name, \
+					"kctx", \
+					element->kctx, \
+					atomic_read(&(element->kctx->used_pages)),
+					each_free_size );
+		}
+		mutex_unlock(&kbdev->kctx_list_lock);
+	}
+	kbase_dev_list_put(kbdev_list);
+	return ret;
+}
+
+void gpu_update_status(void *dev, char *str, u32 val)
+{
+	struct kbase_device *kbdev;
+
+	kbdev = (struct kbase_device *)dev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	if(strcmp(str, "completion_code") == 0)
+	{
+		if(val == 0x58) // DATA_INVALID_FAULT
+			((struct exynos_context *)kbdev->platform_context)->data_invalid_fault_count ++;
+		else if((val & 0xf0) == 0xc0) // MMU_FAULT
+			((struct exynos_context *)kbdev->platform_context)->mmu_fault_count ++;
+
+	}
+	else if(strcmp(str, "reset_count") == 0)
+		((struct exynos_context *)kbdev->platform_context)->reset_count++;
+}
+
+/* MALI_SEC_SECURE_RENDERING */
+#ifdef CONFIG_MALI_EXYNOS_SECURE_RENDERING
+void gpu_cacheclean(struct kbase_device *kbdev)
+{
+    /* Limit the number of loops to avoid a hang if the interrupt is missed */
+    u32 max_loops = KBASE_CLEAN_CACHE_MAX_LOOPS;
+
+    /* use GPU_COMMAND completion solution */
+    /* clean the caches */
+    kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_CLEAN_CACHES, NULL);
+
+    /* wait for cache flush to complete before continuing */
+    while (--max_loops && (kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_RAWSTAT), NULL) & CLEAN_CACHES_COMPLETED) == 0)
+        ;
+
+    /* clear the CLEAN_CACHES_COMPLETED irq */
+    kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_CLEAR), CLEAN_CACHES_COMPLETED, NULL);
+    KBASE_DEBUG_ASSERT_MSG(kbdev->hwcnt.state != KBASE_INSTR_STATE_CLEANING,
+        "Instrumentation code was cleaning caches, but Job Management code cleared their IRQ - Instrumentation code will now hang.");
+}
+#endif
+
+void kbase_mem_set_max_size(struct kbase_context *kctx)
+{
+#ifdef R7P0_EAC_BLOCK
+	struct kbase_mem_allocator *allocator = &kctx->osalloc;
+	mutex_lock(&allocator->free_list_lock);
+	allocator->free_list_max_size = MEM_FREE_DEFAULT;
+	mutex_unlock(&allocator->free_list_lock);
+#endif
+}
+
+void kbase_mem_free_list_cleanup(struct kbase_context *kctx)
+{
+#ifdef R7P0_EAC_BLOCK
+	int tofree,i=0;
+	struct kbase_mem_allocator *allocator = &kctx->osalloc;
+	tofree = MAX(MEM_FREE_LIMITS, atomic_read(&allocator->free_list_size)) - MEM_FREE_LIMITS;
+	if (tofree > 0)
+	{
+		struct page *p;
+		mutex_lock(&allocator->free_list_lock);
+	        allocator->free_list_max_size = MEM_FREE_LIMITS;
+		for(i=0; i < tofree; i++)
+		{
+			p = list_first_entry(&allocator->free_list_head, struct page, lru);
+			list_del(&p->lru);
+			if (likely(0 != p))
+			{
+			    dma_unmap_page(allocator->kbdev->dev, page_private(p),
+				    PAGE_SIZE,
+				    DMA_BIDIRECTIONAL);
+			    ClearPagePrivate(p);
+			    __free_page(p);
+			}
+		}
+		atomic_set(&allocator->free_list_size, MEM_FREE_LIMITS);
+		mutex_unlock(&allocator->free_list_lock);
+	}
+#endif
+}
+
+#define KBASE_MMU_PAGE_ENTRIES	512
+
+static phys_addr_t mmu_pte_to_phy_addr(u64 entry)
+{
+	if (!(entry & 1))
+		return 0;
+
+	return entry & ~0xFFF;
+}
+
+/* MALI_SEC_INTEGRATION */
+static void gpu_page_table_info_dp_level(struct kbase_context *kctx, u64 vaddr, phys_addr_t pgd, int level)
+{
+	u64 *pgd_page;
+	int i;
+	int index = (vaddr >> (12 + ((3 - level) * 9))) & 0x1FF;
+	int min_index = index - 3;
+	int max_index = index + 3;
+
+	if (min_index < 0)
+		min_index = 0;
+	if (max_index >= KBASE_MMU_PAGE_ENTRIES)
+		max_index = KBASE_MMU_PAGE_ENTRIES - 1;
+
+	/* Map and dump entire page */
+
+	pgd_page = kmap(pfn_to_page(PFN_DOWN(pgd)));
+
+	dev_err(kctx->kbdev->dev, "Dumping level %d @ physical address 0x%016llX (matching index %d):\n", level, pgd, index);
+
+	if (!pgd_page) {
+		dev_err(kctx->kbdev->dev, "kmap failure\n");
+		return;
+	}
+
+	for (i = min_index; i <= max_index; i++) {
+		if (i == index) {
+			dev_err(kctx->kbdev->dev, "[%03d]: 0x%016llX *\n", i, pgd_page[i]);
+		} else {
+			dev_err(kctx->kbdev->dev, "[%03d]: 0x%016llX\n", i, pgd_page[i]);
+		}
+	}
+
+	/* parse next level (if any) */
+
+	if ((pgd_page[index] & 3) == ENTRY_IS_PTE) {
+		phys_addr_t target_pgd = mmu_pte_to_phy_addr(pgd_page[index]);
+		gpu_page_table_info_dp_level(kctx, vaddr, target_pgd, level + 1);
+	} else if ((pgd_page[index] & 3) == ENTRY_IS_ATE) {
+		dev_err(kctx->kbdev->dev, "Final physical address: 0x%016llX\n", pgd_page[index] & ~(0xFFF | ENTRY_FLAGS_MASK));
+	} else {
+		dev_err(kctx->kbdev->dev, "Final physical address: INVALID!\n");
+	}
+
+	kunmap(pfn_to_page(PFN_DOWN(pgd)));
+}
+
+void gpu_debug_pagetable_info(void *ctx, u64 vaddr)
+{
+	struct kbase_context *kctx;
+
+	kctx = (struct kbase_context *)ctx;
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	dev_err(kctx->kbdev->dev, "Looking up virtual GPU address: 0x%016llX\n", vaddr);
+	gpu_page_table_info_dp_level(kctx, vaddr, kctx->pgd, 0);
+}
+
+#ifdef CONFIG_MALI_SEC_CL_BOOST
+void gpu_cl_boost_init(void *dev)
+{
+	struct kbase_device *kbdev;
+
+	kbdev = (struct kbase_device *)dev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	atomic_set(&kbdev->pm.backend.metrics.time_compute_jobs, 0);
+	atomic_set(&kbdev->pm.backend.metrics.time_vertex_jobs, 0);
+	atomic_set(&kbdev->pm.backend.metrics.time_fragment_jobs, 0);
+}
+
+void gpu_cl_boost_update_utilization(void *dev, void *atom, u64 microseconds_spent)
+{
+	struct kbase_jd_atom *katom;
+	struct kbase_device *kbdev;
+
+	kbdev = (struct kbase_device *)dev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	katom = (struct kbase_jd_atom *)atom;
+	KBASE_DEBUG_ASSERT(katom != NULL);
+
+	if (katom->core_req & BASE_JD_REQ_ONLY_COMPUTE)
+		atomic_add((microseconds_spent >> KBASE_PM_TIME_SHIFT), &kbdev->pm.backend.metrics.time_compute_jobs);
+	else if (katom->core_req & BASE_JD_REQ_FS)
+		atomic_add((microseconds_spent >> KBASE_PM_TIME_SHIFT), &kbdev->pm.backend.metrics.time_fragment_jobs);
+	else if (katom->core_req & BASE_JD_REQ_CS)
+		atomic_add((microseconds_spent >> KBASE_PM_TIME_SHIFT), &kbdev->pm.backend.metrics.time_vertex_jobs);
+}
+#endif
+
+#ifdef MALI_SEC_FENCE_INTEGRATION
+#define KBASE_FENCE_TIMEOUT 1000
+#define DUMP_CHUNK 256
+
+#ifdef KBASE_FENCE_DUMP
+static const char *kbase_sync_status_str(int status)
+{
+	if (status > 0)
+		return "signaled";
+	else if (status == 0)
+		return "active";
+	else
+		return "error";
+}
+
+static void kbase_sync_print_pt(struct seq_file *s, struct sync_pt *pt, bool fence)
+{
+	int status;
+
+	if (pt == NULL)
+		return;
+	status = pt->status;
+
+	seq_printf(s, "  %s%spt %s",
+		   fence ? pt->parent->name : "",
+		   fence ? "_" : "",
+		   kbase_sync_status_str(status));
+	if (pt->status) {
+		struct timeval tv = ktime_to_timeval(pt->timestamp);
+		seq_printf(s, "@%ld.%06ld", tv.tv_sec, tv.tv_usec);
+	}
+
+	if (pt->parent->ops->timeline_value_str &&
+	    pt->parent->ops->pt_value_str) {
+		char value[64];
+		pt->parent->ops->pt_value_str(pt, value, sizeof(value));
+		seq_printf(s, ": %s", value);
+		if (fence) {
+			pt->parent->ops->timeline_value_str(pt->parent, value,
+						    sizeof(value));
+			seq_printf(s, " / %s", value);
+		}
+	} else if (pt->parent->ops->print_pt) {
+		seq_printf(s, ": ");
+		pt->parent->ops->print_pt(s, pt);
+	}
+
+	seq_printf(s, "\n");
+}
+
+static void kbase_fence_print(struct seq_file *s, struct sync_fence *fence)
+{
+	struct list_head *pos;
+	unsigned long flags;
+
+	seq_printf(s, "[%p] %s: %s\n", fence, fence->name,
+		   kbase_sync_status_str(fence->status));
+
+	list_for_each(pos, &fence->pt_list_head) {
+		struct sync_pt *pt =
+			container_of(pos, struct sync_pt, pt_list);
+		kbase_sync_print_pt(s, pt, true);
+	}
+
+	spin_lock_irqsave(&fence->waiter_list_lock, flags);
+	list_for_each(pos, &fence->waiter_list_head) {
+		struct sync_fence_waiter *waiter =
+			container_of(pos, struct sync_fence_waiter,
+				     waiter_list);
+
+		if (waiter)
+			seq_printf(s, "waiter %pF\n", waiter->callback);
+	}
+	spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
+}
+
+static char kbase_sync_dump_buf[64 * 1024];
+static void kbase_fence_dump(struct sync_fence *fence)
+{
+	int i;
+	struct seq_file s = {
+		.buf = kbase_sync_dump_buf,
+		.size = sizeof(kbase_sync_dump_buf) - 1,
+	};
+
+	kbase_fence_print(&s, fence);
+	for (i = 0; i < s.count; i += DUMP_CHUNK) {
+		if ((s.count - i) > DUMP_CHUNK) {
+			char c = s.buf[i + DUMP_CHUNK];
+			s.buf[i + DUMP_CHUNK] = 0;
+			pr_cont("%s", s.buf + i);
+			s.buf[i + DUMP_CHUNK] = c;
+		} else {
+			s.buf[s.count] = 0;
+			pr_cont("%s", s.buf + i);
+		}
+	}
+}
+#endif
+
+/* MALI_SEC_INTEGRATION */
+static void kbase_fence_timeout(unsigned long data)
+{
+	struct kbase_jd_atom *katom;
+	unsigned long flags;
+
+	katom = (struct kbase_jd_atom *)data;
+	KBASE_DEBUG_ASSERT(NULL != katom);
+
+	if (katom == NULL || katom->fence == NULL)
+		return;
+
+	spin_lock_irqsave(&katom->fence_lock, flags);
+	if (katom == NULL || katom->fence == NULL) {
+		spin_unlock_irqrestore(&katom->fence_lock, flags);
+		return;
+	}
+
+	if (katom->fence->status != 0) {
+		spin_unlock_irqrestore(&katom->fence_lock, flags);
+		kbase_fence_del_timer(katom);
+		return;
+	}
+	pr_info("Release fence is not signaled on [%p] for %d ms\n", katom->fence, KBASE_FENCE_TIMEOUT);
+
+#ifdef KBASE_FENCE_DUMP
+	kbase_fence_dump(katom->fence);
+#endif
+#ifdef KBASE_FENCE_TIMEOUT_FAKE_SIGNAL
+	{
+		struct sync_pt *pt;
+		struct sync_timeline *timeline;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
+		pt = list_first_entry(&katom->fence->pt_list_head, struct sync_pt, pt_list);
+#else
+		pt = container_of(katom->fence->cbs[0].sync_pt, struct sync_pt, base);
+#endif
+		if (pt == NULL) {
+			spin_unlock_irqrestore(&katom->fence_lock, flags);
+			return;
+		}
+
+		timeline = sync_pt_parent(pt);
+
+		sync_timeline_signal(timeline);
+	}
+	spin_unlock_irqrestore(&katom->fence_lock, flags);
+
+	pr_info("Release fence is not signaled on [%p] for %d ms\n", katom->fence, KBASE_FENCE_TIMEOUT);
+#endif
+	return;
+}
+
+void kbase_fence_timer_init(void *atom)
+{
+	const u32 timeout = msecs_to_jiffies(KBASE_FENCE_TIMEOUT);
+	struct kbase_jd_atom *katom;
+
+	katom = (struct kbase_jd_atom *)atom;
+	KBASE_DEBUG_ASSERT(NULL != katom);
+
+	if (katom == NULL)
+		return;
+
+	init_timer(&katom->fence_timer);
+	katom->fence_timer.function = kbase_fence_timeout;
+	katom->fence_timer.data = (unsigned long)katom;
+	katom->fence_timer.expires = jiffies + timeout;
+
+	add_timer(&katom->fence_timer);
+	return;
+}
+
+void kbase_fence_del_timer(void *atom)
+{
+	struct kbase_jd_atom *katom;
+
+	katom = (struct kbase_jd_atom *)atom;
+	KBASE_DEBUG_ASSERT(NULL != katom);
+
+	if (katom == NULL)
+		return;
+
+	if (katom->fence_timer.function == kbase_fence_timeout)
+		del_timer(&katom->fence_timer);
+	katom->fence_timer.function = NULL;
+	return;
+}
+#endif
+
+#ifdef CONFIG_MALI_DVFS
+static void dvfs_callback(struct work_struct *data)
+{
+	unsigned long flags;
+	struct kbasep_pm_metrics_data *metrics;
+	struct kbase_device *kbdev;
+	struct exynos_context *platform;
+
+	KBASE_DEBUG_ASSERT(data != NULL);
+
+	metrics = container_of(data, struct kbasep_pm_metrics_data, work.work);
+
+	kbdev = metrics->kbdev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	platform = (struct exynos_context *)kbdev->platform_context;
+	KBASE_DEBUG_ASSERT(platform != NULL);
+
+	kbase_platform_dvfs_event(metrics->kbdev, 0);
+
+	spin_lock_irqsave(&metrics->lock, flags);
+
+	if (metrics->timer_active)
+		queue_delayed_work_on(0, platform->dvfs_wq,
+				platform->delayed_work, msecs_to_jiffies(platform->polling_speed));
+
+	spin_unlock_irqrestore(&metrics->lock, flags);
+}
+
+void gpu_pm_metrics_init(void *dev)
+{
+	struct kbase_device *kbdev;
+	struct exynos_context *platform;
+
+	kbdev = (struct kbase_device *)dev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	platform = (struct exynos_context *)kbdev->platform_context;
+	KBASE_DEBUG_ASSERT(platform != NULL);
+
+	INIT_DELAYED_WORK(&kbdev->pm.backend.metrics.work, dvfs_callback);
+	platform->dvfs_wq = create_workqueue("g3d_dvfs");
+	platform->delayed_work = &kbdev->pm.backend.metrics.work;
+
+	queue_delayed_work_on(0, platform->dvfs_wq,
+		platform->delayed_work, msecs_to_jiffies(platform->polling_speed));
+}
+
+void gpu_pm_metrics_term(void *dev)
+{
+	struct kbase_device *kbdev;
+	struct exynos_context *platform;
+
+	kbdev = (struct kbase_device *)dev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	platform = (struct exynos_context *)kbdev->platform_context;
+	KBASE_DEBUG_ASSERT(platform != NULL);
+
+	cancel_delayed_work(platform->delayed_work);
+	flush_workqueue(platform->dvfs_wq);
+	destroy_workqueue(platform->dvfs_wq);
+}
+#endif
+
+/* caller needs to hold kbdev->pm.backend.metrics.lock before calling this function */
+#ifdef CONFIG_MALI_DVFS
+int gpu_pm_get_dvfs_utilisation(struct kbase_device *kbdev, int *util_gl_share, int util_cl_share[2])
+{
+	unsigned long flags;
+	int utilisation = 0;
+#if !defined(CONFIG_MALI_SEC_CL_BOOST)
+	int busy;
+#else
+	int compute_time = 0, vertex_time = 0, fragment_time = 0, total_time = 0, compute_time_rate = 0;
+#endif
+
+	ktime_t now = ktime_get();
+	ktime_t diff;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	spin_lock_irqsave(&kbdev->pm.backend.metrics.lock, flags);
+	diff = ktime_sub(now, kbdev->pm.backend.metrics.time_period_start);
+
+	if (kbdev->pm.backend.metrics.gpu_active) {
+		u32 ns_time = (u32) (ktime_to_ns(diff) >> KBASE_PM_TIME_SHIFT);
+		kbdev->pm.backend.metrics.time_busy += ns_time;
+		kbdev->pm.backend.metrics.busy_cl[0] += ns_time * kbdev->pm.backend.metrics.active_cl_ctx[0];
+		kbdev->pm.backend.metrics.busy_cl[1] += ns_time * kbdev->pm.backend.metrics.active_cl_ctx[1];
+#ifdef R7P0_EAC_BLOCK
+		kbdev->pm.backend.metrics.busy_gl += ns_time * kbdev->pm.backend.metrics.active_gl_ctx;
+#endif
+		kbdev->pm.backend.metrics.time_period_start = now;
+	} else {
+		kbdev->pm.backend.metrics.time_idle += (u32) (ktime_to_ns(diff) >> KBASE_PM_TIME_SHIFT);
+		kbdev->pm.backend.metrics.time_period_start = now;
+	}
+	spin_unlock_irqrestore(&kbdev->pm.backend.metrics.lock, flags);
+	if (kbdev->pm.backend.metrics.time_idle + kbdev->pm.backend.metrics.time_busy == 0) {
+		/* No data - so we return NOP */
+		utilisation = -1;
+#if !defined(CONFIG_MALI_SEC_CL_BOOST)
+		if (util_gl_share)
+			*util_gl_share = -1;
+		if (util_cl_share) {
+			util_cl_share[0] = -1;
+			util_cl_share[1] = -1;
+		}
+#endif
+		goto out;
+	}
+
+	utilisation = (100 * kbdev->pm.backend.metrics.time_busy) /
+			(kbdev->pm.backend.metrics.time_idle +
+			 kbdev->pm.backend.metrics.time_busy);
+
+#if !defined(CONFIG_MALI_SEC_CL_BOOST)
+	busy = kbdev->pm.backend.metrics.busy_gl +
+		kbdev->pm.backend.metrics.busy_cl[0] +
+		kbdev->pm.backend.metrics.busy_cl[1];
+
+	if (busy != 0) {
+		if (util_gl_share)
+			*util_gl_share =
+				(100 * kbdev->pm.backend.metrics.busy_gl) / busy;
+		if (util_cl_share) {
+			util_cl_share[0] =
+				(100 * kbdev->pm.backend.metrics.busy_cl[0]) / busy;
+			util_cl_share[1] =
+				(100 * kbdev->pm.backend.metrics.busy_cl[1]) / busy;
+		}
+	} else {
+		if (util_gl_share)
+			*util_gl_share = -1;
+		if (util_cl_share) {
+			util_cl_share[0] = -1;
+			util_cl_share[1] = -1;
+		}
+	}
+#endif
+
+#ifdef CONFIG_MALI_SEC_CL_BOOST
+	compute_time = atomic_read(&kbdev->pm.backend.metrics.time_compute_jobs);
+	vertex_time = atomic_read(&kbdev->pm.backend.metrics.time_vertex_jobs);
+	fragment_time = atomic_read(&kbdev->pm.backend.metrics.time_fragment_jobs);
+	total_time = compute_time + vertex_time + fragment_time;
+
+#if 0
+	if (compute_time > 0 && total_time > 0)
+	{
+		compute_time_rate = (100 * compute_time) / total_time;
+		utilisation = utilisation * (COMPUTE_JOB_WEIGHT * compute_time_rate + 100 * (100 - compute_time_rate));
+		utilisation /= 10000;
+
+		if (utilisation >= 100) utilisation = 100;
+	}
+#endif
+	if (compute_time > 0) {
+		compute_time_rate = (100 * compute_time) / total_time;
+		if (compute_time_rate == 100)
+			kbdev->pm.backend.metrics.is_full_compute_util = true;
+		else
+			kbdev->pm.backend.metrics.is_full_compute_util = false;
+	} else
+		kbdev->pm.backend.metrics.is_full_compute_util = false;
+#endif
+ out:
+
+	spin_lock_irqsave(&kbdev->pm.backend.metrics.lock, flags);
+	kbdev->pm.backend.metrics.time_idle = 0;
+	kbdev->pm.backend.metrics.time_busy = 0;
+#if !defined(CONFIG_MALI_SEC_CL_BOOST)
+	kbdev->pm.backend.metrics.busy_cl[0] = 0;
+	kbdev->pm.backend.metrics.busy_cl[1] = 0;
+	kbdev->pm.backend.metrics.busy_gl = 0;
+#else
+	atomic_set(&kbdev->pm.backend.metrics.time_compute_jobs, 0);
+	atomic_set(&kbdev->pm.backend.metrics.time_vertex_jobs, 0);
+	atomic_set(&kbdev->pm.backend.metrics.time_fragment_jobs, 0);
+#endif
+	spin_unlock_irqrestore(&kbdev->pm.backend.metrics.lock, flags);
+	return utilisation;
+}
+#endif /* CONFIG_MALI_DVFS */
+
+static bool dbg_enable = false;
+static void gpu_set_poweron_dbg(bool enable_dbg)
+{
+	dbg_enable = enable_dbg;
+}
+
+static bool gpu_get_poweron_dbg(void)
+{
+	return dbg_enable;
+}
+
+/* S.LSI INTERGRATION */
+static bool gpu_mem_profile_check_kctx(void *ctx)
+{
+	struct kbase_context *kctx;
+	struct kbase_device *kbdev;
+	struct kbasep_kctx_list_element *element, *tmp;
+	bool found_element = false;
+
+	kctx = (struct kbase_context *)ctx;
+	kbdev = gpu_get_device_structure();
+
+	list_for_each_entry_safe(element, tmp, &kbdev->kctx_list, link) {
+		if (element->kctx == kctx) {
+			if (kctx->destroying_context == false) {
+				found_element = true;
+				break;
+			}
+		}
+	}
+
+	return found_element;
+}
+
+struct kbase_vendor_callbacks exynos_callbacks = {
+	.create_context = gpu_create_context,
+	.destroy_context = gpu_destroy_context,
+#ifdef CONFIG_MALI_SEC_CL_BOOST
+	.cl_boost_init = gpu_cl_boost_init,
+	.cl_boost_update_utilization = gpu_cl_boost_update_utilization,
+#else
+	.cl_boost_init = NULL,
+	.cl_boost_update_utilization = NULL,
+#endif
+#ifdef MALI_SEC_FENCE_INTEGRATION
+	.fence_timer_init = kbase_fence_timer_init,
+	.fence_del_timer = kbase_fence_del_timer,
+#else
+	.fence_timer_init = NULL,
+	.fence_del_timer = NULL,
+#endif
+#if defined(CONFIG_SOC_EXYNOS7420) || defined(CONFIG_SOC_EXYNOS7890)
+	.init_hw = exynos_gpu_init_hw,
+#else
+	.init_hw = NULL,
+#endif
+#ifdef CONFIG_MALI_SEC_HWCNT
+	.hwcnt_attach = dvfs_hwcnt_attach,
+	.hwcnt_update = dvfs_hwcnt_update,
+	.hwcnt_detach = dvfs_hwcnt_detach,
+	.hwcnt_enable = dvfs_hwcnt_enable,
+	.hwcnt_disable = dvfs_hwcnt_disable,
+	.hwcnt_force_start = dvfs_hwcnt_force_start,
+	.hwcnt_force_stop = dvfs_hwcnt_force_stop,
+#else
+	.hwcnt_attach = NULL,
+	.hwcnt_update = NULL,
+	.hwcnt_detach = NULL,
+	.hwcnt_enable = NULL,
+	.hwcnt_disable = NULL,
+	.hwcnt_force_start = NULL,
+	.hwcnt_force_stop = NULL,
+#endif
+#ifdef CONFIG_MALI_DVFS
+	.pm_metrics_init = gpu_pm_metrics_init,
+	.pm_metrics_term = gpu_pm_metrics_term,
+#else
+	.pm_metrics_init = NULL,
+	.pm_metrics_term = NULL,
+#endif
+	.set_poweron_dbg = gpu_set_poweron_dbg,
+	.get_poweron_dbg = gpu_get_poweron_dbg,
+	.debug_pagetable_info = gpu_debug_pagetable_info,
+	.mem_profile_check_kctx = gpu_mem_profile_check_kctx,
+#ifdef MALI_SEC_SEPERATED_UTILIZATION
+	.pm_record_state = gpu_pm_record_state,
+#else
+	.pm_record_state = NULL,
+#endif
+	.register_dump = gpu_register_dump,
+};
+
+uintptr_t gpu_get_callbacks(void)
+{
+	return ((uintptr_t)&exynos_callbacks);
+}
+
diff -Nur r5p0/platform/gpu_integration_defs.h r15p0/platform/exynos/gpu_integration_defs.h
--- r5p0/platform/gpu_integration_defs.h	1970-01-01 01:00:00.000000000 +0100
+++ r15p0/platform/exynos/gpu_integration_defs.h	2017-07-20 16:14:04.620559419 +0200
@@ -0,0 +1,88 @@
+/* drivers/gpu/arm/.../platform/gpu_integration_defs.h
+ *
+ * Copyright 2011 by S.LSI. Samsung Electronics Inc.
+ * San#24, Nongseo-Dong, Giheung-Gu, Yongin, Korea
+ *
+ * Samsung SoC Mali-T Series DDK porting layer
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software FoundatIon.
+ */
+
+/**
+ * @file gpu_integration_defs.h
+ * DDK porting layer.
+ */
+
+#ifndef _SEC_INTEGRATION_H_
+#define _SEC_INTEGRATION_H_
+
+#include <mali_kbase.h>
+#include <mali_kbase_mem_linux.h>
+#include "mali_kbase_platform.h"
+#include "gpu_dvfs_handler.h"
+
+#ifdef CONFIG_MALI_SEC_HWCNT
+#include "gpu_hwcnt_sec.h"
+#endif
+
+/* kctx initialized with zero from vzalloc, so initialized value required only */
+#define CTX_UNINITIALIZED 0x0
+#define CTX_INITIALIZED 0x1
+#define CTX_DESTROYED 0x2
+#define CTX_NAME_SIZE 32
+
+/* MALI_SEC_SECURE_RENDERING */
+#ifdef CONFIG_MALI_SEC_ASP_SECURE_BUF_CTRL
+#define SMC_GPU_CRC_REGION_NUM		8
+#endif
+
+/* MALI_SEC_INTEGRATION */
+#define KBASE_PM_TIME_SHIFT			8
+
+/* MALI_SEC_INTEGRATION */
+#define MEM_FREE_LIMITS 16384
+#define MEM_FREE_DEFAULT 16384
+
+uintptr_t gpu_get_callbacks(void);
+int gpu_vendor_dispatch(struct kbase_context *kctx, void * const args, u32 args_size);
+void gpu_cacheclean(struct kbase_device *kbdev);
+void kbase_mem_free_list_cleanup(struct kbase_context *kctx);
+void kbase_mem_set_max_size(struct kbase_context *kctx);
+
+#ifdef MALI_SEC_FENCE_INTEGRATION
+void kbase_fence_del_timer(void *atom);
+#endif
+
+struct kbase_vendor_callbacks {
+	void (* create_context)(void *ctx);
+	void (* destroy_context)(void *ctx);
+	void (* pm_metrics_init)(void *dev);
+	void (* pm_metrics_term)(void *dev);
+	void (* cl_boost_init)(void *dev);
+	void (* cl_boost_update_utilization)(void *dev, void *atom, u64 microseconds_spent);
+	void (* fence_timer_init)(void *atom);
+	void (* fence_del_timer)(void *atom);
+	int (* get_core_mask)(void *dev);
+	int (* init_hw)(void *dev);
+	void (* hwcnt_attach)(void *dev);
+	void (* hwcnt_update)(void *dev);
+	void (* hwcnt_detach)(void *dev);
+	void (* hwcnt_enable)(void *dev);
+	void (* hwcnt_disable)(void *dev);
+	void (* hwcnt_force_start)(void *dev);
+	void (* hwcnt_force_stop)(void *dev);
+	void (* set_poweron_dbg)(bool enable_dbg);
+	bool (* get_poweron_dbg)(void);
+	void (* debug_pagetable_info)(void *ctx, u64 vaddr);
+	void (* update_status)(void *dev, char *str, u32 val);
+	bool (* mem_profile_check_kctx)(void *ctx);
+	void (* pm_record_state)(void *kbdev, bool is_active);
+	int (* register_dump)(void);
+#ifdef CONFIG_MALI_DVFS_USER
+	bool (* dvfs_process_job)(void *atom);
+#endif
+};
+
+#endif /* _SEC_INTEGRATION_H_ */
diff -Nur r5p0/platform/gpu_notifier.c r15p0/platform/exynos/gpu_notifier.c
--- r5p0/platform/gpu_notifier.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_notifier.c	2017-07-20 16:14:04.620559419 +0200
@@ -27,7 +27,11 @@
 #include "gpu_control.h"
 
 #ifdef CONFIG_EXYNOS_THERMAL
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
 #include <mach/tmu.h>
+#else
+#include <soc/samsung/tmu.h>
+#endif
 #endif /* CONFIG_EXYNOS_THERMAL */
 
 #ifdef CONFIG_EXYNOS_NOC_DEBUGGING
@@ -41,7 +45,9 @@
 #ifdef CONFIG_MALI_DVFS
 	struct exynos_context *platform;
 	int lock_clock;
-
+#ifdef CONFIG_EXYNOS_SNAPSHOT_THERMAL
+	char *cooling_device_name = "GPU";
+#endif
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 
 	platform = (struct exynos_context *)kbdev->platform_context;
@@ -49,29 +55,29 @@
 		return -ENODEV;
 
 	switch (event) {
-	case GPU_THROTTLING1:
-		lock_clock = platform->tmu_lock_clk[THROTTLING1];
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "THROTTLING1\n");
-		break;
-	case GPU_THROTTLING2:
-		lock_clock = platform->tmu_lock_clk[THROTTLING2];
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "THROTTLING2\n");
-		break;
-	case GPU_THROTTLING3:
-		lock_clock = platform->tmu_lock_clk[THROTTLING3];
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "THROTTLING3\n");
-		break;
-	case GPU_THROTTLING4:
-		lock_clock = platform->tmu_lock_clk[THROTTLING4];
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "THROTTLING4\n");
-		break;
-	case GPU_TRIPPING:
-		lock_clock = platform->tmu_lock_clk[TRIPPING];
-		GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "TRIPPING\n");
-		break;
-	default:
-		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: wrong event, %lu\n", __func__, event);
-		return 0;
+		case GPU_THROTTLING1:
+			lock_clock = platform->tmu_lock_clk[THROTTLING1];
+			GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "THROTTLING1\n");
+			break;
+		case GPU_THROTTLING2:
+			lock_clock = platform->tmu_lock_clk[THROTTLING2];
+			GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "THROTTLING2\n");
+			break;
+		case GPU_THROTTLING3:
+			lock_clock = platform->tmu_lock_clk[THROTTLING3];
+			GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "THROTTLING3\n");
+			break;
+		case GPU_THROTTLING4:
+			lock_clock = platform->tmu_lock_clk[THROTTLING4];
+			GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "THROTTLING4\n");
+			break;
+		case GPU_TRIPPING:
+			lock_clock = platform->tmu_lock_clk[TRIPPING];
+			GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "TRIPPING\n");
+			break;
+		default:
+			GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "%s: wrong event, %lu\n", __func__, event);
+			return 0;
 	}
 
 	gpu_dvfs_clock_lock(GPU_DVFS_MAX_LOCK, TMU_LOCK, lock_clock);
@@ -133,18 +139,18 @@
 	struct kbase_device *kbdev = pkbdev;
 
 	switch (event) {
-	case PM_SUSPEND_PREPARE:
-		if (kbdev)
-			kbase_device_suspend(kbdev);
-		GPU_LOG(DVFS_DEBUG, LSI_SUSPEND, 0u, 0u, "%s: suspend event\n", __func__);
-		break;
-	case PM_POST_SUSPEND:
-		if (kbdev)
-			kbase_device_resume(kbdev);
-		GPU_LOG(DVFS_DEBUG, LSI_RESUME, 0u, 0u, "%s: resume event\n", __func__);
-		break;
-	default:
-		break;
+		case PM_SUSPEND_PREPARE:
+			if (kbdev)
+				kbase_device_suspend(kbdev);
+			GPU_LOG(DVFS_DEBUG, LSI_SUSPEND, 0u, 0u, "%s: suspend event\n", __func__);
+			break;
+		case PM_POST_SUSPEND:
+			if (kbdev)
+				kbase_device_resume(kbdev);
+			GPU_LOG(DVFS_DEBUG, LSI_RESUME, 0u, 0u, "%s: resume event\n", __func__);
+			break;
+		default:
+			break;
 	}
 	return err;
 }
@@ -160,6 +166,7 @@
 
 static int gpu_power_on(struct kbase_device *kbdev)
 {
+	int ret;
 	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
 	if (!platform)
 		return -ENODEV;
@@ -168,14 +175,18 @@
 
 	gpu_control_disable_customization(kbdev);
 
-	if (pm_runtime_resume(kbdev->dev)) {
+	ret = pm_runtime_resume(kbdev->dev);
+	if (ret > 0) {
 		if (platform->early_clk_gating_status) {
 			GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "already power on\n");
 			gpu_control_enable_clock(kbdev);
 		}
 		return 0;
-	} else {
+	} else if (ret == 0) {
 		return 1;
+	} else {
+		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "runtime pm returned %d\n", ret);
+		return 0;
 	}
 }
 
@@ -214,10 +225,10 @@
 };
 
 static struct notifier_block gpu_noc_nb = {
-	.notifier_call = gpu_noc_notifier
+		.notifier_call = gpu_noc_notifier
 };
 
-static mali_error gpu_device_runtime_init(struct kbase_device *kbdev)
+static int gpu_device_runtime_init(struct kbase_device *kbdev)
 {
 	pm_suspend_ignore_children(kbdev->dev, true);
 	return 0;
@@ -230,26 +241,35 @@
 
 static int pm_callback_dvfs_on(struct kbase_device *kbdev)
 {
+#ifdef CONFIG_MALI_DVFS
 	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
 
 	gpu_dvfs_timer_control(true);
 
 	if (platform->dvfs_pending)
 		platform->dvfs_pending = 0;
+#endif
 
 	return 0;
 }
 
-static int pm_callback_change_dvfs_level(struct kbase_device *kbdev, mali_bool enabledebug)
+static int pm_callback_change_dvfs_level(struct kbase_device *kbdev)
 {
+#ifdef CONFIG_MALI_DVFS
 	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
+	bool enabledebug = false;
+
+	if(kbdev->vendor_callbacks->get_poweron_dbg)
+		enabledebug = kbdev->vendor_callbacks->get_poweron_dbg();
+#if 0
 	if (enabledebug)
 		GPU_LOG(DVFS_ERROR, DUMMY, 0u, 0u, "asv table[%u] clk[%d to %d]MHz, vol[%d (margin : %d) real: %d]mV\n",
 				exynos_get_table_ver(), gpu_get_cur_clock(platform), platform->gpu_dvfs_start_clock,
 				gpu_get_cur_voltage(platform), platform->voltage_margin, platform->cur_voltage);
+#endif
 	gpu_set_target_clk_vol(platform->gpu_dvfs_start_clock, false);
 	gpu_dvfs_reset_env_data(kbdev);
-
+#endif
 	return 0;
 }
 
@@ -261,15 +281,19 @@
 
 	GPU_LOG(DVFS_INFO, LSI_GPU_ON, 0u, 0u, "runtime on callback\n");
 
+	platform->power_status = true;
 	gpu_control_enable_clock(kbdev);
 	gpu_dvfs_start_env_data_gathering(kbdev);
 #ifdef CONFIG_MALI_DVFS
-	if (platform->dvfs_status && platform->wakeup_lock)
+	if (platform->dvfs_status && platform->wakeup_lock && !kbdev->pm.backend.metrics.is_full_compute_util)
 		gpu_set_target_clk_vol(platform->gpu_dvfs_start_clock, false);
 	else
 #endif /* CONFIG_MALI_DVFS */
 		gpu_set_target_clk_vol(platform->cur_clock, false);
-	platform->power_status = true;
+
+#ifdef CONFIG_MALI_DVFS_USER_GOVERNOR
+	gpu_dvfs_notify_poweron();
+#endif
 
 	return 0;
 }
@@ -282,6 +306,10 @@
 
 	GPU_LOG(DVFS_INFO, LSI_GPU_OFF, 0u, 0u, "runtime off callback\n");
 
+#ifdef CONFIG_MALI_DVFS_USER_GOVERNOR
+	gpu_dvfs_notify_poweroff();
+#endif
+
 	platform->power_status = false;
 
 	gpu_dvfs_stop_env_data_gathering(kbdev);
@@ -293,10 +321,12 @@
 	if (!platform->early_clk_gating_status)
 		gpu_control_disable_clock(kbdev);
 
+#if defined(CONFIG_SOC_EXYNOS7420) || defined(CONFIG_SOC_EXYNOS7890)
 	preload_balance_setup(kbdev);
+#endif
 }
 
-kbase_pm_callback_conf pm_callbacks = {
+struct kbase_pm_callback_conf pm_callbacks = {
 	.power_on_callback = gpu_power_on,
 	.power_off_callback = gpu_power_off,
 	.power_suspend_callback = gpu_power_suspend,
@@ -316,6 +346,23 @@
 	.power_change_dvfs_level_callback = NULL,
 #endif /* CONFIG_MALI_RT_PM */
 };
+
+#ifdef CONFIG_EXYNOS_BUSMONITOR
+static int gpu_noc_notifier(struct notifier_block *nb, unsigned long event, void *cmd)
+{
+	if (strstr((char *)cmd, "G3D")) {
+		GPU_LOG(DVFS_ERROR, LSI_RESUME, 0u, 0u, "%s: gpu_noc_notifier\n", __func__);
+		gpu_register_dump();
+	}
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_EXYNOS_BUSMONITOR
+static struct notifier_block gpu_noc_nb = {
+	.notifier_call = gpu_noc_notifier
+};
+#endif
 #endif /* CONFIG_MALI_RT_PM */
 
 int gpu_notifier_init(struct kbase_device *kbdev)
@@ -339,6 +386,8 @@
 #endif
 	pm_runtime_enable(kbdev->dev);
 
+	platform->power_status = true;
+
 	return 0;
 }
 
diff -Nur r5p0/platform/gpu_pmqos.c r15p0/platform/exynos/gpu_pmqos.c
--- r5p0/platform/gpu_pmqos.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_pmqos.c	2017-07-25 18:24:55.000000000 +0200
@@ -26,9 +26,15 @@
 struct pm_qos_request exynos5_g3d_mif_max_qos;
 struct pm_qos_request exynos5_g3d_int_qos;
 struct pm_qos_request exynos5_g3d_cpu_cluster0_min_qos;
-struct pm_qos_request exynos5_g3d_cpu_cluster1_max_qos;
 struct pm_qos_request exynos5_g3d_cpu_cluster1_min_qos;
 
+#ifdef CONFIG_MALI_DVFS_USER
+struct pm_qos_request proactive_mif_min_qos;
+struct pm_qos_request proactive_int_min_qos;
+struct pm_qos_request proactive_apollo_min_qos;
+struct pm_qos_request proactive_atlas_min_qos;
+#endif
+
 int gpu_pm_qos_command(struct exynos_context *platform, gpu_pmqos_state state)
 {
 	DVFS_ASSERT(platform);
@@ -44,7 +50,6 @@
 		if (!platform->pmqos_int_disable)
 			pm_qos_add_request(&exynos5_g3d_int_qos, PM_QOS_DEVICE_THROUGHPUT, 0);
 		pm_qos_add_request(&exynos5_g3d_cpu_cluster0_min_qos, PM_QOS_CLUSTER0_FREQ_MIN, 0);
-		pm_qos_add_request(&exynos5_g3d_cpu_cluster1_max_qos, PM_QOS_CLUSTER1_FREQ_MAX, PM_QOS_CLUSTER1_FREQ_MAX_DEFAULT_VALUE);
 		if (platform->boost_egl_min_lock)
 			pm_qos_add_request(&exynos5_g3d_cpu_cluster1_min_qos, PM_QOS_CLUSTER1_FREQ_MIN, 0);
 		break;
@@ -55,7 +60,6 @@
 		if (!platform->pmqos_int_disable)
 			pm_qos_remove_request(&exynos5_g3d_int_qos);
 		pm_qos_remove_request(&exynos5_g3d_cpu_cluster0_min_qos);
-		pm_qos_remove_request(&exynos5_g3d_cpu_cluster1_max_qos);
 		if (platform->boost_egl_min_lock)
 			pm_qos_remove_request(&exynos5_g3d_cpu_cluster1_min_qos);
 		break;
@@ -72,8 +76,6 @@
 		if (!platform->pmqos_int_disable)
 			pm_qos_update_request(&exynos5_g3d_int_qos, platform->table[platform->step].int_freq);
 		pm_qos_update_request(&exynos5_g3d_cpu_cluster0_min_qos, platform->table[platform->step].cpu_freq);
-		if (!platform->boost_is_enabled)
-			pm_qos_update_request(&exynos5_g3d_cpu_cluster1_max_qos, platform->table[platform->step].cpu_max_freq);
 		break;
 	case GPU_CONTROL_PM_QOS_RESET:
 		pm_qos_update_request(&exynos5_g3d_mif_min_qos, 0);
@@ -82,7 +84,6 @@
 		if (!platform->pmqos_int_disable)
 			pm_qos_update_request(&exynos5_g3d_int_qos, 0);
 		pm_qos_update_request(&exynos5_g3d_cpu_cluster0_min_qos, 0);
-		pm_qos_update_request(&exynos5_g3d_cpu_cluster1_max_qos, PM_QOS_CLUSTER1_FREQ_MAX_DEFAULT_VALUE);
 		break;
 	case GPU_CONTROL_PM_QOS_EGL_SET:
 		pm_qos_update_request(&exynos5_g3d_cpu_cluster1_min_qos, platform->boost_egl_min_lock);
@@ -111,3 +112,90 @@
 
 	return 0;
 }
+
+#ifdef CONFIG_MALI_DVFS_USER
+int proactive_pm_qos_command(struct exynos_context *platform, gpu_pmqos_state state)
+{
+	DVFS_ASSERT(platform);
+
+	if (!platform->devfreq_status)
+		return 0;
+
+	switch (state) {
+		case GPU_CONTROL_PM_QOS_INIT:
+			pm_qos_add_request(&proactive_mif_min_qos, PM_QOS_BUS_THROUGHPUT, 0);
+			pm_qos_add_request(&proactive_apollo_min_qos, PM_QOS_CLUSTER0_FREQ_MIN, 0);
+			pm_qos_add_request(&proactive_atlas_min_qos, PM_QOS_CLUSTER1_FREQ_MIN, 0);
+			if (!platform->pmqos_int_disable)
+				pm_qos_add_request(&proactive_int_min_qos, PM_QOS_DEVICE_THROUGHPUT, 0);
+
+#ifdef CONFIG_PWRCAL
+			update_cal_table();
+#endif
+			break;
+		case GPU_CONTROL_PM_QOS_DEINIT:
+			pm_qos_remove_request(&proactive_mif_min_qos);
+			pm_qos_remove_request(&proactive_apollo_min_qos);
+			pm_qos_remove_request(&proactive_atlas_min_qos);
+			if (!platform->pmqos_int_disable)
+				pm_qos_remove_request(&proactive_int_min_qos);
+			break;
+		case GPU_CONTROL_PM_QOS_RESET:
+			pm_qos_update_request(&proactive_mif_min_qos, 0);
+			pm_qos_update_request(&proactive_apollo_min_qos, 0);
+			pm_qos_update_request(&proactive_atlas_min_qos, 0);
+		default:
+			break;
+	}
+
+	return 0;
+}
+
+int gpu_mif_min_pmqos(struct exynos_context *platform, int mif_step)
+{
+	DVFS_ASSERT(platform);
+
+	if(!platform->devfreq_status)
+		return 0;
+
+	pm_qos_update_request_timeout(&proactive_mif_min_qos, platform->mif_table[mif_step], 30000);
+
+	return 0;
+}
+
+int gpu_int_min_pmqos(struct exynos_context *platform, int int_step)
+{
+	DVFS_ASSERT(platform);
+
+	if(!platform->devfreq_status)
+		return 0;
+
+	pm_qos_update_request_timeout(&proactive_int_min_qos, platform->int_table[int_step], 30000);
+
+	return 0;
+}
+
+int gpu_apollo_min_pmqos(struct exynos_context *platform, int apollo_step)
+{
+	DVFS_ASSERT(platform);
+
+	if(!platform->devfreq_status)
+		return 0;
+
+	pm_qos_update_request_timeout(&proactive_apollo_min_qos, platform->apollo_table[apollo_step], 30000);
+
+	return 0;
+}
+
+int gpu_atlas_min_pmqos(struct exynos_context *platform, int atlas_step)
+{
+	DVFS_ASSERT(platform);
+
+	if(!platform->devfreq_status)
+		return 0;
+
+	pm_qos_update_request_timeout(&proactive_atlas_min_qos, platform->atlas_table[atlas_step], 30000);
+
+	return 0;
+}
+#endif
diff -Nur r5p0/platform/gpu_trace_defs.h r15p0/platform/exynos/gpu_trace_defs.h
--- r5p0/platform/gpu_trace_defs.h	1970-01-01 01:00:00.000000000 +0100
+++ r15p0/platform/exynos/gpu_trace_defs.h	2017-07-20 16:14:04.620559419 +0200
@@ -0,0 +1,53 @@
+/* drivers/gpu/arm/.../platform/gpu_treace_defs.h
+ *
+ * Copyright 2011 by S.LSI. Samsung Electronics Inc.
+ * San#24, Nongseo-Dong, Giheung-Gu, Yongin, Korea
+ *
+ * Samsung SoC Mali-T Series DDK porting layer
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software FoundatIon.
+ */
+
+/**
+ * @file gpu_trace_defs.h
+ * DDK porting layer.
+ */
+
+#if 0 /* Dummy section to avoid breaking formatting */
+int dummy_array[] = {
+#endif
+
+	/* MALI_SEC_INTEGRATION */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_GPU_ON), /* gpu on */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_GPU_OFF), /* gpu off */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_SUSPEND), /* suspend */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_RESUME), /* resume */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_CLOCK_VALUE), /* clock */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_TMU_VALUE), /* TMU LOCK info */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_VOL_VALUE), /* voltage */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_REGISTER_DUMP), /* CMU & PMU info */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_CLOCK_ON), /* GPU CLOCK ON */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_CLOCK_OFF), /* GPU CLOCK OFF*/
+
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_HWCNT_ON_DVFS), /* HWCNT ON DVFS */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_HWCNT_OFF_DVFS), /* HWCNT OFF DVFS */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_HWCNT_ON_GPR), /* HWCNT ON GPR */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_HWCNT_OFF_GPR), /* HWCNT OFF GPR */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_HWCNT_BT_ON), /* HWCNT BT ON */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_HWCNT_BT_OFF), /* HWCNT BT OFF */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_HWCNT_VSYNC_SKIP), /* HWCNT VSYNC SKIP */
+
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_SECURE_WORLD_ENTER), /* SECURE RENDERING START */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_SECURE_WORLD_EXIT), /* SECURE RENDERING END */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_EXYNOS_GPU_INIT_HW),
+
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_CHECKSUM), /* CHECKSUM*/
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_GPU_MAX_LOCK), /* GPU MAX CLOCK LOCK */
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_GPU_MIN_LOCK), /* GPU MIN CLOCK LOCK */
+
+	KBASE_TRACE_CODE_MAKE_CODE(LSI_GPU_SECURE), /* GPU Secure Rendering */
+#if 0
+};
+#endif
diff -Nur r5p0/platform/gpu_utilization.c r15p0/platform/exynos/gpu_utilization.c
--- r5p0/platform/gpu_utilization.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/gpu_utilization.c	2017-07-20 16:14:04.620559419 +0200
@@ -21,13 +21,16 @@
 #include "gpu_control.h"
 #include "gpu_dvfs_handler.h"
 #include "gpu_perf.h"
-#include "gpu_hwcnt.h"
 #include "gpu_ipa.h"
+#ifdef CONFIG_MALI_SEC_HWCNT
+#include "gpu_hwcnt_sec.h"
+#endif
 
 extern struct kbase_device *pkbdev;
 
 /* MALI_SEC_INTEGRATION */
-extern int kbase_pm_get_dvfs_utilisation(struct kbase_device *kbdev, int *, int *);
+#ifdef CONFIG_MALI_DVFS
+extern int gpu_pm_get_dvfs_utilisation(struct kbase_device *kbdev, int *, int *);
 static void gpu_dvfs_update_utilization(struct kbase_device *kbdev)
 {
 	unsigned long flags;
@@ -38,28 +41,26 @@
 #if defined(CONFIG_MALI_DVFS) && defined(CONFIG_CPU_THERMAL_IPA)
 	if (platform->time_tick < platform->gpu_dvfs_time_interval) {
 		platform->time_tick++;
-		platform->time_busy += kbdev->pm.metrics.time_busy;
-		platform->time_idle += kbdev->pm.metrics.time_idle;
+		platform->time_busy += kbdev->pm.backend.metrics.time_busy;
+		platform->time_idle += kbdev->pm.backend.metrics.time_idle;
 	} else {
-		platform->time_busy = kbdev->pm.metrics.time_busy;
-		platform->time_idle = kbdev->pm.metrics.time_idle;
+		platform->time_busy = kbdev->pm.backend.metrics.time_busy;
+		platform->time_idle = kbdev->pm.backend.metrics.time_idle;
 		platform->time_tick = 0;
 	}
 #endif /* CONFIG_MALI_DVFS && CONFIG_CPU_THERMAL_IPA */
 
 	spin_lock_irqsave(&platform->gpu_dvfs_spinlock, flags);
-/* MALI_SEC_INTEGRATION */
-#if 1 /* Temporary patch */
-	platform->env_data.utilization = kbase_pm_get_dvfs_utilisation(kbdev, 0, 0);
-#else
-	platform->env_data.utilization = kbase_platform_dvfs_event(kbdev, 0);
-#endif
+
+	platform->env_data.utilization = gpu_pm_get_dvfs_utilisation(kbdev, 0, 0);
+
 	spin_unlock_irqrestore(&platform->gpu_dvfs_spinlock, flags);
 
 #if defined(CONFIG_MALI_DVFS) && defined(CONFIG_CPU_THERMAL_IPA)
 	gpu_ipa_dvfs_calc_norm_utilisation(kbdev);
 #endif /* CONFIG_MALI_DVFS && CONFIG_CPU_THERMAL_IPA */
 }
+#endif /* CONFIG_MALI_DVFS */
 
 static int gpu_dvfs_update_perf(struct kbase_device *kbdev)
 {
@@ -92,31 +93,6 @@
 	return 0;
 }
 
-static int gpu_dvfs_update_hwc(struct kbase_device *kbdev)
-{
-	unsigned long flags;
-	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
-
-	DVFS_ASSERT(platform);
-
-#ifdef MALI_SEC_HWCNT
-	if ((!platform->hwcnt_gathering_status) || (kbdev->hwcnt.is_init != TRUE))
-		return 0;
-
-	mutex_lock(&kbdev->hwcnt.mlock);
-	exynos_gpu_hwcnt_update(kbdev);
-	mutex_unlock(&kbdev->hwcnt.mlock);
-#endif
-
-	spin_lock_irqsave(&platform->gpu_dvfs_spinlock, flags);
-	platform->env_data.hwcnt = 0;
-	spin_unlock_irqrestore(&platform->gpu_dvfs_spinlock, flags);
-
-	GPU_LOG(DVFS_INFO, DUMMY, 0u, 0u, "Current HWC: %d\n", platform->env_data.hwcnt);
-
-	return 0;
-}
-
 int gpu_dvfs_start_env_data_gathering(struct kbase_device *kbdev)
 {
 	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
@@ -138,6 +114,7 @@
 	return 0;
 }
 
+#ifdef CONFIG_MALI_DVFS
 int gpu_dvfs_reset_env_data(struct kbase_device *kbdev)
 {
 	unsigned long flags;
@@ -146,8 +123,8 @@
 	DVFS_ASSERT(platform);
 	/* reset gpu utilization value */
 	spin_lock_irqsave(&platform->gpu_dvfs_spinlock, flags);
-	kbdev->pm.metrics.time_idle = kbdev->pm.metrics.time_idle + kbdev->pm.metrics.time_busy;
-	kbdev->pm.metrics.time_busy = 0;
+	kbdev->pm.backend.metrics.time_idle = kbdev->pm.backend.metrics.time_idle + kbdev->pm.backend.metrics.time_busy;
+	kbdev->pm.backend.metrics.time_busy = 0;
 	spin_unlock_irqrestore(&platform->gpu_dvfs_spinlock, flags);
 
 	return 0;
@@ -156,14 +133,40 @@
 int gpu_dvfs_calculate_env_data(struct kbase_device *kbdev)
 {
 	struct exynos_context *platform = (struct exynos_context *) kbdev->platform_context;
+	static int polling_period = 0;
 
 	DVFS_ASSERT(platform);
 
 	gpu_dvfs_update_utilization(kbdev);
-	gpu_dvfs_update_hwc(kbdev);
 
+	polling_period -= platform->polling_speed;
+	if (polling_period > 0)
+		return 0;
+
+	if (platform->dvs_is_enabled == true)
+		return 0;
+
+#ifdef CONFIG_MALI_SEC_HWCNT
+	if (kbdev->hwcnt.is_hwcnt_attach == true && kbdev->hwcnt.is_hwcnt_gpr_enable == false) {
+		polling_period = platform->hwcnt_polling_speed;
+		if (!gpu_control_is_power_on(kbdev))
+			return 0;
+		mutex_lock(&kbdev->hwcnt.mlock);
+		if (platform->cur_clock >= platform->gpu_max_clock_limit || platform->hwcnt_profile == true ) {
+			if (kbdev->vendor_callbacks->hwcnt_update) {
+				kbdev->vendor_callbacks->hwcnt_update(kbdev);
+				dvfs_hwcnt_get_resource(kbdev);
+				dvfs_hwcnt_utilization_equation(kbdev);
+			}
+		}
+		else
+			platform->hwcnt_bt_clk = false;
+		mutex_unlock(&kbdev->hwcnt.mlock);
+	}
+#endif
 	return 0;
 }
+#endif
 
 int gpu_dvfs_calculate_env_data_ppmu(struct kbase_device *kbdev)
 {
diff -Nur r5p0/platform/Kbuild r15p0/platform/exynos/Kbuild
--- r5p0/platform/Kbuild	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/Kbuild	2017-07-20 16:14:04.620559419 +0200
@@ -14,11 +14,12 @@
 
 MALI_CUSTOMER_RELEASE = 1
 
-platform_name := $(shell echo $(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME))
+# platform_name := $(shell echo $(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME))
 
+obj-y += gpu_integration_callbacks.o
 obj-y += mali_kbase_platform.o
 obj-y += gpu_notifier.o
-obj-y += gpu_exynos$(platform_name).o
+obj-y += gpu_exynos7420.o
 obj-y += gpu_control.o
 obj-y += gpu_pmqos.o
 obj-y += gpu_utilization.o
@@ -26,7 +27,7 @@
 obj-y += gpu_dvfs_api.o
 obj-y += gpu_dvfs_governor.o
 obj-y += gpu_perf.o
-obj-y += gpu_hwcnt.o
 obj-y += gpu_balance.o
+obj-$(CONFIG_MALI_SEC_HWCNT) += gpu_hwcnt_sec.o
 obj-$(CONFIG_MALI_DEBUG_SYS) += gpu_custom_interface.o
 obj-$(CONFIG_CPU_THERMAL_IPA) += gpu_ipa.o
diff -Nur r5p0/platform/Kconfig r15p0/platform/exynos/Kconfig
--- r5p0/platform/Kconfig	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/Kconfig	2017-07-20 16:14:04.620559419 +0200
@@ -21,4 +21,104 @@
 #
 # Where xxx is the platform name is the name set in MALI_PLATFORM_THIRDPARTY_NAME
 #
+config EXYNOS_SOC_NAME
+    depends on MALI_T7XX
+    string "Third party soc name"
+    help
+      soc name.
 
+# { SRUK-MALI_SYSTRACE_SUPPORT
+config MALI_SYSTRACE_SUPPORT
+    bool "Enable Exynos systrace Debug support"
+    depends on MALI_T7XX
+    default y
+    help
+      Enable systrace marker in kernel
+# SRUK-MALI_SYSTRACE_SUPPORT }
+
+config MALI_SEC_HWCNT
+    bool "Enable sec hwcnt feature"
+    depends on MALI_T7XX
+    default y
+    help
+      Enable sec hwcnt feature.
+
+config MALI_DVFS
+    bool "Enable EXYNOS DVFS"
+    depends on MALI_T7XX
+    default n
+    help
+      Choose this option to enable DVFS in the Mali Midgard DDK.
+
+config MALI_RT_PM
+    bool "Enable EXYNOS Runtime power management"
+    depends on MALI_T7XX
+    depends on PM
+    default y
+    help
+      Choose this option to enable runtime power management in the Mali Midgard DDK.
+
+config MALI_EXYNOS_TRACE
+    bool "Enable EXYNOS kbase tracing"
+    depends on MALI_T7XX
+    default y
+    help
+      Enables tracing in kbase.  Trace log available through
+      the "mali_trace" debugfs file, when the CONFIG_DEBUG_FS is enabled
+
+config MALI_DEBUG_SYS
+    bool "Enable sysfs for the Mali Midgard DDK "
+    depends on MALI_T7XX && SYSFS
+    default y
+    help
+      Enables sysfs for the Mali Midgard DDK. Set/Monitor the Mali Midgard DDK
+
+config MALI_SEC_CL_BOOST
+    bool "Enable EXYNOS cl booster"
+    depends on MALI_T7XX
+    default y
+    help
+      Enables open cl dvfs booster.
+
+
+config MALI_SEC_UTILIZATION
+    bool "Enable EXYNOS custom utilization"
+    depends on MALI_T7XX
+    default y
+    help
+      Enables exynos custom utilization.
+
+config MALI_PM_QOS
+    bool "Enable DVFS with QoS"
+    depends on MALI_T7XX && MALI_DVFS && PM_DEVFREQ && CPU_FREQ
+    default y
+    help
+      Choose this option to enable PM_QOS in the Mali tMIX DDK.
+
+config MALI_BTS_OPTIMIZATION
+    bool "Enable GPU BTS"
+    depends on MALI_DVFS
+    default n
+    help
+      Choose this option to enable BTS in the Mali tMIX DDK.
+
+config MALI_DEBUG_KERNEL_SYSFS
+       bool "Support Kernel Group Debug SysFS"
+       depends on MALI_T7XX && MALI_DEBUG_SYS
+       default y
+       help
+               Support Support Kernel Group Debug SysFS on /sys/kernel/gpu
+
+config MALI_EXYNOS_SECURE_RENDERING
+    bool "Enable EXYNOS Secure Rendering Support"
+    depends on MALI_T7XX && ION_EXYNOS && EXYNOS_CONTENT_PATH_PROTECTION
+    default n
+    help
+      Support Secure Rendering on Exynos SoC
+
+config MALI_SEC_ASP_SECURE_BUF_CTRL
+    bool "Enable EXYNOS Secure Rendering whit ASP Support"
+    depends on MALI_T8XX && MALI_EXYNOS_SECURE_RENDERING && ION_EXYNOS && EXYNOS_CONTENT_PATH_PROTECTION
+    default n
+    help
+      Support Secure Rendering with ASP on Exynos SoC
diff -Nur r5p0/platform/mali_kbase_config_platform.h r15p0/platform/exynos/mali_kbase_config_platform.h
--- r5p0/platform/mali_kbase_config_platform.h	1970-01-01 01:00:00.000000000 +0100
+++ r15p0/platform/exynos/mali_kbase_config_platform.h	2017-07-20 16:14:04.620559419 +0200
@@ -0,0 +1,92 @@
+/*
+ *
+ * (C) COPYRIGHT 2014-2015 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * Maximum frequency GPU will be clocked at. Given in kHz.
+ * This must be specified as there is no default value.
+ *
+ * Attached value: number in kHz
+ * Default value: NA
+ */
+#define GPU_FREQ_KHZ_MAX (5000)
+/**
+ * Minimum frequency GPU will be clocked at. Given in kHz.
+ * This must be specified as there is no default value.
+ *
+ * Attached value: number in kHz
+ * Default value: NA
+ */
+#define GPU_FREQ_KHZ_MIN (5000)
+
+/**
+ * CPU_SPEED_FUNC - A pointer to a function that calculates the CPU clock
+ *
+ * CPU clock speed of the platform is in MHz - see kbase_cpu_clk_speed_func
+ * for the function prototype.
+ *
+ * Attached value: A kbase_cpu_clk_speed_func.
+ * Default Value:  NA
+ */
+#define CPU_SPEED_FUNC (NULL)
+
+/**
+ * GPU_SPEED_FUNC - A pointer to a function that calculates the GPU clock
+ *
+ * GPU clock speed of the platform in MHz - see kbase_gpu_clk_speed_func
+ * for the function prototype.
+ *
+ * Attached value: A kbase_gpu_clk_speed_func.
+ * Default Value:  NA
+ */
+#define GPU_SPEED_FUNC (NULL)
+
+/**
+ * Power management configuration
+ *
+ * Attached value: pointer to @ref kbase_pm_callback_conf
+ * Default value: See @ref kbase_pm_callback_conf
+ */
+#define POWER_MANAGEMENT_CALLBACKS (&pm_callbacks)
+
+/**
+ * Platform specific configuration functions
+ *
+ * Attached value: pointer to @ref kbase_platform_funcs_conf
+ * Default value: See @ref kbase_platform_funcs_conf
+ */
+//#define PLATFORM_FUNCS (NULL)
+#define PLATFORM_FUNCS (&platform_funcs)
+
+/** Power model for IPA
+ *
+ * Attached value: pointer to @ref mali_pa_model_ops
+ */
+#define POWER_MODEL_CALLBACKS (NULL)
+
+extern struct kbase_pm_callback_conf pm_callbacks;
+extern struct kbase_platform_funcs_conf platform_funcs;
+
+/**
+ * Secure mode switch
+ *
+ * Attached value: pointer to @ref kbase_secure_ops
+ */
+#ifdef CONFIG_MALI_EXYNOS_SECURE_RENDERING
+#define PROTECTED_CALLBACKS (&exynos_secure_ops)
+extern struct kbase_protected_ops exynos_secure_ops;
+#endif
+
diff -Nur r5p0/platform/mali_kbase_platform.c r15p0/platform/exynos/mali_kbase_platform.c
--- r5p0/platform/mali_kbase_platform.c	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/mali_kbase_platform.c	2017-07-20 16:14:04.620559419 +0200
@@ -25,8 +25,16 @@
 #include "gpu_control.h"
 
 /* MALI_SEC_SECURE_RENDERING */
+#ifdef CONFIG_MALI_EXYNOS_SECURE_RENDERING
 #include <linux/smc.h>
 #include <asm/cacheflush.h>
+#ifdef CONFIG_MALI_SEC_ASP_SECURE_BUF_CTRL
+#include <linux/exynos_ion.h>
+extern struct ion_device *ion_exynos;
+#endif
+#define PROT_G3D        0xC
+#define SMC_TZPC_OK     0x2
+#endif
 
 struct kbase_device *pkbdev;
 static int gpu_debug_level;
@@ -47,6 +55,24 @@
 }
 
 #ifdef CONFIG_MALI_EXYNOS_TRACE
+struct kbase_trace exynos_trace_buf[KBASE_TRACE_SIZE];
+extern const struct file_operations kbasep_trace_debugfs_fops;
+static int gpu_trace_init(struct kbase_device *kbdev)
+{
+	kbdev->trace_rbuf = exynos_trace_buf;
+
+	spin_lock_init(&kbdev->trace_lock);
+//	kbasep_trace_debugfs_init(kbdev);
+/* below work : register entry from making debugfs create file to trace_dentry
+ * is same work as kbasep_trace_debugfs_init */
+#ifdef MALI_SEC_INTEGRATION
+	kbdev->trace_dentry = debugfs_create_file("mali_trace", S_IRUGO,
+			kbdev->mali_debugfs_directory, kbdev,
+			&kbasep_trace_debugfs_fops);
+#endif /* MALI_SEC_INTEGRATION */
+	return 0;
+}
+
 static int gpu_trace_level;
 
 void gpu_set_trace_level(int level)
@@ -81,6 +107,9 @@
 	case KBASE_TRACE_CODE(LSI_CLOCK_OFF):
 	case KBASE_TRACE_CODE(LSI_GPU_MAX_LOCK):
 	case KBASE_TRACE_CODE(LSI_GPU_MIN_LOCK):
+	case KBASE_TRACE_CODE(LSI_SECURE_WORLD_ENTER):
+	case KBASE_TRACE_CODE(LSI_SECURE_WORLD_EXIT):
+	case KBASE_TRACE_CODE(LSI_EXYNOS_GPU_INIT_HW):
 		level = TRACE_CLK;
 		break;
 	case KBASE_TRACE_CODE(LSI_VOL_VALUE):
@@ -236,13 +265,16 @@
 	data = gpu_get_attrib_data(attrib, GPU_PERF_GATHERING);
 	platform->perf_gathering_status = data == 0 ? 0 : data;
 
-#ifdef MALI_SEC_HWCNT
+#ifdef CONFIG_MALI_SEC_HWCNT
 	data = gpu_get_attrib_data(attrib, GPU_HWCNT_GATHERING);
 	platform->hwcnt_gathering_status = data == 0 ? 0 : data;
 
 	data = gpu_get_attrib_data(attrib, GPU_HWCNT_GPR);
 	platform->hwcnt_gpr_status = data == 0 ? 0 : data;
 
+	data = gpu_get_attrib_data(attrib, GPU_HWCNT_PROFILE);
+	platform->hwcnt_profile = data == 0 ? 0 : data;
+
 	data = gpu_get_attrib_data(attrib, GPU_HWCNT_POLLING_TIME);
 	platform->hwcnt_polling_speed = data == 0 ? 0 : (u32) data;
 
@@ -276,13 +308,20 @@
 	data = gpu_get_attrib_data(attrib, GPU_TRACE_LEVEL);
 	gpu_set_trace_level(data == 0 ? TRACE_ALL : (u32) data);
 #endif /* CONFIG_MALI_EXYNOS_TRACE */
+#ifdef CONFIG_MALI_DVFS_USER
+	data = gpu_get_attrib_data(attrib, GPU_UDVFS_ENABLE);
+	platform->udvfs_enable = data == 0 ? 0 : (u32) data;
+#endif
+	data = gpu_get_attrib_data(attrib, GPU_MO_MIN_CLOCK);
+	platform->mo_min_clock = data == 0 ? 0 : (u32) data;
 
 	return 0;
 }
-extern void preload_balance_init(struct kbase_device *kbdev);
+
 static int gpu_context_init(struct kbase_device *kbdev)
 {
 	struct exynos_context *platform;
+	struct mali_base_gpu_core_props *core_props;
 
 	platform = kmalloc(sizeof(struct exynos_context), GFP_KERNEL);
 
@@ -293,56 +332,76 @@
 	kbdev->platform_context = (void *) platform;
 	pkbdev = kbdev;
 
+#ifdef CONFIG_MALI_SEC_HWCNT
+	mutex_init(&kbdev->hwcnt.dvs_lock);
+#endif
+
 	mutex_init(&platform->gpu_clock_lock);
 	mutex_init(&platform->gpu_dvfs_handler_lock);
+#ifdef CONFIG_MALI_DVFS_USER
+	mutex_init(&platform->gpu_process_job_lock);
+#endif
 	spin_lock_init(&platform->gpu_dvfs_spinlock);
 
 	gpu_validate_attrib_data(platform);
-	preload_balance_init(kbdev);
 
+	core_props = &(kbdev->gpu_props.props.core_props);
+	core_props->gpu_freq_khz_min = platform->gpu_min_clock * 1000;
+	core_props->gpu_freq_khz_max = platform->gpu_max_clock * 1000;
+
+	kbdev->vendor_callbacks = (struct kbase_vendor_callbacks *)gpu_get_callbacks();
+
+#ifdef CONFIG_MALI_EXYNOS_TRACE
+	if (gpu_trace_init(kbdev) != 0)
+		return -1;
+#endif
 	return 0;
 }
 
 /**
  ** Exynos5 hardware specific initialization
  **/
-static mali_bool kbase_platform_exynos5_init(struct kbase_device *kbdev)
+static int kbase_platform_exynos5_init(struct kbase_device *kbdev)
 {
 	/* gpu context init */
 	if (gpu_context_init(kbdev) < 0)
 		goto init_fail;
 
+#if defined(CONFIG_SOC_EXYNOS7420) || defined(CONFIG_SOC_EXYNOS7890)
+	if(gpu_device_specific_init(kbdev) < 0)
+		goto init_fail;
+#endif
 	/* gpu control module init */
 	if (gpu_control_module_init(kbdev) < 0)
 		goto init_fail;
 
+	/* gpu notifier init */
+	if (gpu_notifier_init(kbdev) < 0)
+		goto init_fail;
+
+#ifdef CONFIG_MALI_DVFS
 	/* gpu utilization moduel init */
 	gpu_dvfs_utilization_init(kbdev);
 
 	/* dvfs governor init */
 	gpu_dvfs_governor_init(kbdev);
 
-#ifdef CONFIG_MALI_DVFS
 	/* dvfs handler init */
 	gpu_dvfs_handler_init(kbdev);
 #endif /* CONFIG_MALI_DVFS */
 
-	/* gpu notifier init */
-	if (gpu_notifier_init(kbdev) < 0)
-		goto init_fail;
-
 #ifdef CONFIG_MALI_DEBUG_SYS
 	/* gpu sysfs file init */
 	if (gpu_create_sysfs_file(kbdev->dev) < 0)
 		goto init_fail;
 #endif /* CONFIG_MALI_DEBUG_SYS */
 
-	return true;
+	return 0;
 
 init_fail:
 	kfree(kbdev->platform_context);
 
-	return false;
+	return -1;
 }
 
 /**
@@ -371,155 +430,268 @@
 #endif /* CONFIG_MALI_DEBUG_SYS */
 }
 
-kbase_platform_funcs_conf platform_funcs = {
+struct kbase_platform_funcs_conf platform_funcs = {
 	.platform_init_func = &kbase_platform_exynos5_init,
 	.platform_term_func = &kbase_platform_exynos5_term,
 };
 
 /* MALI_SEC_SECURE_RENDERING */
-static int exynos_secure_mode_enable(void)
+#ifdef CONFIG_MALI_EXYNOS_SECURE_RENDERING
+static int exynos_secure_mode_enable(struct kbase_device *kbdev)
 {
 	/* enable secure mode : TZPC */
 	int ret = 0;
 
+	if (!kbdev)
+		goto secure_out;
+
+	if (!kbdev->protected_mode_support) {
+		GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong operation! DDK cannot support Secure Rendering\n", __func__);
+		ret = -EINVAL;
+		goto secure_out;
+	}
+#if defined(CONFIG_EXYNOS_CONTENT_PATH_PROTECTION)
+	gpu_cacheclean(kbdev);
+
+#ifdef CONFIG_MALI_SEC_ASP_SECURE_BUF_CTRL
+	ret = exynos_smc(SMC_DRM_SECBUF_CFW_PROT,
+                     kbdev->sec_sr_info.secure_crc_phys, kbdev->sec_sr_info.secure_crc_sizes,
+                     PROT_G3D);
+
+	if (ret != DRMDRV_OK) {
+		GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: CRC : failed to set secure buffer region by err 0x%x, physical addr 0x%08x\n",
+			__func__, ret, (unsigned int)kbdev->sec_sr_info.secure_crc_phys);
+		goto secure_out;
+	}
+#endif
+
 	ret = exynos_smc(SMC_PROTECTION_SET, 0,
-                            0xc, SMC_PROTECTION_ENABLE);
+                    PROT_G3D, SMC_PROTECTION_ENABLE);
+
+	GPU_LOG(DVFS_INFO, LSI_SECURE_WORLD_ENTER, 0u, 0u, "LSI_SECURE_WORLD_ENTER\n");
+	if (ret == SMC_TZPC_OK)
+		ret = 0;
 
+#endif // defined(CONFIG_EXYNOS_CONTENT_PATH_PROTECTION)
+secure_out:
 	return ret;
 }
 
-static int exynos_secure_mode_disable(void)
+static int exynos_secure_mode_disable(struct kbase_device *kbdev)
 {
 	/* Turn off secure mode and reset GPU : TZPC */
 	int ret = 0;
 
+	if (!kbdev)
+		goto secure_out;
+
+	if (!kbdev->protected_mode_support) {
+		GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong operation! DDK cannot support Secure Rendering\n", __func__);
+		ret = -EINVAL;
+		goto secure_out;
+	}
+#if defined(CONFIG_EXYNOS_CONTENT_PATH_PROTECTION)
+	gpu_cacheclean(kbdev);
+
+#ifdef CONFIG_MALI_SEC_ASP_SECURE_BUF_CTRL
+	ret = exynos_smc(SMC_DRM_SECBUF_CFW_UNPROT,
+                     kbdev->sec_sr_info.secure_crc_phys, kbdev->sec_sr_info.secure_crc_sizes,
+                     PROT_G3D);
+
+	if(ret != DRMDRV_OK) {
+		GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: CRC : failed to unset secure buffer region by err 0x%x, physical addr 0x%08x\n",
+			__func__, ret, (unsigned int)kbdev->sec_sr_info.secure_crc_phys);
+		goto secure_out;
+	}
+#endif
+
 	ret = exynos_smc(SMC_PROTECTION_SET, 0,
-                            0xc, SMC_PROTECTION_DISABLE);
+                     PROT_G3D, SMC_PROTECTION_DISABLE);
+
+	GPU_LOG(DVFS_INFO, LSI_SECURE_WORLD_EXIT, 0u, 0u, "LSI_SECURE_WORLD_EXIT\n");
+	if (ret == SMC_TZPC_OK)
+		ret = 0;
 
+#endif // defined(CONFIG_EXYNOS_CONTENT_PATH_PROTECTION)
+secure_out:
 	return ret;
 }
 
-static int exynos_secure_mem_enable(void)
+static bool exynos_secure_mode_init(struct kbase_device *kbdev)
+{
+	int ret = -EINVAL;
+
+#if defined(CONFIG_ION) && defined(CONFIG_EXYNOS_CONTENT_PATH_PROTECTION)
+#ifdef CONFIG_MALI_SEC_ASP_SECURE_BUF_CTRL
+	ret = ion_exynos_contig_heap_info(SMC_GPU_CRC_REGION_NUM,
+		&kbdev->sec_sr_info.secure_crc_phys, &kbdev->sec_sr_info.secure_crc_sizes);
+
+	if (!ret) {
+		GPU_LOG(DVFS_WARNING, LSI_GPU_SECURE, 0u, 0u, "%s: supporting Secure Rendering : region - 0x%08x, sizes - 0x%x\n",
+			__func__, (unsigned int)kbdev->sec_sr_info.secure_crc_phys, (unsigned int)kbdev->sec_sr_info.secure_crc_sizes);
+	} else
+#else
+	ret = 0;
+
+	GPU_LOG(DVFS_WARNING, LSI_GPU_SECURE, 0u, 0u, "%s: supporting Secure Rendering, NO use ASP feature.\n", __func__);
+#endif
+#endif
+	if (ret) {
+		GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: can NOT support Secure Rendering, error %d\n", __func__, ret);
+		return false;
+	}
+
+	return true;
+}
+
+static int exynos_secure_mem_enable(struct kbase_device *kbdev, int ion_fd, u64 flags, struct kbase_va_region *reg)
 {
 	/* enable secure world mode : TZASC */
 	int ret = 0;
 
-	flush_all_cpu_caches();
-	ret = exynos_smc(SMC_MEM_PROT_SET, 0, 0, 1);
-	if( ret == SMC_CALL_ERROR ) {
-		exynos_smc(SMC_MEM_PROT_SET, 0, 0, 0);
+	if (!kbdev)
+		goto secure_out;
+
+	if (!kbdev->protected_mode_support) {
+		GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong operation! DDK cannot support Secure Rendering\n", __func__);
+		ret = -EINVAL;
+		goto secure_out;
 	}
 
+	if (!reg) {
+		GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong input argument, reg %p\n",
+			__func__, reg);
+		goto secure_out;
+	}
+#if defined(CONFIG_ION) && defined(CONFIG_EXYNOS_CONTENT_PATH_PROTECTION)
+#ifdef CONFIG_MALI_SEC_ASP_SECURE_BUF_CTRL
+	{
+		struct ion_client *client;
+		struct ion_handle *ion_handle;
+		size_t len = 0;
+		unsigned int CRC_CHECK_MASK = 0xFFFF & kbdev->sec_sr_info.secure_flags_crc_asp;
+		u64 SECURE_CRC_FLAGS = (kbdev->sec_sr_info.secure_flags_crc_asp >> CRC_CHECK_MASK) & 0xFFFF;
+		u64 input_flags = (flags >> CRC_CHECK_MASK) & 0xFFFF;
+
+		ion_phys_addr_t phys = 0;
+
+		flush_all_cpu_caches();
+
+		if (input_flags == SECURE_CRC_FLAGS) {
+			reg->flags |= KBASE_REG_SECURE_CRC | KBASE_REG_SECURE;
+		} else {
+			client = ion_client_create(ion_exynos, "G3D");
+			if (IS_ERR(client)) {
+				GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: Failed to get ion_client of G3D\n",
+						__func__);
+				goto secure_out;
+			}
+
+			ion_handle = ion_import_dma_buf(client, ion_fd);
+
+			if (IS_ERR(ion_handle)) {
+				GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: Failed to get ion_handle of G3D\n",
+						__func__);
+				ion_client_destroy(client);
+				goto secure_out;
+			}
+
+			if (ion_phys(client, ion_handle, &phys, &len)) {
+				GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: Failed to get phys. addr of G3D\n",
+						__func__);
+				ion_free(client, ion_handle);
+				ion_client_destroy(client);
+				goto secure_out;
+			}
+
+			ion_free(client, ion_handle);
+			ion_client_destroy(client);
+
+			ret = exynos_smc(SMC_DRM_SECBUF_CFW_PROT, phys, len, PROT_G3D);
+			if (ret != DRMDRV_OK) {
+				GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: failed to set secure buffer region of G3D buffer, phy 0x%08x, error 0x%x\n",
+					__func__, (unsigned int)phys, ret);
+				BUG();
+			}
+
+			reg->flags |= KBASE_REG_SECURE;
+		}
+
+		reg->phys_by_ion = phys;
+		reg->len_by_ion = len;
+	}
+#else
+	ret = 0;
+
+	reg->flags |= KBASE_REG_SECURE;
+#endif
+#else
+	GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong operation! DDK cannot support Secure Rendering\n", __func__);
+	ret = -EINVAL;
+#endif // defined(CONFIG_ION) && defined(CONFIG_EXYNOS_CONTENT_PATH_PROTECTION)
+
+	return ret;
+secure_out:
+	ret = -EINVAL;
 	return ret;
 }
-static int exynos_secure_mem_disable(void)
+
+static int exynos_secure_mem_disable(struct kbase_device *kbdev, struct kbase_va_region *reg)
 {
 	/* Turn off secure world mode : TZASC */
 	int ret = 0;
 
-	ret = exynos_smc(SMC_MEM_PROT_SET, 0, 0, 0);
+	if (!kbdev)
+		goto secure_out;
+
+	if (!kbdev->protected_mode_support) {
+		GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong operation! DDK cannot support Secure Rendering\n", __func__);
+		ret = -EINVAL;
+		goto secure_out;
+	}
 
+	if (!reg) {
+		GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong input argument, reg %p\n",
+			__func__, reg);
+		ret = -EINVAL;
+		goto secure_out;
+	}
+#if defined(CONFIG_ION) && defined(CONFIG_EXYNOS_CONTENT_PATH_PROTECTION)
+#ifdef CONFIG_MALI_SEC_ASP_SECURE_BUF_CTRL
+	if ( (reg->flags & KBASE_REG_SECURE) &&
+	    !(reg->flags & KBASE_REG_SECURE_CRC)) {
+		int ret;
+
+		ret = exynos_smc(SMC_DRM_SECBUF_CFW_UNPROT,
+				reg->phys_by_ion, reg->len_by_ion, PROT_G3D);
+
+		if (ret != DRMDRV_OK) {
+			GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: failed to unset secure buffer region of G3D buffer, phys 0x%08x, error 0x%x\n",
+				__func__, (unsigned int)reg->phys_by_ion, ret);
+			BUG();
+		}
+	}
+#endif
+#else
+	GPU_LOG(DVFS_ERROR, LSI_GPU_SECURE, 0u, 0u, "%s: wrong operation! DDK cannot support Secure Rendering\n", __func__);
+	ret = -EINVAL;
+#endif // defined(CONFIG_ION) && defined(CONFIG_EXYNOS_CONTENT_PATH_PROTECTION)
+
+secure_out:
 	return ret;
 }
 
-static struct kbase_secure_ops exynos_secure_ops = {
-	.secure_mode_enable  = exynos_secure_mode_enable,
-	.secure_mode_disable = exynos_secure_mode_disable,
+struct kbase_protected_ops exynos_secure_ops = {
+	.protected_mode_enter = exynos_secure_mode_enable,
+	.protected_mode_reset = exynos_secure_mode_disable,
 /* MALI_SEC_SECURE_RENDERING */
+	.protected_mode_supported = exynos_secure_mode_init,
 	.secure_mem_enable   = exynos_secure_mem_enable,
 	.secure_mem_disable  = exynos_secure_mem_disable,
 };
+#endif
 
-extern kbase_pm_callback_conf pm_callbacks;
-
-static kbase_attribute config_attributes[] = {
-#ifdef CONFIG_MALI_RT_PM
-	{
-		KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS,
-		(uintptr_t)&pm_callbacks
-	},
-#endif /* CONFIG_MALI_RT_PM */
-	{
-		KBASE_CONFIG_ATTR_POWER_MANAGEMENT_DVFS_FREQ,
-		100
-	}, /* 100ms */
-	{
-		KBASE_CONFIG_ATTR_PLATFORM_FUNCS,
-		(uintptr_t)&platform_funcs
-	},
-	{
-		KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS,
-		50 /* 50ms before cancelling stuck jobs */
-	},
-	{
-		KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
-		6 /* 0.3sec */
-	},
-	{
-		KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
-		7 /* 0.35sec */
-	},
-	{
-		KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
-		8 /* 0.4sec */
-	},
-	{
-		KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
-		12 /* 0.6sec */
-	},
-	{
-		KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
-		14 /* 0.7sec */
-	},
-	{
-		KBASE_CONFIG_ATTR_CPU_SPEED_FUNC,
-		(uintptr_t)&get_cpu_clock_speed
-	},
-	{
-		KBASE_CONFIG_ATTR_SECURE_CALLBACKS,
-		(uintptr_t)&exynos_secure_ops
-	},
-	{
-		KBASE_CONFIG_ATTR_END,
-		0
-	}
-};
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 10, 0)
-static kbase_io_resources io_resources = {
-	.job_irq_number   = JOB_IRQ_NUMBER,
-	.mmu_irq_number   = MMU_IRQ_NUMBER,
-	.gpu_irq_number   = GPU_IRQ_NUMBER,
-	.io_memory_region = {
-		.start = EXYNOS5_PA_G3D,
-		.end   = EXYNOS5_PA_G3D + (4096 * 5) - 1
-	}
-};
-#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(3, 10, 0) */
-
-kbase_platform_config platform_config = {
-		.attributes                = config_attributes,
-#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 10, 0)
-		.io_resources              = &io_resources,
-#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(3, 10, 0) */
-};
-
-int kbase_platform_early_init(struct platform_device *pdev)
+int kbase_platform_early_init(void)
 {
-	kbase_platform_config *config;
-	int attribute_count;
-
-	config = &platform_config;
-	attribute_count = kbasep_get_config_attribute_count(config->attributes);
-
-	return platform_device_add_data(
-#ifndef CONFIG_MALI_PLATFORM_FAKE
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 10, 0)
-		pdev,
-#else
-		&exynos5_device_g3d,
-#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(3, 10, 0) */
-#endif /* CONFIG_MALI_PLATFORM_FAKE */
-		config->attributes,
-		attribute_count * sizeof(config->attributes[0]));
+	return 0;
 }
diff -Nur r5p0/platform/mali_kbase_platform.h r15p0/platform/exynos/mali_kbase_platform.h
--- r5p0/platform/mali_kbase_platform.h	2017-07-20 16:14:04.636559268 +0200
+++ r15p0/platform/exynos/mali_kbase_platform.h	2017-07-25 18:24:55.753690731 +0200
@@ -78,6 +78,9 @@
 typedef enum {
 	TMU_LOCK = 0,
 	SYSFS_LOCK,
+#ifdef CONFIG_MALI_DVFS_USER
+	USER_LOCK,
+#endif
 #ifdef CONFIG_CPU_THERMAL_IPA
 	IPA_LOCK,
 #endif /* CONFIG_CPU_THERMAL_IPA */
@@ -136,9 +139,10 @@
 	GPU_EARLY_CLK_GATING,
 	GPU_DVS,
 	GPU_PERF_GATHERING,
-#ifdef MALI_SEC_HWCNT
+#ifdef CONFIG_MALI_SEC_HWCNT
 	GPU_HWCNT_GATHERING,
 	GPU_HWCNT_GPR,
+	GPU_HWCNT_PROFILE,
 	GPU_HWCNT_POLLING_TIME,
 	GPU_HWCNT_UP_STEP,
 	GPU_HWCNT_DOWN_STEP,
@@ -157,6 +161,11 @@
 	GPU_CL_DVFS_START_BASE,
 	GPU_DEBUG_LEVEL,
 	GPU_TRACE_LEVEL,
+#ifdef CONFIG_MALI_DVFS_USER
+	GPU_UDVFS_ENABLE,
+	GPU_UHWCNT_ENABLE,
+#endif
+	GPU_MO_MIN_CLOCK,
 	GPU_CONFIG_LIST_END,
 } gpu_config_list;
 
@@ -176,7 +185,6 @@
 	int mem_freq;
 	int int_freq;
 	int cpu_freq;
-	int cpu_max_freq;
 } gpu_dvfs_info;
 
 typedef struct _gpu_dvfs_governor_info {
@@ -194,10 +202,57 @@
 	int hwcnt;
 } gpu_dvfs_env_data;
 
+#ifdef CONFIG_MALI_DVFS_USER
+typedef enum {
+	HWC_DATA_NUM,
+	HWC_DATA_CLOCK,
+	HWC_DATA_UTILIZATION,
+	HWC_DATA_GPU_ACTIVE,
+	HWC_DATA_TRIPIPE_ACTIVE,
+	HWC_DATA_ARITH_WORDS,
+	HWC_DATA_LS_ISSUES,
+	HWC_DATA_TEX_ISSUES,
+
+	HWC_DATA_GPU_JS0_ACTIVE,
+	HWC_DATA_GPU_JS1_ACTIVE,
+	HWC_DATA_GPU_JS2_ACTIVE,
+	HWC_DATA_ARITH_CYCLES_REG,
+	HWC_DATA_ARITH_CYCLES_L0,
+	HWC_DATA_ARITH_FRAG_DEPEND,
+	HWC_DATA_LS_WORDS,
+	HWC_DATA_TEX_WORDS,
+	HWC_DATA_FRAG_ACTIVE,
+	HWC_DATA_FRAG_PRIMITIVES,
+	HWC_DATA_COMPUTE_ACTIVE,
+	HWC_DATA_TILER_ACTIVE,
+	HWC_DATA_L2_EXT_RD_BEAT,
+	HWC_DATA_L2_EXT_WR_BEAT,
+	HWC_DATA_MMU_HIT,
+	HWC_DATA_NEW_MISS,
+	HWC_DATA_MAX
+} HWC_DATA_IDX;
+
+typedef struct _gpu_dvfs_hwc_setup{
+	u32 profile_mode;
+	u32 jm_bm;
+	u32 tiler_bm;
+	u32 sc_bm;
+	u32 memory_bm;
+} gpu_dvfs_hwc_setup;
+
+typedef struct _gpu_dvfs_hwc_data {
+	u32 data[HWC_DATA_MAX];
+} gpu_dvfs_hwc_data;
+#endif
+
+
 struct exynos_context {
 	/* lock variables */
 	struct mutex gpu_clock_lock;
 	struct mutex gpu_dvfs_handler_lock;
+#ifdef CONFIG_MALI_DVFS_USER
+	struct mutex gpu_process_job_lock;
+#endif
 	spinlock_t gpu_dvfs_spinlock;
 
 	/* clock & voltage related variables */
@@ -264,7 +319,21 @@
 	int boost_gpu_min_lock;
 	int boost_egl_min_lock;
 	bool boost_is_enabled;
-
+#ifdef CONFIG_MALI_DVFS_USER
+	/* other boost lock */
+	int mif_min_step;
+	int int_min_step;
+	int apollo_min_step;
+	int atlas_min_step;
+	int *mif_table;
+	int *int_table;
+	int *atlas_table;
+	int *apollo_table;
+	int mif_table_size;
+	int int_table_size;
+	int atlas_table_size;
+	int apollo_table_size;
+#endif
 	bool tmu_status;
 	int tmu_lock_clk[TMU_LOCK_CLK_END];
 	int cold_min_vol;
@@ -280,7 +349,7 @@
 	bool power_status;
 
 	bool perf_gathering_status;
-#ifdef MALI_SEC_HWCNT
+#ifdef CONFIG_MALI_SEC_HWCNT
 	bool hwcnt_gathering_status;
 	bool hwcnt_gpr_status;
 	int hwcnt_polling_speed;
@@ -294,6 +363,8 @@
 	int hwcnt_choose_mmu_l2;
 
 	bool hwcnt_bt_clk;
+	int hwcnt_allow_vertex_throttle;
+	bool hwcnt_profile;
 #endif
 
 	int polling_speed;
@@ -312,8 +383,16 @@
 	int data_invalid_fault_count;
 	int mmu_fault_count;
 	int balance_retry_count[BMAX_RETRY_CNT];
-
+#ifdef CONFIG_MALI_DVFS_USER
+	int udvfs_enable;
+	struct kbase_context *dvfs_kctx;
+	int atom_idx;
+	gpu_dvfs_hwc_data hwc_data;
+#endif
 	gpu_attribute *attrib;
+	int mo_min_clock;
+	int *save_cpu_max_freq;
+	const struct kbase_pm_policy *cur_policy;
 };
 
 struct kbase_device *gpu_get_device_structure(void);
@@ -326,11 +405,20 @@
 uintptr_t gpu_get_attrib_data(gpu_attribute *attrib, int id);
 int gpu_platform_context_init(struct exynos_context *platform);
 
-int kbase_platform_early_init(struct platform_device *pdev);
 int gpu_set_rate_for_pm_resume(struct kbase_device *kbdev, int clk);
 void gpu_clock_disable(struct kbase_device *kbdev);
 
 bool balance_init(struct kbase_device *kbdev);
-int exynos_gpu_init_hw(struct kbase_device *kbdev);
+int exynos_gpu_init_hw(void *dev);
 
+#ifdef CONFIG_MALI_DVFS_USER
+bool gpu_dvfs_process_job(void *pkatom);
+unsigned int gpu_get_config_attr_size(void);
+void gpu_dvfs_notify_poweron(void);
+void gpu_dvfs_notify_poweroff(void);
+void gpu_dvfs_check_destroy_context(struct kbase_context *kctx);
+#ifdef CONFIG_PWRCAL
+bool update_cal_table(void);
+#endif
+#endif
 #endif /* _GPU_PLATFORM_H_ */
